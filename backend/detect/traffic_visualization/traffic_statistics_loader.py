import pandas as pd
import numpy as np
import os
import json
from typing import List, Dict, Tuple, Optional, Union, Any
from datetime import datetime, timedelta, timezone
import logging

class TrafficStatisticsLoader:
    """ä¸“ç”¨äºæµé‡ç»Ÿè®¡çš„æ•°æ®åŠ è½½å™¨ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§ï¼Œä¸è¿›è¡Œé‡‡æ ·"""
    
    def __init__(self, data_dir: str = None):
        """
        åˆå§‹åŒ–æµé‡ç»Ÿè®¡æ•°æ®åŠ è½½å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        if data_dir is None:
            # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
            current_dir = os.path.dirname(os.path.abspath(__file__))
            # å°è¯•å¤šç§å¯èƒ½çš„æ•°æ®è·¯å¾„
            possible_paths = [
                os.path.join(current_dir, 'data'),
                os.path.join(current_dir, '..', '..', 'data'),
                os.path.join(current_dir, '..', '..', '..', 'data')
            ]
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå­˜åœ¨çš„è·¯å¾„
            for path in possible_paths:
                if os.path.exists(path) and os.path.isdir(path):
                    self.data_dir = path
                    break
            else:
                # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
                self.data_dir = os.path.join(current_dir, 'data')
        else:
            self.data_dir = data_dir
            
        print(f"æµé‡ç»Ÿè®¡æ•°æ®ç›®å½•: {self.data_dir}")
        
        # é¢„å¤„ç†æ•°æ®ç›®å½•
        self.processed_dir = os.path.join(self.data_dir, 'processed')
        self.index_dir = os.path.join(self.data_dir, 'indexes')
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é¢„å¤„ç†æ•°æ®
        self.use_preprocessed = (
            os.path.exists(self.processed_dir) and 
            os.path.exists(self.index_dir) and
            len(os.listdir(self.processed_dir)) > 0
        )
        
        if self.use_preprocessed:
            print("âœ“ å‘ç°é¢„å¤„ç†æ•°æ®ï¼Œå°†ä½¿ç”¨é«˜æ•ˆç»Ÿè®¡æ¨¡å¼")
            self._load_indexes()
        else:
            print("âœ— æœªå‘ç°é¢„å¤„ç†æ•°æ®ï¼Œå°†ä½¿ç”¨åŸå§‹CSVæ–‡ä»¶ç»Ÿè®¡")
        
        # æµé‡ç»Ÿè®¡ä¸“ç”¨ç¼“å­˜
        self._hourly_cache = {}       # æŒ‰å°æ—¶ç»Ÿè®¡ç¼“å­˜
        self._daily_cache = {}        # æŒ‰å¤©ç»Ÿè®¡ç¼“å­˜
        self._weekly_cache = {}       # æŒ‰å‘¨ç»Ÿè®¡ç¼“å­˜
        self._cache_maxsize = 20      # å¢å¤§ç¼“å­˜ä»¥æ”¯æŒç»Ÿè®¡æŸ¥è¯¢
        self._csv_files = None
        
        # æ—¥å¿—é…ç½®
        self.logger = logging.getLogger(__name__)
    
    def _load_indexes(self):
        """åŠ è½½é¢„å¤„ç†çš„ç´¢å¼•æ•°æ®"""
        try:
            # åŠ è½½æ—¶é—´ç´¢å¼•ï¼ˆç”¨äºå¿«é€Ÿå®šä½æ—¶é—´æ®µï¼‰
            time_index_path = os.path.join(self.index_dir, 'time_index.json')
            if os.path.exists(time_index_path):
                with open(time_index_path, 'r') as f:
                    self.time_index = json.load(f)
                print(f"âœ“ åŠ è½½æ—¶é—´ç´¢å¼•")
            else:
                self.time_index = {}
            
        except Exception as e:
            print(f"è­¦å‘Šï¼šåŠ è½½ç´¢å¼•æ—¶å‡ºé”™: {e}")
            self.use_preprocessed = False

    def get_csv_files(self) -> List[str]:
        """è·å–æ•°æ®ç›®å½•ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶"""
        if self._csv_files is None:
            self._csv_files = [
                os.path.join(self.data_dir, f) 
                for f in os.listdir(self.data_dir) 
                if f.endswith('.csv')
            ]
        return self._csv_files
    
    def load_traffic_count_data(self, start_time: float, end_time: float) -> pd.DataFrame:
        """
        ä¸“é—¨ç”¨äºæµé‡ç»Ÿè®¡çš„æ•°æ®åŠ è½½ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
        åªåŠ è½½å¿…è¦çš„åˆ—ï¼šæ—¶é—´æˆ³å’Œè½¦è¾†IDï¼Œç”¨äºè®¡æ•°ç»Ÿè®¡
        
        Args:
            start_time: å¼€å§‹æ—¶é—´æˆ³
            end_time: ç»“æŸæ—¶é—´æˆ³
            
        Returns:
            åŒ…å«æ—¶é—´æˆ³å’Œè½¦è¾†IDçš„å®Œæ•´DataFrameï¼Œç”¨äºç»Ÿè®¡
        """
        print(f"ğŸš— åŠ è½½æµé‡ç»Ÿè®¡æ•°æ®: {start_time} åˆ° {end_time}")
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"traffic_count_{start_time}_{end_time}"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self._hourly_cache:
            print("ğŸ“Š ä½¿ç”¨æµé‡ç»Ÿè®¡ç¼“å­˜")
            return self._hourly_cache[cache_key]
        
        # ä½¿ç”¨ç›¸åº”çš„åŠ è½½æ–¹æ³•
        if self.use_preprocessed:
            result = self._load_traffic_count_fast(start_time, end_time)
        else:
            result = self._load_traffic_count_legacy(start_time, end_time)
        
        # æ›´æ–°ç¼“å­˜ï¼ˆLRUç­–ç•¥ï¼‰
        if len(self._hourly_cache) >= self._cache_maxsize:
            oldest_key = next(iter(self._hourly_cache))
            del self._hourly_cache[oldest_key]
        
        self._hourly_cache[cache_key] = result
        print(f"âœ… æµé‡ç»Ÿè®¡æ•°æ®åŠ è½½å®Œæˆ: {len(result)} æ¡è®°å½•")
        return result
    
    def _load_traffic_count_fast(self, start_time: float, end_time: float) -> pd.DataFrame:
        """ä½¿ç”¨é¢„å¤„ç†æ•°æ®è¿›è¡Œå¿«é€Ÿæµé‡ç»Ÿè®¡åŠ è½½"""
        print("ğŸš€ ä½¿ç”¨é¢„å¤„ç†æ•°æ®è¿›è¡Œæµé‡ç»Ÿè®¡...")
        
        # è®¡ç®—éœ€è¦çš„æ—¶é—´æ®µï¼ˆå°æ—¶çº§åˆ«ï¼‰
        start_hour = (int(start_time) // 3600) * 3600
        end_hour = (int(end_time) // 3600) * 3600
        
        # åŠ è½½ç›¸å…³æ—¶é—´æ®µçš„æ•°æ®
        data_frames = []
        current_hour = start_hour
        
        while current_hour <= end_hour:
            filename = f"hour_{int(current_hour)}.parquet"
            filepath = os.path.join(self.processed_dir, filename)
            
            if os.path.exists(filepath):
                try:
                    # åªåŠ è½½ç»Ÿè®¡å¿…éœ€çš„åˆ—
                    columns_to_load = ['UTC', 'COMMADDR']
                    df = pd.read_parquet(filepath, columns=columns_to_load)
                    
                    # ç²¾ç¡®æ—¶é—´è¿‡æ»¤
                    df = df[(df['UTC'] >= start_time) & (df['UTC'] <= end_time)]
                    
                    if not df.empty:
                        data_frames.append(df)
                        print(f"   ğŸ“ {filename}: {len(df)} æ¡è®°å½•")
                        
                except Exception as e:
                    print(f"âŒ è¯»å– {filename} æ—¶å‡ºé”™: {e}")
            
            current_hour += 3600  # ä¸‹ä¸€å°æ—¶
        
        # åˆå¹¶æ•°æ®
        if data_frames:
            result = pd.concat(data_frames, ignore_index=True)
            # æŒ‰æ—¶é—´æ’åº
            result = result.sort_values('UTC')
            print(f"âš¡ å¿«é€Ÿæµé‡ç»Ÿè®¡åŠ è½½å®Œæˆï¼Œå…± {len(result)} æ¡è®°å½•")
            return result
        else:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æµé‡æ•°æ®")
            return pd.DataFrame()

    def _load_traffic_count_legacy(self, start_time: float, end_time: float) -> pd.DataFrame:
        """ä½¿ç”¨åŸå§‹CSVæ–‡ä»¶è¿›è¡Œæµé‡ç»Ÿè®¡åŠ è½½"""
        print("ğŸ“ˆ ä½¿ç”¨åŸå§‹CSVæ–‡ä»¶è¿›è¡Œæµé‡ç»Ÿè®¡...")
        
        # æ·»åŠ æ•°æ®é›†æ—¶é—´èŒƒå›´éªŒè¯
        min_valid_time = 1378944000  # 2013-09-12 00:00:00 UTC
        max_valid_time = 1379548799  # 2013-09-18 23:59:59 UTC
        
        # æ£€æŸ¥è¯·æ±‚çš„æ—¶é—´èŒƒå›´æ˜¯å¦ä¸æ•°æ®é›†æ—¶é—´èŒƒå›´æœ‰äº¤é›†
        if end_time < min_valid_time or start_time > max_valid_time:
            print(f"è­¦å‘Šï¼šè¯·æ±‚çš„æ—¶é—´èŒƒå›´ ({start_time}-{end_time}) è¶…å‡ºæ•°æ®é›†èŒƒå›´ ({min_valid_time}-{max_valid_time})")
            return pd.DataFrame()
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"legacy_traffic_{start_time}_{end_time}"
        
        # è·å–æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = self.get_csv_files()
        
        if not csv_files:
            print("æœªæ‰¾åˆ°CSVæ–‡ä»¶")
            return pd.DataFrame()
        
        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶ï¼Œè¿›è¡Œæµé‡ç»Ÿè®¡")
        
        # å­˜å‚¨æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®
        all_data = []
        
        for i, file_path in enumerate(csv_files):
            print(f"ç»Ÿè®¡æ–‡ä»¶ {i+1}/{len(csv_files)}: {os.path.basename(file_path)}")
            
            try:
                # ä½¿ç”¨åˆ†å—è¯»å–ï¼Œä½†ä¸é™åˆ¶æ•°æ®é‡ï¼ˆç¡®ä¿ç»Ÿè®¡å®Œæ•´æ€§ï¼‰
                chunk_size = 100000
                chunks = pd.read_csv(file_path, chunksize=chunk_size)
                
                for chunk_num, chunk in enumerate(chunks):
                    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
                    required_cols = ['UTC', 'COMMADDR']
                    if not all(col in chunk.columns for col in required_cols):
                        continue
                    
                    # åªä¿ç•™ç»Ÿè®¡éœ€è¦çš„åˆ—
                    chunk = chunk[required_cols]
                    
                    # æ—¶é—´è¿‡æ»¤
                    filtered_chunk = chunk[(chunk['UTC'] >= start_time) & (chunk['UTC'] <= end_time)]
                    
                    if not filtered_chunk.empty:
                        all_data.append(filtered_chunk)
                        print(f"   å¤„ç†å— {chunk_num + 1}: {len(filtered_chunk)} æ¡è®°å½•")
            
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {os.path.basename(file_path)} æ—¶å‡ºé”™: {e}")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if all_data:
            result_df = pd.concat(all_data, ignore_index=True)
            print(f"ğŸ“Š æµé‡ç»Ÿè®¡æ•°æ®åŠ è½½å®Œæˆ: {len(result_df)} æ¡è®°å½•")
            return result_df
        else:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æµé‡æ•°æ®")
            return pd.DataFrame()
    
    def get_hourly_traffic_counts(self, start_time: float, end_time: float) -> List[int]:
        """
        è·å–æŒ‡å®šæ—¶é—´æ®µçš„æ¯å°æ—¶è½¦è¾†ç»Ÿè®¡
        
        Args:
            start_time: å¼€å§‹æ—¶é—´æˆ³
            end_time: ç»“æŸæ—¶é—´æˆ³
            
        Returns:
            24ä¸ªå°æ—¶çš„è½¦è¾†æ•°åˆ—è¡¨ï¼ˆå¦‚æœè·¨åº¦å°äº24å°æ—¶åˆ™æŒ‰å®é™…å°æ—¶æ•°è¿”å›ï¼‰
        """
        # åŠ è½½æ•°æ®
        df = self.load_traffic_count_data(start_time, end_time)
        
        if df.empty:
            # è¿”å›24å°æ—¶çš„é›¶å€¼
            return [0] * 24
        
        # æ£€æŸ¥æ—¶é—´åˆ—
        time_col = self._get_time_column(df)
        if not time_col:
            return [0] * 24
        
        # ğŸ”§ ä¿®å¤æ—¶åŒºé—®é¢˜ï¼šå°†UTCæ—¶é—´è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
        print(f"ğŸ• å¤„ç†æ—¶åŒºè½¬æ¢ï¼šUTC â†’ åŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰")
        
        # å°†UTCæ—¶é—´æˆ³è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´çš„å°æ—¶
        df['datetime_utc'] = pd.to_datetime(df[time_col], unit='s')
        df['datetime_beijing'] = df['datetime_utc'] + pd.Timedelta(hours=8)
        df['hour'] = df['datetime_beijing'].dt.hour
        
        # æŒ‰å°æ—¶èšåˆè½¦è¾†æ•°
        hourly_counts = df.groupby('hour').size().reindex(range(24), fill_value=0).tolist()
        
        print(f"ğŸ“Š å°æ—¶ç»Ÿè®¡å®Œæˆ: æ€»è®¡ {sum(hourly_counts)} è½¦æ¬¡")
        print(f"ğŸ• æ—¶é—´è½¬æ¢éªŒè¯:")
        print(f"   æ ·æœ¬UTCæ—¶é—´: {df['datetime_utc'].iloc[0] if len(df) > 0 else 'N/A'}")
        print(f"   æ ·æœ¬åŒ—äº¬æ—¶é—´: {df['datetime_beijing'].iloc[0] if len(df) > 0 else 'N/A'}")
        
        return hourly_counts
    
    def get_daily_traffic_counts(self, start_time: float, end_time: float, days: int = 7) -> List[Dict[str, Union[str, int]]]:
        """
        è·å–æŒ‡å®šæ—¶é—´æ®µçš„æ¯æ—¥è½¦è¾†ç»Ÿè®¡
        
        Args:
            start_time: å¼€å§‹æ—¶é—´æˆ³
            end_time: ç»“æŸæ—¶é—´æˆ³
            days: å¤©æ•°ï¼Œé»˜è®¤7å¤©
            
        Returns:
            åŒ…å«æ—¥æœŸå’Œè½¦è¾†æ€»æ•°çš„å­—å…¸åˆ—è¡¨
        """
        # åŠ è½½æ•°æ®
        df = self.load_traffic_count_data(start_time, end_time)
        
        if df.empty:
            # è¿”å›æŒ‡å®šå¤©æ•°çš„é›¶å€¼ - ä¿®å¤æ—¶åŒºé—®é¢˜
            result = []
            # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨UTCæ—¶é—´è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´
            start_datetime_utc = datetime.fromtimestamp(start_time, tz=timezone.utc)
            start_datetime_beijing = start_datetime_utc + timedelta(hours=8)
            
            for i in range(days):
                date_obj = (start_datetime_beijing + timedelta(days=i)).date()
                date_str = date_obj.strftime("%Y-%m-%d")
                result.append({"date": date_str, "totalVehicles": 0})
            return result
        
        # æ£€æŸ¥æ—¶é—´åˆ—
        time_col = self._get_time_column(df)
        if not time_col:
            return []
        
        # ğŸ”§ ä¿®å¤æ—¶åŒºé—®é¢˜ï¼šå°†UTCæ—¶é—´è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
        print(f"ğŸ• å¤„ç†æ¯æ—¥ç»Ÿè®¡æ—¶åŒºè½¬æ¢ï¼šUTC â†’ åŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰")
        
        # å°†UTCæ—¶é—´æˆ³è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´çš„æ—¥æœŸ
        df['datetime_utc'] = pd.to_datetime(df[time_col], unit='s')
        df['datetime_beijing'] = df['datetime_utc'] + pd.Timedelta(hours=8)
        df['date'] = df['datetime_beijing'].dt.date
        
        # æŒ‰å¤©èšåˆè½¦è¾†æ•°
        daily_counts = df.groupby('date').size().reset_index(name='totalVehicles')
        
        # ğŸ”§ ä¿®å¤ï¼šè¡¥é½æŒ‡å®šå¤©æ•°çš„æ•°æ® - ç¡®ä¿æ—¶åŒºä¸€è‡´æ€§
        result = []
        # ä½¿ç”¨ä¸æ•°æ®å¤„ç†ç›¸åŒçš„æ—¶åŒºè½¬æ¢é€»è¾‘
        start_datetime_utc = datetime.fromtimestamp(start_time, tz=timezone.utc)
        start_datetime_beijing = start_datetime_utc + timedelta(hours=8)
        
        for i in range(days):
            date_obj = (start_datetime_beijing + timedelta(days=i)).date()
            date_str = date_obj.strftime("%Y-%m-%d")
            # æŸ¥æ‰¾è¯¥æ—¥æœŸçš„è½¦è¾†æ•°
            matching_count = daily_counts[daily_counts['date'] == date_obj]['totalVehicles']
            count = matching_count.iloc[0] if len(matching_count) > 0 else 0
            result.append({"date": date_str, "totalVehicles": int(count)})
        
        total_vehicles = sum(item['totalVehicles'] for item in result)
        print(f"ğŸ“Š æ¯æ—¥ç»Ÿè®¡å®Œæˆ: {days}å¤©æ€»è®¡ {total_vehicles} è½¦æ¬¡")
        
        # æ˜¾ç¤ºæ—¶åŒºè½¬æ¢éªŒè¯ä¿¡æ¯
        if len(df) > 0:
            print(f"ğŸ• æ¯æ—¥ç»Ÿè®¡æ—¶é—´è½¬æ¢éªŒè¯:")
            print(f"   æ ·æœ¬UTCæ—¶é—´: {df['datetime_utc'].iloc[0]}")
            print(f"   æ ·æœ¬åŒ—äº¬æ—¶é—´: {df['datetime_beijing'].iloc[0]}")
            print(f"   åŸºå‡†æ—¥æœŸèŒƒå›´: {start_datetime_beijing.date()} åˆ° {(start_datetime_beijing + timedelta(days=days-1)).date()}")
        
        return result
    
    def _get_time_column(self, df: pd.DataFrame) -> Optional[str]:
        """è·å–æ—¶é—´åˆ—å"""
        if 'UTC' in df.columns:
            return 'UTC'
        elif 'timestamp' in df.columns:
            return 'timestamp'
        elif 'TIMESTAMP' in df.columns:
            return 'TIMESTAMP'
        else:
            print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æ—¶é—´åˆ—ï¼Œå¯ç”¨åˆ—: {list(df.columns)}")
            return None
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        self._hourly_cache.clear()
        self._daily_cache.clear()
        self._weekly_cache.clear()
        print("ğŸ§¹ æµé‡ç»Ÿè®¡ç¼“å­˜å·²æ¸…é™¤")
    
    def load_metrics_data(self, start_time: float, end_time: float) -> pd.DataFrame:
        """
        ä¸“é—¨ç”¨äºå…³é”®æŒ‡æ ‡è®¡ç®—çš„æ•°æ®åŠ è½½ï¼ŒåŒ…å«é€Ÿåº¦ç­‰é¢å¤–ä¿¡æ¯
        
        Args:
            start_time: å¼€å§‹æ—¶é—´æˆ³
            end_time: ç»“æŸæ—¶é—´æˆ³
            
        Returns:
            åŒ…å«æ—¶é—´æˆ³ã€è½¦è¾†IDã€é€Ÿåº¦ç­‰çš„DataFrameï¼Œç”¨äºæŒ‡æ ‡è®¡ç®—
        """
        print(f"ğŸ“Š åŠ è½½æŒ‡æ ‡è®¡ç®—æ•°æ®: {start_time} åˆ° {end_time}")
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"metrics_data_{start_time}_{end_time}"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self._daily_cache:
            print("ğŸ“ˆ ä½¿ç”¨æŒ‡æ ‡è®¡ç®—ç¼“å­˜")
            return self._daily_cache[cache_key]
        
        # ä½¿ç”¨ç›¸åº”çš„åŠ è½½æ–¹æ³•
        if self.use_preprocessed:
            result = self._load_metrics_data_fast(start_time, end_time)
        else:
            result = self._load_metrics_data_legacy(start_time, end_time)
        
        # æ›´æ–°ç¼“å­˜
        if len(self._daily_cache) >= self._cache_maxsize:
            oldest_key = next(iter(self._daily_cache))
            del self._daily_cache[oldest_key]
        
        self._daily_cache[cache_key] = result
        print(f"âœ… æŒ‡æ ‡è®¡ç®—æ•°æ®åŠ è½½å®Œæˆ: {len(result)} æ¡è®°å½•")
        return result
    
    def _load_metrics_data_fast(self, start_time: float, end_time: float) -> pd.DataFrame:
        """ä½¿ç”¨é¢„å¤„ç†æ•°æ®è¿›è¡Œå¿«é€ŸæŒ‡æ ‡æ•°æ®åŠ è½½"""
        print("ğŸš€ ä½¿ç”¨é¢„å¤„ç†æ•°æ®è¿›è¡ŒæŒ‡æ ‡è®¡ç®—...")
        
        # è®¡ç®—éœ€è¦çš„æ—¶é—´æ®µï¼ˆå°æ—¶çº§åˆ«ï¼‰
        start_hour = (int(start_time) // 3600) * 3600
        end_hour = (int(end_time) // 3600) * 3600
        
        # åŠ è½½ç›¸å…³æ—¶é—´æ®µçš„æ•°æ®
        data_frames = []
        current_hour = start_hour
        
        while current_hour <= end_hour:
            filename = f"hour_{int(current_hour)}.parquet"
            filepath = os.path.join(self.processed_dir, filename)
            
            if os.path.exists(filepath):
                try:
                    # åŠ è½½æŒ‡æ ‡è®¡ç®—éœ€è¦çš„åˆ—
                    columns_to_load = ['UTC', 'COMMADDR', 'SPEED', 'LAT', 'LON']
                    df = pd.read_parquet(filepath, columns=columns_to_load)
                    
                    # ç²¾ç¡®æ—¶é—´è¿‡æ»¤
                    df = df[(df['UTC'] >= start_time) & (df['UTC'] <= end_time)]
                    
                    if not df.empty:
                        data_frames.append(df)
                        print(f"   ğŸ“ {filename}: {len(df)} æ¡è®°å½•")
                        
                except Exception as e:
                    print(f"âŒ è¯»å– {filename} æ—¶å‡ºé”™: {e}")
            
            current_hour += 3600  # ä¸‹ä¸€å°æ—¶
        
        # åˆå¹¶æ•°æ®
        if data_frames:
            result = pd.concat(data_frames, ignore_index=True)
            # æŒ‰æ—¶é—´æ’åº
            result = result.sort_values('UTC')
            print(f"âš¡ å¿«é€ŸæŒ‡æ ‡æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(result)} æ¡è®°å½•")
            return result
        else:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æŒ‡æ ‡æ•°æ®")
            return pd.DataFrame()

    def _load_metrics_data_legacy(self, start_time: float, end_time: float) -> pd.DataFrame:
        """ä½¿ç”¨åŸå§‹CSVæ–‡ä»¶è¿›è¡ŒæŒ‡æ ‡æ•°æ®åŠ è½½"""
        print("ğŸ“ˆ ä½¿ç”¨åŸå§‹CSVæ–‡ä»¶è¿›è¡ŒæŒ‡æ ‡è®¡ç®—...")
        
        # æ·»åŠ æ•°æ®é›†æ—¶é—´èŒƒå›´éªŒè¯
        min_valid_time = 1378944000  # 2013-09-12 00:00:00 UTC
        max_valid_time = 1379548799  # 2013-09-18 23:59:59 UTC
        
        # æ£€æŸ¥è¯·æ±‚çš„æ—¶é—´èŒƒå›´æ˜¯å¦ä¸æ•°æ®é›†æ—¶é—´èŒƒå›´æœ‰äº¤é›†
        if end_time < min_valid_time or start_time > max_valid_time:
            print(f"è­¦å‘Šï¼šè¯·æ±‚çš„æ—¶é—´èŒƒå›´ ({start_time}-{end_time}) è¶…å‡ºæ•°æ®é›†èŒƒå›´ ({min_valid_time}-{max_valid_time})")
            return pd.DataFrame()
        
        # è·å–æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = self.get_csv_files()
        
        if not csv_files:
            print("æœªæ‰¾åˆ°CSVæ–‡ä»¶")
            return pd.DataFrame()
        
        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶ï¼Œè¿›è¡ŒæŒ‡æ ‡è®¡ç®—")
        
        # å­˜å‚¨æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®
        all_data = []
        
        for i, file_path in enumerate(csv_files):
            print(f"å¤„ç†æ–‡ä»¶ {i+1}/{len(csv_files)}: {os.path.basename(file_path)}")
            
            try:
                # ä½¿ç”¨åˆ†å—è¯»å–
                chunk_size = 100000
                chunks = pd.read_csv(file_path, chunksize=chunk_size)
                
                for chunk_num, chunk in enumerate(chunks):
                    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
                    required_cols = ['UTC', 'COMMADDR']
                    if not all(col in chunk.columns for col in required_cols):
                        continue
                    
                    # ä¿ç•™æŒ‡æ ‡è®¡ç®—éœ€è¦çš„åˆ—
                    available_cols = ['UTC', 'COMMADDR']
                    if 'SPEED' in chunk.columns:
                        available_cols.append('SPEED')
                    if 'LAT' in chunk.columns:
                        available_cols.append('LAT')
                    if 'LON' in chunk.columns:
                        available_cols.append('LON')
                    
                    chunk = chunk[available_cols]
                    
                    # æ—¶é—´è¿‡æ»¤
                    filtered_chunk = chunk[(chunk['UTC'] >= start_time) & (chunk['UTC'] <= end_time)]
                    
                    if not filtered_chunk.empty:
                        all_data.append(filtered_chunk)
                        print(f"   å¤„ç†å— {chunk_num + 1}: {len(filtered_chunk)} æ¡è®°å½•")
            
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {os.path.basename(file_path)} æ—¶å‡ºé”™: {e}")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if all_data:
            result_df = pd.concat(all_data, ignore_index=True)
            print(f"ğŸ“Š æŒ‡æ ‡æ•°æ®åŠ è½½å®Œæˆ: {len(result_df)} æ¡è®°å½•")
            return result_df
        else:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æŒ‡æ ‡æ•°æ®")
            return pd.DataFrame()
    
    def calculate_key_metrics(self, start_time: float, end_time: float, hours: int) -> Dict[str, any]:
        """
        è®¡ç®—å…³é”®æŒ‡æ ‡
        
        Args:
            start_time: å¼€å§‹æ—¶é—´æˆ³
            end_time: ç»“æŸæ—¶é—´æˆ³
            hours: æ—¶é—´è·¨åº¦ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
        """
        print(f"ğŸ“Š è®¡ç®—å…³é”®æŒ‡æ ‡ï¼Œæ—¶é—´è·¨åº¦: {hours} å°æ—¶")
        
        # åŠ è½½æŒ‡æ ‡è®¡ç®—æ•°æ®
        df = self.load_metrics_data(start_time, end_time)
        
        if df.empty:
            return {
                "total_vehicles": 0,
                "avg_speed": 0.0,
                "peak_hours": 0.0,
                "unique_vehicles": 0,
                "data_available": False
            }
        
        # 1. æ€»æµé‡ï¼ˆæ•°æ®ç‚¹æ€»æ•°ï¼‰
        total_vehicles = len(df)
        
        # 2. å¹³å‡é€Ÿåº¦
        avg_speed = 0.0
        if 'SPEED' in df.columns:
            # è¿‡æ»¤æ‰å¼‚å¸¸é€Ÿåº¦å€¼
            valid_speeds = df['SPEED'][(df['SPEED'] >= 0) & (df['SPEED'] <= 200)]
            if len(valid_speeds) > 0:
                avg_speed = valid_speeds.mean()
        
        # 3. æ´»è·ƒç”¨æˆ·ï¼ˆå”¯ä¸€è½¦è¾†æ•°ï¼‰
        unique_vehicles = df['COMMADDR'].nunique()
        
        # 4. é«˜å³°æ—¶é•¿è®¡ç®— - ğŸ”§ ä¿®å¤æ—¶åŒºé—®é¢˜
        peak_hours = 0.0
        if hours > 0:
            time_col = self._get_time_column(df)
            if time_col:
                print(f"ğŸ• è®¡ç®—é«˜å³°æ—¶é•¿ï¼Œå¤„ç†æ—¶åŒºè½¬æ¢ï¼šUTC â†’ åŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰")
                
                # å°†UTCæ—¶é—´è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´å†æŒ‰å°æ—¶åˆ†ç»„
                df['datetime_utc'] = pd.to_datetime(df[time_col], unit='s')
                df['datetime_beijing'] = df['datetime_utc'] + pd.Timedelta(hours=8)
                df['hour'] = df['datetime_beijing'].dt.floor('H')
                
                hourly_counts = df.groupby('hour').size()
                
                if len(hourly_counts) > 0:
                    avg_vehicles_per_hour = total_vehicles / hours
                    # å®šä¹‰é«˜å³°ä¸ºè¶…è¿‡å¹³å‡å€¼1.3å€çš„å°æ—¶
                    peak_threshold = avg_vehicles_per_hour * 1.3
                    peak_hours = len([count for count in hourly_counts if count > peak_threshold])
                    
                    print(f"ğŸš¦ é«˜å³°åˆ†æ: å¹³å‡{avg_vehicles_per_hour:.1f}è½¦æ¬¡/å°æ—¶, é˜ˆå€¼{peak_threshold:.1f}, é«˜å³°æ—¶æ®µ{peak_hours}å°æ—¶")
        
        return {
            "total_vehicles": total_vehicles,
            "avg_speed": round(avg_speed, 1),
            "peak_hours": round(peak_hours, 1),
            "unique_vehicles": unique_vehicles,
            "data_available": True
        }

    def get_cache_info(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
        return {
            "hourly_cache_size": len(self._hourly_cache),
            "daily_cache_size": len(self._daily_cache),
            "weekly_cache_size": len(self._weekly_cache),
            "total_cached_items": len(self._hourly_cache) + len(self._daily_cache) + len(self._weekly_cache)
        }