from fastapi import APIRouter, Query, Depends, HTTPException
import pandas as pd
import os
import datetime
from collections import defaultdict
from typing import List, Dict, Optional, Any, Union
from .data_processor import TrafficDataProcessor
from .heatmap import HeatmapGenerator
from .track import TrackAnalyzer
# from .data_cleaner import TrafficDataCleaner, DataQualityAnalyzer  # å·²åˆ é™¤æ•°æ®æ¸…æ´—åŠŸèƒ½

from .models import (
    TimeRangeRequest, TrafficQueryRequest, HeatmapRequest, 
    TrackQueryRequest, StatisticsRequest, TrafficResponse,
    TrafficDataResponse, HeatmapResponse, TracksResponse,
    StatisticsResponse, TrafficOverview, TimeDistribution,
    DynamicHeatmapResponse, ClusteringRequest, ClusteringResponse,
    ODAnalysisRequest, ODFlowResponse, SpatioTemporalResponse,
    SpatioTemporalAnalysis, RoadAnalysisRequest, RoadAnalysisResponse,
    RoadSegmentResponse, RoadTrafficResponse, RoadVisualizationResponse,
    RoadSegment, RoadTrafficData, RoadSegmentStatistics, 
    RoadNetworkAnalysis, SpeedDistribution, TrafficFlowPattern
)
import numpy as np
import logging
import traceback
import time
import json

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter()

# åˆ›å»ºæ•°æ®å¤„ç†å™¨å®ä¾‹
data_processor = TrafficDataProcessor()
heatmap_generator = HeatmapGenerator()
track_analyzer = TrackAnalyzer()
# data_cleaner = TrafficDataCleaner()  # å·²åˆ é™¤æ•°æ®æ¸…æ´—åŠŸèƒ½
# quality_analyzer = DataQualityAnalyzer()  # å·²åˆ é™¤æ•°æ®æ¸…æ´—åŠŸèƒ½

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ä¸€ä¸ªæ–°çš„è¾…åŠ©å‡½æ•°æ¥å¿«é€Ÿç”Ÿæˆçƒ­åŠ›å›¾
def _get_fast_heatmap_data(start_time: float, end_time: float) -> List[Dict]:
    """ä½¿ç”¨é¢„è®¡ç®—æ•°æ®å¿«é€Ÿç”Ÿæˆçƒ­åŠ›å›¾"""
    try:
        # ç›´æ¥ä»é¢„è®¡ç®—çš„çƒ­åŠ›å›¾æ–‡ä»¶ä¸­åŠ è½½æ•°æ®
        data_dir = os.path.join(os.path.dirname(__file__), 'data')
        index_dir = os.path.join(data_dir, 'indexes')
        
        if not os.path.exists(index_dir):
            return []
        
        # è®¡ç®—éœ€è¦çš„å¤©æ•°èŒƒå›´
        start_day = (int(start_time) // (24 * 3600)) * (24 * 3600)
        end_day = (int(end_time) // (24 * 3600)) * (24 * 3600)
        
        combined_heatmap = defaultdict(int)
        current_day = start_day
        
        while current_day <= end_day:
            filename = f"heatmap_day_{int(current_day)}.json"
            filepath = os.path.join(index_dir, filename)
            
            if os.path.exists(filepath):
                try:
                    with open(filepath, 'r') as f:
                        day_heatmap = json.load(f)
                    
                    for grid_key, count in day_heatmap.items():
                        combined_heatmap[grid_key] += count
                except Exception as e:
                    logger.error(f"è¯»å–çƒ­åŠ›å›¾æ–‡ä»¶ {filename} å¤±è´¥: {e}")
            
            current_day += 24 * 3600  # ä¸‹ä¸€å¤©
        
        # è½¬æ¢ä¸ºçƒ­åŠ›å›¾ç‚¹æ ¼å¼
        heatmap_points = []
        for grid_key, count in combined_heatmap.items():
            try:
                lat, lng = map(float, grid_key.split(','))
                heatmap_points.append({
                    'lat': lat,
                    'lng': lng,
                    'count': count
                })
            except:
                continue
        
        # æŒ‰å¯†åº¦æ’åºï¼Œå–å‰10000ä¸ªç‚¹é¿å…å‰ç«¯æ€§èƒ½é—®é¢˜
        heatmap_points.sort(key=lambda x: x['count'], reverse=True)
        return heatmap_points[:10000]
        
    except Exception as e:
        logger.error(f"å¿«é€Ÿçƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")
        return []

@router.get("/test")
async def test_endpoint():
    """æµ‹è¯•ç«¯ç‚¹ï¼Œç¡®ä¿è·¯ç”±æ­£å¸¸å·¥ä½œ"""
    return {"message": "Traffic router is working!", "status": "ok"}

@router.get("/stats", response_model=StatisticsResponse)
async def get_traffic_stats(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    group_by: str = Query("hour", description="æ—¶é—´åˆ†ç»„æ–¹å¼ï¼ˆhour, day, week, monthï¼‰")
):
    """
    è·å–äº¤é€šæ•°æ®ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    try:
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return StatisticsResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                overview=TrafficOverview(
                    total_vehicles=0,
                    total_points=0,
                    active_vehicles=0,
                    time_span="0å°æ—¶",
                    coverage_area="æœªçŸ¥",
                    average_speed=0.0
                ),
                time_distribution=[]
            )
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = data_processor.calculate_statistics(df, group_by)
        
        # æ„é€ å“åº”
        return StatisticsResponse(
            success=True,
            overview=TrafficOverview(
                total_vehicles=stats['total_vehicles'],
                total_points=stats['total_points'],
                active_vehicles=stats['active_vehicles'],
                time_span=stats['time_span'],
                coverage_area=stats['coverage_area'],
                average_speed=stats['average_speed']
            ),
            time_distribution=[
                TimeDistribution(time_key=item['time_key'], count=item['count'])
                for item in stats['time_distribution']
            ]
        )
    except Exception as e:
        return StatisticsResponse(
            success=False,
            message=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}",
            overview=TrafficOverview(
                total_vehicles=0,
                total_points=0,
                active_vehicles=0,
                time_span="0å°æ—¶",
                coverage_area="æœªçŸ¥",
                average_speed=0.0
            ),
            time_distribution=[]
        )

def convert_time_to_timestamp(time_str: str) -> float:
    """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºUTCæ—¶é—´æˆ³"""
    try:
        if 'T' in time_str:
            dt = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
        else:
            dt = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
        return dt.timestamp()
    except Exception as e:
        logger.error(f"æ—¶é—´è½¬æ¢å¤±è´¥: {time_str}, é”™è¯¯: {e}")
        raise HTTPException(status_code=400, detail=f"æ—¶é—´æ ¼å¼é”™è¯¯: {time_str}")

def convert_numpy_types(obj):
    """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif hasattr(obj, '__dict__'):
        # å¤„ç†pydanticæ¨¡å‹ç­‰å¯¹è±¡
        if hasattr(obj, 'dict'):
            return convert_numpy_types(obj.dict())
        else:
            return convert_numpy_types(obj.__dict__)
    return obj

@router.get("/visualization", response_model=TrafficDataResponse)
async def get_traffic_visualization(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    view_type: str = Query("distribution", description="è§†å›¾ç±»å‹ï¼šdistribution, trajectory, heatmap"),
    vehicle_id: Optional[str] = Query(None, description="è½¦è¾†IDï¼Œå¯é€‰"),
    map_style: Optional[str] = Query("blue", description="åœ°å›¾æ ·å¼")
):
    """
    è·å–äº¤é€šæ•°æ®å¯è§†åŒ–æ‰€éœ€æ•°æ®ï¼ˆåˆ†å¸ƒè§†å›¾ã€è½¨è¿¹è§†å›¾ã€çƒ­åŠ›å›¾ï¼‰ã€‚
    """
    try:
        print(f"=== å¼€å§‹å¤„ç†å¯è§†åŒ–è¯·æ±‚ ===")
        print(f"å‚æ•°: start_time={start_time}, end_time={end_time}, view_type={view_type}, vehicle_id={vehicle_id}")
        
        # éªŒè¯æ—¶é—´èŒƒå›´
        min_valid_time = 1378944000  # 2013-09-12 00:00:00 UTC
        max_valid_time = 1379548799  # 2013-09-18 23:59:59 UTC
        
        if end_time < min_valid_time or start_time > max_valid_time:
            print(f"æ—¶é—´èŒƒå›´éªŒè¯å¤±è´¥: {start_time}-{end_time} è¶…å‡ºæœ‰æ•ˆèŒƒå›´ {min_valid_time}-{max_valid_time}")
            return TrafficDataResponse(
                success=False,
                message="æŸ¥è¯¢æ—¶é—´è¶…å‡ºæ•°æ®é›†èŒƒå›´ï¼ˆ2013å¹´9æœˆ12æ—¥è‡³9æœˆ18æ—¥ï¼‰",
                view_type=view_type,
                data=[]
            )
        
        # åŠ è½½æ•°æ®
        try:
            print(f"å¼€å§‹åŠ è½½æ•°æ®...")
            df = data_processor.load_data(start_time, end_time, vehicle_id)
            print(f"æ•°æ®åŠ è½½å®Œæˆ, å…± {len(df)} æ¡è®°å½•")
        except Exception as load_error:
            print(f"æ•°æ®åŠ è½½é”™è¯¯: {str(load_error)}")
            import traceback
            print(traceback.format_exc())
            return TrafficDataResponse(
                success=False,
                message=f"æ•°æ®åŠ è½½å¤±è´¥: {str(load_error)}",
                view_type=view_type,
                data=[]
            )
        
        if df.empty:
            print("åŠ è½½çš„æ•°æ®ä¸ºç©º")
            return TrafficDataResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                view_type=view_type,
                data=[]
            )
        
        # æ ¹æ®è§†å›¾ç±»å‹å¤„ç†æ•°æ®
        try:
            print(f"å¼€å§‹å¤„ç† {view_type} è§†å›¾æ•°æ®...")
            if view_type == "heatmap":
                # ä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—çš„å¿«é€Ÿçƒ­åŠ›å›¾æ•°æ®
                print("å°è¯•ä½¿ç”¨é¢„è®¡ç®—çƒ­åŠ›å›¾æ•°æ®...")
                fast_heatmap_data = _get_fast_heatmap_data(start_time, end_time)
                
                if fast_heatmap_data and not vehicle_id:  # åªåœ¨æ²¡æœ‰ç‰¹å®šè½¦è¾†è¿‡æ»¤æ—¶ä½¿ç”¨é¢„è®¡ç®—æ•°æ®
                    print(f"ä½¿ç”¨å¿«é€Ÿçƒ­åŠ›å›¾æ•°æ®: {len(fast_heatmap_data)} ä¸ªç‚¹")
                    data = fast_heatmap_data
                else:
                    print("é¢„è®¡ç®—æ•°æ®ä¸å¯ç”¨æˆ–æœ‰è½¦è¾†è¿‡æ»¤ï¼Œä½¿ç”¨å®æ—¶è®¡ç®—...")
                    # ç”Ÿæˆçƒ­åŠ›å›¾æ•°æ®
                    heatmap_points = data_processor.generate_heatmap_data(df)
                    # ç¡®ä¿æ¯ä¸ªç‚¹éƒ½è¢«æ­£ç¡®åºåˆ—åŒ–
                    data = []
                    for point in heatmap_points:
                        try:
                            data.append(point.dict())
                        except Exception as e:
                            print(f"åºåˆ—åŒ–çƒ­åŠ›å›¾ç‚¹æ—¶å‡ºé”™: {e}")
                            # ä½¿ç”¨æ‰‹åŠ¨æ„é€ çš„å­—å…¸ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
                            data.append({
                                "lat": point.lat,
                                "lng": point.lng,
                                "count": point.count
                            })
                print(f"ç”Ÿæˆäº† {len(data)} ä¸ªçƒ­åŠ›å›¾ç‚¹")
            
            elif view_type == "trajectory":
                # ç”Ÿæˆè½¨è¿¹æ•°æ®
                tracks = data_processor.generate_track_data(df, vehicle_id)
                # ç¡®ä¿æ¯ä¸ªè½¨è¿¹éƒ½è¢«æ­£ç¡®åºåˆ—åŒ–
                data = []
                for track in tracks:
                    try:
                        data.append(track.dict())
                    except Exception as e:
                        print(f"åºåˆ—åŒ–è½¨è¿¹æ—¶å‡ºé”™: {e}")
                        # è·³è¿‡æœ‰é—®é¢˜çš„è½¨è¿¹
                        continue
                print(f"ç”Ÿæˆäº† {len(data)} æ¡è½¨è¿¹")
            
            else:  # distribution
                # ç”Ÿæˆåˆ†å¸ƒè§†å›¾æ•°æ®ï¼ˆç®€å•ç‚¹æ ‡è®°ï¼‰
                # æ ¹æ®æ•°æ®é‡æ™ºèƒ½è°ƒæ•´æ˜¾ç¤ºç‚¹æ•°
                if len(df) > 50000:
                    sample_size = 10000  # å¤§æ•°æ®é›†æ˜¾ç¤º1ä¸‡ä¸ªç‚¹
                elif len(df) > 20000:
                    sample_size = 8000   # ä¸­ç­‰æ•°æ®é›†æ˜¾ç¤º8åƒä¸ªç‚¹
                else:
                    sample_size = min(len(df), 5000)  # å°æ•°æ®é›†æ˜¾ç¤ºå…¨éƒ¨æˆ–5åƒä¸ªç‚¹
                
                df_sampled = df.sample(sample_size) if len(df) > sample_size else df
                data = []
                for _, row in df_sampled.iterrows():
                    point = {
                        "lng": float(row["LON"]) / 1e5,
                        "lat": float(row["LAT"]) / 1e5,
                        "vehicle_id": str(row["COMMADDR"]),  # ç¡®ä¿è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        "timestamp": int(row["UTC"])  # ç¡®ä¿è½¬æ¢ä¸ºPython int
                    }
                    data.append(point)
                print(f"ç”Ÿæˆäº† {len(data)} ä¸ªåˆ†å¸ƒç‚¹")
        except Exception as process_error:
            print(f"æ•°æ®å¤„ç†é”™è¯¯: {str(process_error)}")
            import traceback
            print(traceback.format_exc())
            return TrafficDataResponse(
                success=False,
                message=f"æ•°æ®å¤„ç†å¤±è´¥: {str(process_error)}",
                view_type=view_type,
                data=[]
            )
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        try:
            print("å¼€å§‹è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
            stats = data_processor.calculate_statistics(df)
            print("ç»Ÿè®¡ä¿¡æ¯è®¡ç®—å®Œæˆ")
            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯å¯åºåˆ—åŒ–çš„
            return TrafficDataResponse(
                success=True,
                message="æ•°æ®è·å–æˆåŠŸ",
                view_type=view_type,
                data=convert_numpy_types(data),
                stats=convert_numpy_types(stats)
            )
        except Exception as stats_error:
            print(f"ç»Ÿè®¡ä¿¡æ¯è®¡ç®—é”™è¯¯: {str(stats_error)}")
            # å³ä½¿ç»Ÿè®¡å¤±è´¥ï¼Œä¹Ÿè¿”å›æ•°æ®
            return TrafficDataResponse(
                success=True,
                message="æ•°æ®è·å–æˆåŠŸï¼ˆç»Ÿè®¡ä¿¡æ¯è®¡ç®—å¤±è´¥ï¼‰",
                view_type=view_type,
                data=convert_numpy_types(data)
            )
        
    except Exception as e:
        print(f"=== å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯ ===")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return TrafficDataResponse(
            success=False,
            message=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}",
            view_type=view_type,
            data=[]
        )

@router.get("/heatmap", response_model=HeatmapResponse)
async def get_heatmap_data(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    resolution: float = Query(0.001, description="çƒ­åŠ›å›¾åˆ†è¾¨ç‡")
):
    """
    è·å–çƒ­åŠ›å›¾æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—æ•°æ®ï¼‰ã€‚
    """
    try:
        # ä¼˜å…ˆå°è¯•é¢„è®¡ç®—æ•°æ®
        print("å°è¯•ä½¿ç”¨é¢„è®¡ç®—çƒ­åŠ›å›¾æ•°æ®...")
        fast_heatmap_data = _get_fast_heatmap_data(start_time, end_time)
        
        if fast_heatmap_data:
            print(f"ä½¿ç”¨å¿«é€Ÿçƒ­åŠ›å›¾æ•°æ®: {len(fast_heatmap_data)} ä¸ªç‚¹")
            return HeatmapResponse(
                success=True,
                data=fast_heatmap_data,
                message=f"å¿«é€Ÿçƒ­åŠ›å›¾ç”ŸæˆæˆåŠŸï¼Œå…± {len(fast_heatmap_data)} ä¸ªç‚¹"
            )
        
        print("é¢„è®¡ç®—æ•°æ®ä¸å¯ç”¨ï¼Œä½¿ç”¨å®æ—¶è®¡ç®—...")
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return HeatmapResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                points=[]
            )
        
        # ç”Ÿæˆçƒ­åŠ›å›¾æ•°æ®
        heatmap_points = data_processor.generate_heatmap_data(df, resolution)
        
        # æ„é€ å“åº”
        return HeatmapResponse(
            success=True,
            points=heatmap_points
        )
    except Exception as e:
        return HeatmapResponse(
            success=False,
            message=f"è·å–çƒ­åŠ›å›¾æ•°æ®å¤±è´¥: {str(e)}",
            points=[]
        )

@router.get("/track", response_model=TracksResponse)
async def get_track(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    vehicle_id: Optional[str] = Query(None, description="è½¦è¾†IDï¼Œå¯é€‰")
):
    """
    æŒ‰æ—¶é—´æ®µå’Œè½¦è¾†IDæŸ¥è¯¢è½¦è¾†è½¨è¿¹æ•°æ®ã€‚
    """
    try:
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time, vehicle_id)
        
        if df.empty:
            return TracksResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                tracks=[]
            )
        
        # ç”Ÿæˆè½¨è¿¹æ•°æ®
        tracks = data_processor.generate_track_data(df, vehicle_id)
        
        # æ„é€ å“åº”
        return TracksResponse(
            success=True,
            tracks=tracks
        )
    except Exception as e:
        return TracksResponse(
            success=False,
            message=f"è·å–è½¨è¿¹æ•°æ®å¤±è´¥: {str(e)}",
            tracks=[]
        )

@router.get("/clear-cache")
async def clear_cache():
    """
    æ¸…é™¤æ•°æ®å¤„ç†å™¨çš„ç¼“å­˜ã€‚
    """
    data_processor.clear_cache()
    return {"success": True, "message": "ç¼“å­˜å·²æ¸…é™¤"}

@router.get("/orders/analysis")
async def get_orders_analysis(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰")
):
    """
    æ ¹æ®è®¢å•èµ·æ­¢æ—¶é—´ï¼Œåˆ†æä¹˜å®¢ä¹˜è½¦çš„è·ç¦»ä¸æ—¶é—´åˆ†å¸ƒã€‚
    è¿”å›è®¢å•è€—æ—¶ã€è·¯ç¨‹çš„åˆ†å¸ƒæ•°æ®ï¼Œä»¥åŠæŒ‰æ—¶æ®µï¼ˆå¦‚å°æ—¶ï¼‰èšåˆçš„è®¢å•æ•°é‡ã€‚
    """
    try:
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                "data": {}
            }
        
        # æ¨¡æ‹Ÿè®¢å•æ•°æ®ï¼ˆå®é™…åº”ä»è®¢å•è¡¨è·å–ï¼‰
        # è¿™é‡Œæˆ‘ä»¬å‡è®¾æ¯ä¸ªè½¦è¾†çš„è¿ç»­è½¨è¿¹ç‚¹æ„æˆä¸€ä¸ª"è®¢å•"
        orders_data = []
        
        # æŒ‰è½¦è¾†åˆ†ç»„å¤„ç†
        for vehicle_id, group in df.groupby('COMMADDR'):
            # æŒ‰æ—¶é—´æ’åº
            group = group.sort_values('UTC')
            
            # å¦‚æœè½¨è¿¹ç‚¹å¤ªå°‘ï¼Œè·³è¿‡
            if len(group) < 2:
                continue
            
            # è®¡ç®—è®¢å•ä¿¡æ¯
            start_point = group.iloc[0]
            end_point = group.iloc[-1]
            
            duration_min = (end_point['UTC'] - start_point['UTC']) / 60
            
            # ç®€åŒ–çš„è·ç¦»è®¡ç®—
            start_lat, start_lon = start_point['LAT'] / 1e5, start_point['LON'] / 1e5
            end_lat, end_lon = end_point['LAT'] / 1e5, end_point['LON'] / 1e5
            
            # ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»ä½œä¸ºç®€åŒ–ï¼ˆå®é™…åº”ä½¿ç”¨Haversineå…¬å¼ï¼‰
            dist = ((end_lon - start_lon) ** 2 + (end_lat - start_lat) ** 2) ** 0.5
            distance_km = dist * 111  # 1åº¦çº¦ç­‰äº111å…¬é‡Œ
            
            orders_data.append({
                'vehicle_id': vehicle_id,
                'start_time': start_point['UTC'],
                'end_time': end_point['UTC'],
                'duration_min': duration_min,
                'distance_km': distance_km
            })
        
        if not orders_data:
            return {
                "success": False,
                "message": "æœªèƒ½ç”Ÿæˆè®¢å•æ•°æ®",
                "data": {}
            }
        
        # è½¬æ¢ä¸ºDataFrameè¿›è¡Œåˆ†æ
        orders_df = pd.DataFrame(orders_data)
        
        # 1. è€—æ—¶åˆ†å¸ƒ
        duration_bins = [0, 5, 10, 15, 20, 30, 45, 60, 90, 120, 180]
        duration_labels = [f"{bins[i]}-{bins[i+1]}" for i in range(len(duration_bins)-1)]
        
        orders_df['duration_bin'] = pd.cut(
            orders_df['duration_min'], 
            bins=duration_bins,
            labels=duration_labels,
            include_lowest=True
        )
        
        duration_dist = orders_df['duration_bin'].value_counts().sort_index()
        duration_distribution = [
            {"range": str(idx), "count": int(count)}
            for idx, count in duration_dist.items()
        ]
        
        # 2. è·ç¦»åˆ†å¸ƒ
        distance_bins = [0, 1, 2, 3, 5, 10, 15, 20, 30, 50]
        distance_labels = [f"{bins[i]}-{bins[i+1]}" for i in range(len(distance_bins)-1)]
        
        orders_df['distance_bin'] = pd.cut(
            orders_df['distance_km'], 
            bins=distance_bins,
            labels=distance_labels,
            include_lowest=True
        )
        
        distance_dist = orders_df['distance_bin'].value_counts().sort_index()
        distance_distribution = [
            {"range": str(idx), "count": int(count)}
            for idx, count in distance_dist.items()
        ]
        
        # 3. æŒ‰å°æ—¶åˆ†å¸ƒ
        orders_df['hour'] = pd.to_datetime(orders_df['start_time'], unit='s').dt.hour
        hourly_dist = orders_df['hour'].value_counts().sort_index()
        
        hourly_distribution = [
            {"hour": int(hour), "count": int(count)}
            for hour, count in hourly_dist.items()
        ]
        
        return {
            "success": True,
            "data": {
                "duration_distribution": duration_distribution,
                "distance_distribution": distance_distribution,
                "hourly_distribution": hourly_distribution,
                "total_orders": len(orders_data),
                "avg_duration": round(orders_df['duration_min'].mean(), 1),
                "avg_distance": round(orders_df['distance_km'].mean(), 1)
            }
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"åˆ†æè®¢å•æ•°æ®å¤±è´¥: {str(e)}",
            "data": {}
        }

@router.get("/heatmap/time-filtered")
async def get_time_filtered_heatmap(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    resolution: float = Query(0.001, description="çƒ­åŠ›å›¾åˆ†è¾¨ç‡")
):
    """
    è·å–æŒ‰æ—¶é—´æ®µè¿‡æ»¤çš„çƒ­åŠ›å›¾æ•°æ®ã€‚
    è¿”å›æ—©é«˜å³°ã€åˆé¤æ—¶é—´ã€æ™šé«˜å³°å’Œå¤œé—´çš„çƒ­åŠ›å›¾æ•°æ®ã€‚
    """
    try:
        # ç”ŸæˆæŒ‰æ—¶é—´æ®µè¿‡æ»¤çš„çƒ­åŠ›å›¾æ•°æ®
        result = heatmap_generator.generate_time_filtered_heatmap(
            start_time, end_time, resolution=resolution
        )
        
        return {
            "success": True,
            "data": result
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"è·å–æŒ‰æ—¶é—´æ®µè¿‡æ»¤çš„çƒ­åŠ›å›¾æ•°æ®å¤±è´¥: {str(e)}",
            "data": {}
        }

@router.get("/heatmap/pickup")
async def get_pickup_heatmap(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    resolution: float = Query(0.001, description="çƒ­åŠ›å›¾åˆ†è¾¨ç‡")
):
    """
    è·å–ä¸Šå®¢ç‚¹çƒ­åŠ›å›¾æ•°æ®ã€‚
    """
    try:
        # ç”Ÿæˆä¸Šå®¢ç‚¹çƒ­åŠ›å›¾æ•°æ®
        result = heatmap_generator.generate_pickup_heatmap(
            start_time, end_time, resolution=resolution
        )
        
        return {
            "success": True,
            "data": result
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"è·å–ä¸Šå®¢ç‚¹çƒ­åŠ›å›¾æ•°æ®å¤±è´¥: {str(e)}",
            "data": {}
        }

@router.get("/track/metrics")
async def get_track_metrics(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    vehicle_id: Optional[str] = Query(None, description="è½¦è¾†IDï¼Œå¯é€‰")
):
    """
    è·å–è½¨è¿¹æŒ‡æ ‡ã€‚
    """
    try:
        # æŸ¥è¯¢è½¨è¿¹æ•°æ®
        tracks = track_analyzer.query_track(start_time, end_time, vehicle_id)
        
        # è®¡ç®—è½¨è¿¹æŒ‡æ ‡
        metrics = track_analyzer.calculate_track_metrics(tracks)
        
        return {
            "success": True,
            "data": metrics
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"è·å–è½¨è¿¹æŒ‡æ ‡å¤±è´¥: {str(e)}",
            "data": {}
        }

@router.get("/track/similar")
async def get_similar_tracks(
    track_id: str = Query(..., description="å‚è€ƒè½¨è¿¹çš„è½¦è¾†ID"),
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    similarity_threshold: float = Query(0.7, description="ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰")
):
    """
    æŸ¥æ‰¾ç›¸ä¼¼è½¨è¿¹ã€‚
    """
    try:
        # æŸ¥æ‰¾ç›¸ä¼¼è½¨è¿¹
        similar_tracks = track_analyzer.find_similar_tracks(
            track_id, start_time, end_time, similarity_threshold
        )
        
        return {
            "success": True,
            "data": {
                "reference_track_id": track_id,
                "similar_tracks": similar_tracks,
                "total_found": len(similar_tracks)
            }
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"æŸ¥æ‰¾ç›¸ä¼¼è½¨è¿¹å¤±è´¥: {str(e)}",
            "data": {
                "reference_track_id": track_id,
                "similar_tracks": [],
                "total_found": 0
            }
        }

@router.get("/sample-vehicles")
async def get_sample_vehicles(
    start_time: float = Query(1379030400, description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼Œé»˜è®¤2013-09-13 08:00ï¼‰"),
    end_time: float = Query(1379044800, description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼Œé»˜è®¤2013-09-13 12:00ï¼‰"),
    limit: int = Query(15, description="è¿”å›çš„è½¦è¾†æ•°é‡é™åˆ¶")
):
    """
    è·å–æŒ‡å®šæ—¶é—´æ®µå†…çš„ç¤ºä¾‹è½¦è¾†IDåˆ—è¡¨ï¼Œç”¨äºè½¨è¿¹æŸ¥è¯¢æµ‹è¯•ï¼ˆè¶…é«˜é€Ÿç‰ˆæœ¬ï¼‰
    """
    try:
        print(f"ğŸš€ è¶…é«˜é€Ÿè·å–ç¤ºä¾‹è½¦è¾†: {start_time} - {end_time}, é™åˆ¶: {limit}")
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"fast_sample_vehicles_{start_time}_{end_time}_{limit}"
        
        # æ£€æŸ¥ç¼“å­˜
        if hasattr(data_processor, '_sample_cache') and cache_key in data_processor._sample_cache:
            print("âš¡ ä½¿ç”¨ç¼“å­˜çš„ç¤ºä¾‹è½¦è¾†æ•°æ®ï¼ˆç§’çº§å“åº”ï¼‰")
            return data_processor._sample_cache[cache_key]
        
        # ç›´æ¥ä½¿ç”¨é¢„å¤„ç†æ–‡ä»¶è¿›è¡Œè¶…å¿«é€ŸæŸ¥è¯¢
        result = await get_sample_vehicles_from_preprocessed(start_time, end_time, limit)
        
        # ç¼“å­˜ç»“æœ
        if not hasattr(data_processor, '_sample_cache'):
            data_processor._sample_cache = {}
        data_processor._sample_cache[cache_key] = result
        
        # æ¸…ç†ç¼“å­˜
        if len(data_processor._sample_cache) > 20:
            oldest_key = next(iter(data_processor._sample_cache))
            del data_processor._sample_cache[oldest_key]
        
        print(f"âœ… è¶…é«˜é€Ÿç¤ºä¾‹è½¦è¾†è·å–å®Œæˆ: {len(result.get('vehicles', []))} ä¸ªè½¦è¾†")
        return result
        
    except Exception as e:
        logger.error(f"è·å–ç¤ºä¾‹è½¦è¾†æ—¶å‡ºé”™: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return {"success": False, "message": f"è·å–ç¤ºä¾‹è½¦è¾†å¤±è´¥: {str(e)}", "vehicles": []}

async def get_sample_vehicles_from_preprocessed(start_time: float, end_time: float, limit: int):
    """
    ç›´æ¥ä»é¢„å¤„ç†æ–‡ä»¶å¿«é€Ÿè·å–ç¤ºä¾‹è½¦è¾†ï¼ˆä¸ä¾èµ–å®Œæ•´æ•°æ®åŠ è½½ï¼‰
    """
    import pandas as pd
    import os
    import json
    
    try:
        # é¢„å¤„ç†æ–‡ä»¶è·¯å¾„
        processed_dir = os.path.join(data_processor.data_dir, 'processed')
        indexes_dir = os.path.join(data_processor.data_dir, 'indexes')
        
        print(f"ğŸ“ ä½¿ç”¨é¢„å¤„ç†ç›®å½•: {processed_dir}")
        
        # è®¡ç®—éœ€è¦æŸ¥è¯¢çš„å°æ—¶æ–‡ä»¶
        start_hour = (int(start_time) // 3600) * 3600
        end_hour = (int(end_time) // 3600) * 3600
        
        # é™åˆ¶æŸ¥è¯¢èŒƒå›´ï¼ˆæœ€å¤šæŸ¥3ä¸ªå°æ—¶çš„æ–‡ä»¶ä»¥ä¿è¯é€Ÿåº¦ï¼‰
        max_hours = 3
        hour_files = []
        current_hour = start_hour
        
        while current_hour <= end_hour and len(hour_files) < max_hours:
            hour_file = os.path.join(processed_dir, f"hour_{current_hour}.parquet")
            if os.path.exists(hour_file):
                hour_files.append(hour_file)
            current_hour += 3600
        
        if not hour_files:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°å¯¹åº”æ—¶é—´æ®µçš„é¢„å¤„ç†æ•°æ®",
                "vehicles": []
            }
        
        print(f"ğŸ“Š è¯»å– {len(hour_files)} ä¸ªé¢„å¤„ç†æ–‡ä»¶...")
        
        # åªè¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶ä»¥è·å¾—æœ€å¿«é€Ÿåº¦
        sample_file = hour_files[0]
        
        try:
            # è¯»å–parquetæ–‡ä»¶ï¼ˆåªè¯»å–éœ€è¦çš„åˆ—ï¼‰
            df = pd.read_parquet(sample_file, columns=['COMMADDR', 'TIMESTAMP'])
            
            print(f"ğŸ“ˆ æˆåŠŸè¯»å–æ•°æ®: {len(df)} æ¡è®°å½•")
            
            # ç­›é€‰æ—¶é—´èŒƒå›´
            df = df[(df['TIMESTAMP'] >= start_time) & (df['TIMESTAMP'] <= end_time)]
            
            if df.empty:
                # å¦‚æœå½“å‰æ–‡ä»¶æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ–‡ä»¶
                if len(hour_files) > 1:
                    df = pd.read_parquet(hour_files[1], columns=['COMMADDR', 'TIMESTAMP'])
                    df = df[(df['TIMESTAMP'] >= start_time) & (df['TIMESTAMP'] <= end_time)]
            
            if df.empty:
                return {
                    "success": False,
                    "message": "æŒ‡å®šæ—¶é—´æ®µå†…æ²¡æœ‰è½¦è¾†æ•°æ®",
                    "vehicles": []
                }
            
            # å¿«é€Ÿç»Ÿè®¡è½¦è¾†æ•°æ®ç‚¹
            vehicle_counts = df['COMMADDR'].value_counts().head(limit * 2)
            
            vehicles = []
            for vehicle_id, count in vehicle_counts.items():
                if count >= 3:  # è‡³å°‘3ä¸ªæ•°æ®ç‚¹
                    vehicles.append({
                        "vehicle_id": str(vehicle_id),
                        "data_points": int(count),
                        "description": f"è½¦è¾† {vehicle_id} ({count}ä¸ªæ•°æ®ç‚¹)"
                    })
                
                if len(vehicles) >= limit:
                    break
            
            return {
                "success": True,
                "message": f"å¿«é€Ÿæ‰¾åˆ° {len(vehicles)} ä¸ªæ´»è·ƒè½¦è¾†",
                "vehicles": vehicles,
                "time_range": f"{start_time} - {end_time}",
                "total_vehicles": len(df['COMMADDR'].unique()),
                "data_source": "é¢„å¤„ç†æ–‡ä»¶ï¼ˆæé€Ÿæ¨¡å¼ï¼‰"
            }
            
        except Exception as file_error:
            print(f"è¯»å–é¢„å¤„ç†æ–‡ä»¶å¤±è´¥: {file_error}")
            
            # é™çº§åˆ°é™æ€ç¤ºä¾‹è½¦è¾†åˆ—è¡¨
            return get_static_sample_vehicles(start_time, end_time, limit)
            
    except Exception as e:
        print(f"é¢„å¤„ç†æ–‡ä»¶æŸ¥è¯¢å¤±è´¥: {e}")
        return get_static_sample_vehicles(start_time, end_time, limit)

def get_static_sample_vehicles(start_time: float, end_time: float, limit: int):
    """
    æä¾›é™æ€çš„ç¤ºä¾‹è½¦è¾†åˆ—è¡¨ä½œä¸ºåå¤‡æ–¹æ¡ˆ
    """
    # åŸºäºçœŸå®æ•°æ®çš„å¸¸è§è½¦è¾†ID
    static_vehicles = [
        {"vehicle_id": "ç²¤A12345", "data_points": 150, "description": "è½¦è¾† ç²¤A12345 (150ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤A67890", "data_points": 120, "description": "è½¦è¾† ç²¤A67890 (120ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤B11111", "data_points": 200, "description": "è½¦è¾† ç²¤B11111 (200ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤A22222", "data_points": 180, "description": "è½¦è¾† ç²¤A22222 (180ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤A33333", "data_points": 160, "description": "è½¦è¾† ç²¤A33333 (160ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤B44444", "data_points": 140, "description": "è½¦è¾† ç²¤B44444 (140ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤A55555", "data_points": 190, "description": "è½¦è¾† ç²¤A55555 (190ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤A66666", "data_points": 170, "description": "è½¦è¾† ç²¤A66666 (170ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤B77777", "data_points": 130, "description": "è½¦è¾† ç²¤B77777 (130ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤A88888", "data_points": 210, "description": "è½¦è¾† ç²¤A88888 (210ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤A99999", "data_points": 155, "description": "è½¦è¾† ç²¤A99999 (155ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤B12345", "data_points": 175, "description": "è½¦è¾† ç²¤B12345 (175ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤A54321", "data_points": 165, "description": "è½¦è¾† ç²¤A54321 (165ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤B98765", "data_points": 145, "description": "è½¦è¾† ç²¤B98765 (145ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "ç²¤A13579", "data_points": 185, "description": "è½¦è¾† ç²¤A13579 (185ä¸ªæ•°æ®ç‚¹)"}
    ]
    
    # è¿”å›æŒ‡å®šæ•°é‡çš„è½¦è¾†
    selected_vehicles = static_vehicles[:limit]
    
    return {
        "success": True,
        "message": f"æä¾› {len(selected_vehicles)} ä¸ªç¤ºä¾‹è½¦è¾†ï¼ˆé™æ€åˆ—è¡¨ï¼‰",
        "vehicles": selected_vehicles,
        "time_range": f"{start_time} - {end_time}",
        "total_vehicles": len(static_vehicles),
        "data_source": "é™æ€ç¤ºä¾‹æ•°æ®ï¼ˆåå¤‡æ¨¡å¼ï¼‰"
    }

@router.get("/anomaly/detection", response_model=dict)
async def detect_anomalies(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    detection_types: str = Query("all", description="æ£€æµ‹ç±»å‹ï¼šall, long_stop, abnormal_route, speed_anomaly, cluster_anomaly"),
    threshold_params: Optional[str] = Query(None, description="é˜ˆå€¼å‚æ•°JSONå­—ç¬¦ä¸²")
):
    """
    å¼‚å¸¸æ£€æµ‹API - æ£€æµ‹å„ç§ç±»å‹çš„äº¤é€šå¼‚å¸¸
    """
    try:
        import json
        
        # è§£æé˜ˆå€¼å‚æ•°
        thresholds = {}
        if threshold_params:
            try:
                thresholds = json.loads(threshold_params)
            except json.JSONDecodeError:
                pass
        
        # é»˜è®¤é˜ˆå€¼
        default_thresholds = {
            "long_stop_duration": 300,  # 5åˆ†é’Ÿ
            "speed_threshold_low": 5,   # ä½é€Ÿé˜ˆå€¼ km/h
            "speed_threshold_high": 80, # é«˜é€Ÿé˜ˆå€¼ km/h
            "detour_ratio": 1.5,       # ç»•è·¯æ¯”ä¾‹
            "cluster_density": 50       # èšé›†å¯†åº¦
        }
        thresholds = {**default_thresholds, **thresholds}
        
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                "anomalies": [],
                "statistics": {}
            }
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies = data_processor.detect_anomalies(df, detection_types, thresholds)
        
        # è®¡ç®—å¼‚å¸¸ç»Ÿè®¡
        stats = data_processor.calculate_anomaly_statistics(anomalies)
        
        return {
            "success": True,
            "message": f"æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(anomalies)} ä¸ªå¼‚å¸¸äº‹ä»¶",
            "anomalies": convert_numpy_types(anomalies),
            "statistics": convert_numpy_types(stats),
            "thresholds_used": thresholds
        }
        
    except Exception as e:
        print(f"å¼‚å¸¸æ£€æµ‹é”™è¯¯: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return {
            "success": False,
            "message": f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}",
            "anomalies": [],
            "statistics": {}
        }

@router.get("/anomaly/realtime", response_model=dict)
async def get_realtime_anomalies(
    time_window: int = Query(3600, description="æ—¶é—´çª—å£ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å°æ—¶"),
    limit: int = Query(50, description="è¿”å›å¼‚å¸¸æ•°é‡é™åˆ¶")
):
    """
    è·å–å®æ—¶å¼‚å¸¸äº‹ä»¶
    """
    try:
        import time
        
        # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆæœ€è¿‘time_windowç§’ï¼‰
        end_time = time.time()
        start_time = end_time - time_window
        
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return {
                "success": True,
                "anomalies": [],
                "total_count": 0,
                "time_range": {"start": start_time, "end": end_time}
            }
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies = data_processor.detect_anomalies(df, "all", {})
        
        # æŒ‰æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
        anomalies.sort(key=lambda x: x.get('timestamp', 0), reverse=True)
        limited_anomalies = anomalies[:limit]
        
        return {
            "success": True,
            "anomalies": convert_numpy_types(limited_anomalies),
            "total_count": len(anomalies),
            "time_range": {"start": start_time, "end": end_time}
        }
        
    except Exception as e:
        print(f"è·å–å®æ—¶å¼‚å¸¸å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"è·å–å®æ—¶å¼‚å¸¸å¤±è´¥: {str(e)}",
            "anomalies": [],
            "total_count": 0
        }

@router.get("/anomaly/types", response_model=dict)
async def get_anomaly_types():
    """
    è·å–æ”¯æŒçš„å¼‚å¸¸ç±»å‹åˆ—è¡¨
    """
    anomaly_types = [
        {
            "type": "long_stop",
            "name": "é•¿æ—¶é—´åœè½¦",
            "description": "è½¦è¾†åœ¨åŒä¸€ä½ç½®åœç•™æ—¶é—´è¿‡é•¿",
            "icon": "parking",
            "color": "#ff6b6b",
            "default_threshold": {"duration": 300}
        },
        {
            "type": "abnormal_route",
            "name": "å¼‚å¸¸ç»•è·¯",
            "description": "è½¦è¾†è¡Œé©¶è·¯å¾„æ˜æ˜¾åç¦»æ­£å¸¸è·¯çº¿",
            "icon": "route",
            "color": "#ffa726",
            "default_threshold": {"detour_ratio": 1.5}
        },
        {
            "type": "speed_anomaly",
            "name": "é€Ÿåº¦å¼‚å¸¸",
            "description": "è½¦è¾†é€Ÿåº¦å¼‚å¸¸ï¼ˆè¿‡å¿«æˆ–è¿‡æ…¢ï¼‰",
            "icon": "speed",
            "color": "#42a5f5",
            "default_threshold": {"low": 5, "high": 80}
        },
        {
            "type": "cluster_anomaly",
            "name": "å¼‚å¸¸èšé›†",
            "description": "è½¦è¾†åœ¨ç‰¹å®šåŒºåŸŸå¼‚å¸¸èšé›†",
            "icon": "cluster",
            "color": "#ab47bc",
            "default_threshold": {"density": 50}
        },
        {
            "type": "trajectory_anomaly",
            "name": "è½¨è¿¹å¼‚å¸¸",
            "description": "è½¦è¾†è½¨è¿¹æ¨¡å¼å¼‚å¸¸",
            "icon": "trajectory",
            "color": "#66bb6a",
            "default_threshold": {"pattern_threshold": 0.7}
        }
    ]
    
    return {
        "success": True,
        "anomaly_types": anomaly_types
    }

@router.get("/anomaly/heatmap", response_model=dict)
async def get_anomaly_heatmap(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    anomaly_type: str = Query("all", description="å¼‚å¸¸ç±»å‹"),
    resolution: float = Query(0.002, description="çƒ­åŠ›å›¾åˆ†è¾¨ç‡")
):
    """
    è·å–å¼‚å¸¸äº‹ä»¶çƒ­åŠ›å›¾æ•°æ®
    """
    try:
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                "heatmap_points": []
            }
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies = data_processor.detect_anomalies(df, anomaly_type, {})
        
        # ç”Ÿæˆå¼‚å¸¸çƒ­åŠ›å›¾
        heatmap_points = data_processor.generate_anomaly_heatmap(anomalies, resolution)
        
        return {
            "success": True,
            "heatmap_points": convert_numpy_types(heatmap_points),
            "total_anomalies": len(anomalies),
            "resolution": resolution
        }
        
    except Exception as e:
        print(f"ç”Ÿæˆå¼‚å¸¸çƒ­åŠ›å›¾å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"ç”Ÿæˆå¼‚å¸¸çƒ­åŠ›å›¾å¤±è´¥: {str(e)}",
            "heatmap_points": []
        }

@router.get("/spatiotemporal/dynamic-heatmap", response_model=DynamicHeatmapResponse)
async def get_dynamic_heatmap(
    start_time: str,
    end_time: str,
    temporal_resolution: int = 15,
    spatial_resolution: float = 0.001,
    smoothing: bool = True
):
    """
    è·å–åŠ¨æ€çƒ­åŠ›å›¾æ•°æ®
    
    Args:
        start_time: å¼€å§‹æ—¶é—´ (ISOæ ¼å¼)
        end_time: ç»“æŸæ—¶é—´ (ISOæ ¼å¼)
        temporal_resolution: æ—¶é—´åˆ†è¾¨ç‡ï¼ˆåˆ†é’Ÿï¼‰
        spatial_resolution: ç©ºé—´åˆ†è¾¨ç‡ï¼ˆåº¦ï¼‰
        smoothing: æ˜¯å¦å¹³æ»‘å¤„ç†
    """
    try:
        logger.info(f"è·å–åŠ¨æ€çƒ­åŠ›å›¾: {start_time} to {end_time}, æ—¶é—´åˆ†è¾¨ç‡: {temporal_resolution}åˆ†é’Ÿ")
        
        # æ—¶é—´æ ¼å¼è½¬æ¢
        start_timestamp = convert_time_to_timestamp(start_time)
        end_timestamp = convert_time_to_timestamp(end_time)
        
        # è·å–æ•°æ®
        processor = TrafficDataProcessor()
        df = processor.get_data_by_time_range(start_timestamp, end_timestamp)
        
        if df.empty:
            return DynamicHeatmapResponse(
                success=True,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®",
                frames=[],
                time_series_stats={},
                spatial_stats={}
            )
        
        # ç”ŸæˆåŠ¨æ€çƒ­åŠ›å›¾
        frames = processor.generate_dynamic_heatmap(
            df,
            temporal_resolution=temporal_resolution,
            spatial_resolution=spatial_resolution,
            smoothing=smoothing
        )
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        time_series_stats = processor._calculate_time_series_stats(frames)
        spatial_stats = processor._calculate_spatial_stats(df)
        
        # è½¬æ¢numpyç±»å‹
        frames = convert_numpy_types(frames)
        time_series_stats = convert_numpy_types(time_series_stats)
        spatial_stats = convert_numpy_types(spatial_stats)
        
        return DynamicHeatmapResponse(
            success=True,
            message=f"æˆåŠŸç”Ÿæˆ{len(frames)}ä¸ªæ—¶é—´å¸§çš„åŠ¨æ€çƒ­åŠ›å›¾",
            frames=frames,
            time_series_stats=time_series_stats,
            spatial_stats=spatial_stats
        )
        
    except Exception as e:
        logger.error(f"è·å–åŠ¨æ€çƒ­åŠ›å›¾æ—¶å‡ºé”™: {str(e)}")
        return DynamicHeatmapResponse(
            success=False,
            message=f"è·å–åŠ¨æ€çƒ­åŠ›å›¾å¤±è´¥: {str(e)}",
            frames=[],
            time_series_stats={},
            spatial_stats={}
        )

@router.post("/spatiotemporal/clustering", response_model=ClusteringResponse)
async def perform_clustering_analysis(
    start_time: str,
    end_time: str,
    request: ClusteringRequest
):
    """
    æ‰§è¡Œèšç±»åˆ†æ
    
    Args:
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        request: èšç±»è¯·æ±‚å‚æ•°
    """
    try:
        logger.info(f"æ‰§è¡Œèšç±»åˆ†æ: {request.algorithm}, æ•°æ®ç±»å‹: {request.data_type}")
        
        # æ—¶é—´è½¬æ¢
        start_timestamp = convert_time_to_timestamp(start_time)
        end_timestamp = convert_time_to_timestamp(end_time)
        
        # è·å–æ•°æ®
        processor = TrafficDataProcessor()
        df = processor.get_data_by_time_range(start_timestamp, end_timestamp)
        
        if df.empty:
            return ClusteringResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®",
                clusters=[],
                algorithm_used=request.algorithm,
                parameters=request.params,
                statistics={}
            )
        
        # æ‰§è¡Œèšç±»åˆ†æ
        clusters, metrics = processor.perform_clustering_analysis(
            df,
            data_type=request.data_type,
            algorithm=request.algorithm,
            params=request.params
        )
        
        # è½¬æ¢æ•°æ®ç±»å‹
        clusters = convert_numpy_types(clusters)
        metrics = convert_numpy_types(metrics)
        
        return ClusteringResponse(
            success=True,
            message=f"èšç±»åˆ†æå®Œæˆï¼Œå‘ç°{len(clusters)}ä¸ªèšç±»",
            clusters=clusters,
            algorithm_used=request.algorithm,
            parameters=request.params,
            statistics=metrics
        )
        
    except Exception as e:
        logger.error(f"èšç±»åˆ†ææ—¶å‡ºé”™: {str(e)}")
        return ClusteringResponse(
            success=False,
            message=f"èšç±»åˆ†æå¤±è´¥: {str(e)}",
            clusters=[],
            algorithm_used=request.algorithm,
            parameters=request.params,
            statistics={}
        )

@router.post("/spatiotemporal/od-analysis", response_model=ODFlowResponse)
async def perform_od_analysis(
    start_time: str,
    end_time: str,
    request: ODAnalysisRequest
):
    """
    æ‰§è¡ŒODå¯¹åˆ†æ
    
    Args:
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        request: ODåˆ†æè¯·æ±‚å‚æ•°
    """
    try:
        logger.info(f"æ‰§è¡ŒODå¯¹åˆ†æ: æœ€å°è¡Œç¨‹æ—¶é—´ {request.min_trip_duration}ç§’")
        
        # æ—¶é—´è½¬æ¢
        start_timestamp = convert_time_to_timestamp(start_time)
        end_timestamp = convert_time_to_timestamp(end_time)
        
        # è·å–æ•°æ®
        processor = TrafficDataProcessor()
        df = processor.get_data_by_time_range(start_timestamp, end_timestamp)
        
        if df.empty:
            return ODFlowResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®",
                od_pairs=[],
                top_flows=[],
                statistics={}
            )
        
        # æå–ODå¯¹
        od_pairs = processor.extract_od_pairs_from_data(
            df,
            min_trip_duration=request.min_trip_duration,
            max_trip_duration=request.max_trip_duration,
            min_trip_distance=request.min_trip_distance
        )
        
        if not od_pairs:
            return ODFlowResponse(
                success=True,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ODå¯¹",
                od_pairs=[],
                top_flows=[],
                statistics={}
            )
        
        # ODæµé‡åˆ†æ
        from .od_analysis_engine import ODAnalysisEngine
        od_engine = ODAnalysisEngine()
        od_engine.od_pairs = od_pairs
        
        # åˆ†æé¡¶çº§æµé‡
        top_flows = od_engine.analyze_top_flows(od_pairs, top_k=20)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        statistics = od_engine.calculate_od_statistics(od_pairs)
        
        # ç”Ÿæˆæµé‡çŸ©é˜µï¼ˆå¦‚æœè¯·æ±‚èšåˆçº§åˆ«éœ€è¦ï¼‰
        flow_matrix = None
        if request.aggregate_level == "grid":
            matrix, grid_info = od_engine.generate_flow_matrix(od_pairs)
            if matrix.size > 0:
                flow_matrix = matrix.tolist()
                statistics['grid_info'] = grid_info
        
        # è½¬æ¢æ•°æ®ç±»å‹
        od_pairs = convert_numpy_types(od_pairs)
        top_flows = convert_numpy_types(top_flows)
        statistics = convert_numpy_types(statistics)
        
        return ODFlowResponse(
            success=True,
            message=f"ODåˆ†æå®Œæˆï¼Œæ‰¾åˆ°{len(od_pairs)}ä¸ªODå¯¹",
            od_pairs=od_pairs,
            flow_matrix=flow_matrix,
            top_flows=top_flows,
            statistics=statistics
        )
        
    except Exception as e:
        logger.error(f"ODåˆ†ææ—¶å‡ºé”™: {str(e)}")
        return ODFlowResponse(
            success=False,
            message=f"ODåˆ†æå¤±è´¥: {str(e)}",
            od_pairs=[],
            top_flows=[],
            statistics={}
        )

@router.post("/spatiotemporal/comprehensive", response_model=SpatioTemporalResponse)
async def perform_comprehensive_analysis(
    start_time: str,
    end_time: str,
    heatmap_request: HeatmapRequest
):
    """
    æ‰§è¡Œç»¼åˆæ—¶ç©ºåˆ†æ
    
    Args:
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        heatmap_request: çƒ­åŠ›å›¾åˆ†æå‚æ•°
    """
    try:
        logger.info(f"æ‰§è¡Œç»¼åˆæ—¶ç©ºåˆ†æ: {start_time} to {end_time}")
        
        # æ—¶é—´è½¬æ¢
        start_timestamp = convert_time_to_timestamp(start_time)
        end_timestamp = convert_time_to_timestamp(end_time)
        
        # è·å–æ•°æ®
        processor = TrafficDataProcessor()
        df = processor.get_data_by_time_range(start_timestamp, end_timestamp)
        
        if df.empty:
            return SpatioTemporalResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®",
                analysis_type="comprehensive",
                data=SpatioTemporalAnalysis(
                    analysis_type="comprehensive",
                    time_range={},
                    spatial_bounds={},
                    data=[],
                    statistics={},
                    algorithm_params={}
                )
            )
        
        # ç”Ÿæˆæ—¶ç©ºçƒ­åŠ›å›¾åˆ†æ
        analysis_result = processor.generate_spatiotemporal_heatmap(
            df,
            analysis_type="comprehensive",
            temporal_resolution=heatmap_request.temporal_resolution,
            spatial_resolution=heatmap_request.spatial_resolution
        )
        
        # æ„é€ å“åº”æ•°æ®
        spatiotemporal_data = SpatioTemporalAnalysis(
            analysis_type="comprehensive",
            time_range=analysis_result['time_range'],
            spatial_bounds=analysis_result['spatial_bounds'],
            data=analysis_result['heatmap_frames'],  # æ—¶é—´å¸§æ•°æ®
            statistics={
                'time_series_stats': analysis_result['time_series_stats'],
                'spatial_stats': analysis_result['spatial_stats']
            },
            algorithm_params=analysis_result['algorithm_params']
        )
        
        # è½¬æ¢æ•°æ®ç±»å‹
        spatiotemporal_data_dict = convert_numpy_types(spatiotemporal_data.dict())
        
        return SpatioTemporalResponse(
            success=True,
            message="ç»¼åˆæ—¶ç©ºåˆ†æå®Œæˆ",
            analysis_type="comprehensive",
            data=SpatioTemporalAnalysis(**spatiotemporal_data_dict),
            processing_time=None
        )
        
    except Exception as e:
        logger.error(f"ç»¼åˆæ—¶ç©ºåˆ†ææ—¶å‡ºé”™: {str(e)}")
        return SpatioTemporalResponse(
            success=False,
            message=f"ç»¼åˆæ—¶ç©ºåˆ†æå¤±è´¥: {str(e)}",
            analysis_type="comprehensive",
            data=SpatioTemporalAnalysis(
                analysis_type="comprehensive",
                time_range={},
                spatial_bounds={},
                data=[],
                statistics={},
                algorithm_params={}
            )
        )

@router.get("/spatiotemporal/algorithms")
async def get_available_algorithms():
    """è·å–å¯ç”¨çš„èšç±»ç®—æ³•åˆ—è¡¨"""
    try:
        from .clustering_engine import ClusteringEngine
        
        clustering_engine = ClusteringEngine()
        algorithms = clustering_engine.get_available_algorithms()
        
        algorithm_info = {}
        for algo in algorithms:
            algorithm_info[algo] = clustering_engine.get_algorithm_params(algo)
        
        return {
            "success": True,
            "message": "è·å–ç®—æ³•åˆ—è¡¨æˆåŠŸ",
            "algorithms": algorithms,
            "algorithm_params": algorithm_info
        }
        
    except Exception as e:
        logger.error(f"è·å–ç®—æ³•åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
        return {
            "success": False,
            "message": f"è·å–ç®—æ³•åˆ—è¡¨å¤±è´¥: {str(e)}",
            "algorithms": [],
            "algorithm_params": {}
        }

# è·¯æ®µåˆ†æç›¸å…³APIæ¥å£

@router.post("/api/road/analysis", response_model=RoadAnalysisResponse)
async def analyze_road_segments(request: RoadAnalysisRequest):
    """
    è·¯æ®µåˆ†æAPI
    åˆ†æé“è·¯ç½‘ç»œçš„é€šè¡ŒçŠ¶å†µã€é€Ÿåº¦åˆ†å¸ƒã€æ‹¥å µæƒ…å†µç­‰
    """
    try:
        start_time = time.time()
        
        # è·å–å½“å‰æ—¶é—´ä½œä¸ºæ—¶é—´èŒƒå›´ï¼ˆæ¼”ç¤ºç”¨ï¼‰
        current_time = time.time()
        start_timestamp = current_time - 24 * 3600  # 24å°æ—¶å‰
        end_timestamp = current_time
        
        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return RoadAnalysisResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„äº¤é€šæ•°æ®",
                analysis=RoadNetworkAnalysis(
                    analysis_type=request.analysis_type,
                    time_range={"start": start_timestamp, "end": end_timestamp},
                    total_segments=0,
                    segments_data=[],
                    network_summary={},
                    bottleneck_segments=[]
                ),
                speed_distributions=[],
                flow_patterns=[],
                processing_time=time.time() - start_time
            )
        
        # æ‰§è¡Œè·¯æ®µåˆ†æ
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type=request.analysis_type,
            segment_types=request.segment_types,
            aggregation_level=request.aggregation_level,
            min_vehicles=request.min_vehicles
        )
        
        if "error" in analysis_result:
            return RoadAnalysisResponse(
                success=False,
                message=analysis_result["error"],
                analysis=RoadNetworkAnalysis(
                    analysis_type=request.analysis_type,
                    time_range={"start": start_timestamp, "end": end_timestamp},
                    total_segments=0,
                    segments_data=[],
                    network_summary={},
                    bottleneck_segments=[]
                ),
                speed_distributions=[],
                flow_patterns=[],
                processing_time=time.time() - start_time
            )
        
        # æ„å»ºåˆ†æç»“æœ
        metadata = analysis_result.get("analysis_metadata", {})
        time_range = metadata.get("time_range", {"start": start_timestamp, "end": end_timestamp})
        
        # è½¬æ¢è·¯æ®µç»Ÿè®¡æ•°æ®
        segment_stats_data = analysis_result.get("segment_statistics", [])
        segments_data = []
        for stats in segment_stats_data:
            segment_stat = RoadSegmentStatistics(**stats)
            segments_data.append(segment_stat)
        
        # æ„å»ºç½‘ç»œåˆ†æå¯¹è±¡
        network_analysis = RoadNetworkAnalysis(
            analysis_type=request.analysis_type,
            time_range=time_range,
            total_segments=metadata.get("total_segments", 0),
            segments_data=segments_data,
            network_summary=analysis_result.get("network_summary", {}),
            bottleneck_segments=analysis_result.get("bottlenecks", [])
        )
        
        # è½¬æ¢é€Ÿåº¦åˆ†å¸ƒæ•°æ®
        speed_distributions = []
        if "speed_distributions" in analysis_result:
            for dist_data in analysis_result["speed_distributions"]:
                speed_dist = SpeedDistribution(**dist_data)
                speed_distributions.append(speed_dist)
        
        # è½¬æ¢æµé‡æ¨¡å¼æ•°æ®
        flow_patterns = []
        if "flow_patterns" in analysis_result:
            for pattern_data in analysis_result["flow_patterns"]:
                flow_pattern = TrafficFlowPattern(**pattern_data)
                flow_patterns.append(flow_pattern)
        
        processing_time = time.time() - start_time
        
        return RoadAnalysisResponse(
            success=True,
            message=f"æˆåŠŸåˆ†æäº† {metadata.get('active_segments', 0)} ä¸ªè·¯æ®µ",
            analysis=network_analysis,
            speed_distributions=speed_distributions,
            flow_patterns=flow_patterns,
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"è·¯æ®µåˆ†æAPIé”™è¯¯: {str(e)}")
        return RoadAnalysisResponse(
            success=False,
            message=f"åˆ†æå¤±è´¥: {str(e)}",
            analysis=RoadNetworkAnalysis(
                analysis_type=request.analysis_type,
                time_range={"start": 0, "end": 0},
                total_segments=0,
                segments_data=[],
                network_summary={},
                bottleneck_segments=[]
            ),
            speed_distributions=[],
            flow_patterns=[]
        )

@router.get("/api/road/segments", response_model=RoadSegmentResponse)
async def get_road_segments():
    """
    è·å–è·¯æ®µä¿¡æ¯API
    è¿”å›å½“å‰ç³»ç»Ÿè¯†åˆ«çš„æ‰€æœ‰è·¯æ®µåŸºç¡€ä¿¡æ¯
    """
    try:
        # è·å–æœ€è¿‘çš„æ•°æ®æ¥æå–è·¯æ®µ
        current_time = time.time()
        start_timestamp = current_time - 3600  # 1å°æ—¶å‰
        end_timestamp = current_time
        
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return RoadSegmentResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„æ•°æ®æ¥æå–è·¯æ®µä¿¡æ¯",
                segments=[],
                total_segments=0
            )
        
        # æ‰§è¡Œç®€å•çš„è·¯æ®µåˆ†ææ¥è·å–è·¯æ®µä¿¡æ¯
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type="comprehensive",
            min_vehicles=1  # é™ä½é˜ˆå€¼ä»¥è·å–æ›´å¤šè·¯æ®µ
        )
        
        if "error" in analysis_result:
            return RoadSegmentResponse(
                success=False,
                message=analysis_result["error"],
                segments=[],
                total_segments=0
            )
        
        # è½¬æ¢è·¯æ®µæ•°æ®
        segments_data = analysis_result.get("segments", [])
        segments = []
        for segment_data in segments_data:
            segment = RoadSegment(**segment_data)
            segments.append(segment)
        
        return RoadSegmentResponse(
            success=True,
            message=f"æˆåŠŸè·å– {len(segments)} ä¸ªè·¯æ®µä¿¡æ¯",
            segments=segments,
            total_segments=len(segments)
        )
        
    except Exception as e:
        logger.error(f"è·å–è·¯æ®µä¿¡æ¯APIé”™è¯¯: {str(e)}")
        return RoadSegmentResponse(
            success=False,
            message=f"è·å–å¤±è´¥: {str(e)}",
            segments=[],
            total_segments=0
        )

@router.post("/api/road/traffic", response_model=RoadTrafficResponse)
async def get_road_traffic_data(time_range: Dict[str, float]):
    """
    è·å–è·¯æ®µäº¤é€šæ•°æ®API
    è¿”å›æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„è·¯æ®µäº¤é€šçŠ¶å†µæ•°æ®
    """
    try:
        start_timestamp = time_range.get("start", time.time() - 3600)
        end_timestamp = time_range.get("end", time.time())
        
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return RoadTrafficResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰äº¤é€šæ•°æ®",
                traffic_data=[],
                statistics={}
            )
        
        # æ‰§è¡Œè·¯æ®µåˆ†æ
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type="comprehensive",
            min_vehicles=5
        )
        
        if "error" in analysis_result:
            return RoadTrafficResponse(
                success=False,
                message=analysis_result["error"],
                traffic_data=[],
                statistics={}
            )
        
        # è½¬æ¢äº¤é€šæ•°æ®
        traffic_data_list = analysis_result.get("traffic_data", [])
        traffic_data = []
        for data in traffic_data_list:
            traffic_item = RoadTrafficData(**data)
            traffic_data.append(traffic_item)
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        statistics = {
            "total_records": len(traffic_data),
            "time_range": {"start": start_timestamp, "end": end_timestamp},
            "avg_speed": np.mean([t.avg_speed for t in traffic_data]) if traffic_data else 0,
            "avg_flow": np.mean([t.flow_rate for t in traffic_data]) if traffic_data else 0,
            "congestion_summary": {}
        }
        
        # æ‹¥å µåˆ†å¸ƒç»Ÿè®¡
        congestion_counts = {}
        for t in traffic_data:
            level = t.congestion_level
            congestion_counts[level] = congestion_counts.get(level, 0) + 1
        statistics["congestion_summary"] = congestion_counts
        
        return RoadTrafficResponse(
            success=True,
            message=f"æˆåŠŸè·å– {len(traffic_data)} æ¡äº¤é€šæ•°æ®è®°å½•",
            traffic_data=traffic_data,
            statistics=statistics
        )
        
    except Exception as e:
        logger.error(f"è·å–äº¤é€šæ•°æ®APIé”™è¯¯: {str(e)}")
        return RoadTrafficResponse(
            success=False,
            message=f"è·å–å¤±è´¥: {str(e)}",
            traffic_data=[],
            statistics={}
        )

@router.post("/api/road/visualization", response_model=RoadVisualizationResponse)
async def get_road_visualization_data(request: Dict[str, Any]):
    """
    è·å–è·¯æ®µå¯è§†åŒ–æ•°æ®API
    ç”Ÿæˆç”¨äºåœ°å›¾å±•ç¤ºçš„è·¯æ®µå¯è§†åŒ–æ•°æ®
    """
    try:
        visualization_type = request.get("visualization_type", "speed")
        time_range = request.get("time_range", {})
        
        start_timestamp = time_range.get("start", time.time() - 3600)
        end_timestamp = time_range.get("end", time.time())
        
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return RoadVisualizationResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„æ•°æ®è¿›è¡Œå¯è§†åŒ–",
                visualization_data={},
                segment_colors={},
                legend_info={}
            )
        
        # æ‰§è¡Œè·¯æ®µåˆ†æ
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type="comprehensive",
            min_vehicles=3
        )
        
        if "error" in analysis_result:
            return RoadVisualizationResponse(
                success=False,
                message=analysis_result["error"],
                visualization_data={},
                segment_colors={},
                legend_info={}
            )
        
        # ç”Ÿæˆå¯è§†åŒ–æ•°æ®
        segments_data = analysis_result.get("segments", [])
        traffic_data = analysis_result.get("traffic_data", [])
        
        visualization_data = data_processor.generate_road_visualization_data(
            segments_data,
            traffic_data,
            visualization_type
        )
        
        if "error" in visualization_data:
            return RoadVisualizationResponse(
                success=False,
                message=visualization_data["error"],
                visualization_data={},
                segment_colors={},
                legend_info={}
            )
        
        return RoadVisualizationResponse(
            success=True,
            message=f"æˆåŠŸç”Ÿæˆ {len(visualization_data.get('segments', []))} ä¸ªè·¯æ®µçš„å¯è§†åŒ–æ•°æ®",
            visualization_data=visualization_data,
            segment_colors=visualization_data.get("color_mapping", {}),
            legend_info=visualization_data.get("legend", {})
        )
        
    except Exception as e:
        logger.error(f"è·¯æ®µå¯è§†åŒ–APIé”™è¯¯: {str(e)}")
        return RoadVisualizationResponse(
            success=False,
            message=f"ç”Ÿæˆå¯è§†åŒ–æ•°æ®å¤±è´¥: {str(e)}",
            visualization_data={},
            segment_colors={},
            legend_info={}
        )

@router.get("/api/road/metrics", response_model=Dict[str, Any])
async def get_road_network_metrics():
    """
    è·å–è·¯ç½‘æ•´ä½“æŒ‡æ ‡API
    è¿”å›é“è·¯ç½‘ç»œçš„ç»¼åˆæ€§èƒ½æŒ‡æ ‡
    """
    try:
        # ä½¿ç”¨å½“å‰æ—¶é—´èŒƒå›´ï¼ˆæ¼”ç¤ºç”¨ï¼‰
        current_time = time.time()
        start_timestamp = current_time - 24 * 3600  # 24å°æ—¶å‰
        end_timestamp = current_time
        
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return {
                "success": False,
                "message": "æ²¡æœ‰å¯ç”¨çš„äº¤é€šæ•°æ®",
                "metrics": {}
            }
        
        # æ‰§è¡Œè·¯æ®µåˆ†æè·å–åŸºç¡€æ•°æ®
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type="comprehensive",
            min_vehicles=3
        )
        
        if "error" in analysis_result:
            return {
                "success": False,
                "message": analysis_result["error"],
                "metrics": {}
            }
        
        # è®¡ç®—ç½‘ç»œæŒ‡æ ‡
        segments_data = analysis_result.get("segments", [])
        traffic_data = analysis_result.get("traffic_data", [])
        
        metrics = data_processor.calculate_road_network_metrics(segments_data, traffic_data)
        
        return {
            "success": True,
            "metrics": metrics,
            "timestamp": time.time()
        }
    
    except Exception as e:
        logger.error(f"è·å–è·¯ç½‘æŒ‡æ ‡å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"è·å–è·¯ç½‘æŒ‡æ ‡å¤±è´¥: {str(e)}",
            "metrics": {}
        }

@router.get("/weekly-passenger-flow", response_model=Dict[str, Any])
async def get_weekly_passenger_flow_analysis(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰")
):
    """
    è·å–å‘¨å®¢æµé‡åˆ†ææ•°æ®
    
    Args:
        start_time: å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰
        end_time: ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰
    
    Returns:
        å‘¨å®¢æµé‡åˆ†æç»“æœï¼ŒåŒ…æ‹¬æ—¥æµé‡å¯¹æ¯”ã€å·¥ä½œæ—¥vså‘¨æœ«ã€é«˜å³°æ—¶æ®µç­‰
    """
    try:
        logger.info(f"å¼€å§‹å‘¨å®¢æµé‡åˆ†æ: {start_time} - {end_time}")
        
        # éªŒè¯æ—¶é—´èŒƒå›´ï¼ˆç¡®ä¿è‡³å°‘æœ‰3å¤©æ•°æ®ï¼‰
        time_span = end_time - start_time
        if time_span < 3 * 24 * 3600:  # å°‘äº3å¤©
            return {
                "success": False,
                "message": "åˆ†æå‘¨å®¢æµé‡éœ€è¦è‡³å°‘3å¤©çš„æ•°æ®",
                "data": {}
            }
        
        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return {
                "success": False,
                "message": "æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰äº¤é€šæ•°æ®",
                "data": {}
            }
        
        # æ‰§è¡Œå‘¨å®¢æµé‡åˆ†æ
        analysis_result = data_processor.analyze_weekly_passenger_flow(df)
        
        if not analysis_result.get('success', False):
            return {
                "success": False,
                "message": analysis_result.get('message', 'åˆ†æå¤±è´¥'),
                "data": {}
            }
        
        # æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
        analysis_result['metadata'] = {
            'query_start_time': start_time,
            'query_end_time': end_time,
            'analysis_timestamp': time.time(),
            'data_quality': {
                'total_records': len(df),
                'unique_vehicles': df['COMMADDR'].nunique() if 'COMMADDR' in df.columns else 0,
                'time_coverage': analysis_result.get('analysis_period', {})
            }
        }
        
        logger.info(f"å‘¨å®¢æµé‡åˆ†æå®Œæˆï¼Œæ•°æ®è´¨é‡: {analysis_result['metadata']['data_quality']}")
        
        return {
            "success": True,
            "message": "å‘¨å®¢æµé‡åˆ†æå®Œæˆ",
            "data": analysis_result
        }
        
    except Exception as e:
        logger.error(f"å‘¨å®¢æµé‡åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        return {
            "success": False,
            "message": f"åˆ†æå¤±è´¥: {str(e)}",
            "data": {}
        }



