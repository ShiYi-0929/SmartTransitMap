from fastapi import APIRouter, Query, Depends, HTTPException
import pandas as pd
import os
from datetime import datetime, timedelta
from collections import defaultdict
from typing import List, Dict, Optional, Any, Union
from .data_processor import TrafficDataProcessor
from .heatmap import HeatmapGenerator
from .track import TrackAnalyzer
from .traffic_statistics_loader import TrafficStatisticsLoader

from .models import (
    TimeRangeRequest, TrafficQueryRequest, HeatmapRequest, 
    TrackQueryRequest, StatisticsRequest, TrafficResponse,
    TrafficDataResponse, HeatmapResponse, TracksResponse,
    StatisticsResponse, TrafficOverview, TimeDistribution,
    DynamicHeatmapResponse, ClusteringRequest, ClusteringResponse,
    ODAnalysisRequest, ODFlowResponse, SpatioTemporalResponse,
    SpatioTemporalAnalysis, RoadAnalysisRequest, RoadAnalysisResponse,
    RoadSegmentResponse, RoadTrafficResponse, RoadVisualizationResponse,
    RoadSegment, RoadTrafficData, RoadSegmentStatistics, 
    RoadNetworkAnalysis, SpeedDistribution, TrafficFlowPattern,
    SmartPassengerRequest, SmartPassengerResponse, WeatherImpactRequest,
    WeatherImpactResponse, TaxiDemandRequest, TaxiDemandResponse,
    PassengerVisualizationResponse, WeatherData, PassengerFlowData,
    TaxiDemandData, WeatherImpactAnalysis, TaxiSupplyDemand,
    TripAnalysisRequest, TripAnalysisResponse, TripAnalysisStatistics,
    OrderSpeedAnalysisRequest, OrderSpeedAnalysisResponse, 
    TripDistanceClassification, OrderSpeedAnalysis
)
import numpy as np
import logging
import traceback
import time
import json
from pydantic import BaseModel
from pathlib import Path
from typing import List, Dict

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter()

# åˆ›å»ºæ•°æ®å¤„ç†å™¨å®ä¾‹
data_processor = TrafficDataProcessor()
heatmap_generator = HeatmapGenerator()
track_analyzer = TrackAnalyzer()
traffic_stats_loader = TrafficStatisticsLoader()
# data_cleaner = TrafficDataCleaner()  # å·²åˆ é™¤æ•°æ®æ¸…æ´—åŠŸèƒ½
# quality_analyzer = DataQualityAnalyzer()  # å·²åˆ é™¤æ•°æ®æ¸…æ´—åŠŸèƒ½

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ä¸€ä¸ªæ–°çš„è¾…åŠ©å‡½æ•°æ¥å¿«é€Ÿç”Ÿæˆçƒ­åŠ›å›¾
def _get_fast_heatmap_data(start_time: float, end_time: float) -> List[Dict]:
    """ä½¿ç”¨é¢„è®¡ç®—æ•°æ®å¿«é€Ÿç”Ÿæˆçƒ­åŠ›å›¾"""
    try:
        # ç›´æ¥ä»é¢„è®¡ç®—çš„çƒ­åŠ›å›¾æ–‡ä»¶ä¸­åŠ è½½æ•°æ®
        data_dir = os.path.join(os.path.dirname(__file__), 'data')
        index_dir = os.path.join(data_dir, 'indexes')
        
        if not os.path.exists(index_dir):
            return []
        
        # è®¡ç®—éœ€è¦çš„å¤©æ•°èŒƒå›´
        start_day = (int(start_time) // (24 * 3600)) * (24 * 3600)
        end_day = (int(end_time) // (24 * 3600)) * (24 * 3600)
        
        combined_heatmap = defaultdict(int)
        current_day = start_day
        
        while current_day <= end_day:
            filename = f"heatmap_day_{int(current_day)}.json"
            filepath = os.path.join(index_dir, filename)
            
            if os.path.exists(filepath):
                try:
                    with open(filepath, 'r') as f:
                        day_heatmap = json.load(f)
                    
                    for grid_key, count in day_heatmap.items():
                        combined_heatmap[grid_key] += count
                except Exception as e:
                    logger.error(f"è¯»å–çƒ­åŠ›å›¾æ–‡ä»¶ {filename} å¤±è´¥: {e}")
            
            current_day += 24 * 3600  # ä¸‹ä¸€å¤©
        
        # è½¬æ¢ä¸ºçƒ­åŠ›å›¾ç‚¹æ ¼å¼
        heatmap_points = []
        for grid_key, count in combined_heatmap.items():
            try:
                lat, lng = map(float, grid_key.split(','))
                heatmap_points.append({
                    'lat': lat,
                    'lng': lng,
                    'count': count
                })
            except:
                continue
        
        # æŒ‰å¯†åº¦æ’åºï¼Œå–å‰10000ä¸ªç‚¹é¿å…å‰ç«¯æ€§èƒ½é—®é¢˜
        heatmap_points.sort(key=lambda x: x['count'], reverse=True)
        return heatmap_points[:10000]
        
    except Exception as e:
        logger.error(f"å¿«é€Ÿçƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")
        return []

@router.get("/test")
async def test_endpoint():
    """æµ‹è¯•ç«¯ç‚¹ï¼Œç¡®ä¿è·¯ç”±æ­£å¸¸å·¥ä½œ"""
    return {"message": "Traffic router is working!", "status": "ok"}

@router.get("/stats", response_model=StatisticsResponse)
async def get_traffic_stats(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    group_by: str = Query("hour", description="æ—¶é—´åˆ†ç»„æ–¹å¼ï¼ˆhour, day, week, monthï¼‰")
):
    """
    è·å–äº¤é€šæ•°æ®ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    try:
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return StatisticsResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                overview=TrafficOverview(
                    total_vehicles=0,
                    total_points=0,
                    active_vehicles=0,
                    time_span="0å°æ—¶",
                    coverage_area="æœªçŸ¥",
                    average_speed=0.0
                ),
                time_distribution=[]
            )
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = data_processor.calculate_statistics(df, group_by)
        
        # æ„é€ å“åº”
        return StatisticsResponse(
            success=True,
            overview=TrafficOverview(
                total_vehicles=stats['total_vehicles'],
                total_points=stats['total_points'],
                active_vehicles=stats['active_vehicles'],
                time_span=stats['time_span'],
                coverage_area=stats['coverage_area'],
                average_speed=stats['average_speed']
            ),
            time_distribution=[
                TimeDistribution(time_key=item['time_key'], count=item['count'])
                for item in stats['time_distribution']
            ]
        )
    except Exception as e:
        return StatisticsResponse(
            success=False,
            message=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}",
            overview=TrafficOverview(
                total_vehicles=0,
                total_points=0,
                active_vehicles=0,
                time_span="0å°æ—¶",
                coverage_area="æœªçŸ¥",
                average_speed=0.0
            ),
            time_distribution=[]
        )

def convert_time_to_timestamp(time_str: str) -> float:
    """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºUTCæ—¶é—´æˆ³"""
    try:
        if 'T' in time_str:
            dt = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
        else:
            dt = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
        return dt.timestamp()
    except Exception as e:
        logger.error(f"æ—¶é—´è½¬æ¢å¤±è´¥: {time_str}, é”™è¯¯: {e}")
        raise HTTPException(status_code=400, detail=f"æ—¶é—´æ ¼å¼é”™è¯¯: {time_str}")

def convert_numpy_types(obj):
    """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif hasattr(obj, '__dict__'):
        # å¤„ç†pydanticæ¨¡å‹ç­‰å¯¹è±¡
        if hasattr(obj, 'dict'):
            return convert_numpy_types(obj.dict())
        else:
            return convert_numpy_types(obj.__dict__)
    return obj

@router.get("/visualization", response_model=TrafficDataResponse)
async def get_traffic_visualization(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    view_type: str = Query("distribution", description="è§†å›¾ç±»å‹ï¼šdistribution, trajectory, heatmap"),
    vehicle_id: Optional[str] = Query(None, description="è½¦è¾†IDï¼Œå¯é€‰"),
    map_style: Optional[str] = Query("blue", description="åœ°å›¾æ ·å¼")
):
    """
    è·å–äº¤é€šæ•°æ®å¯è§†åŒ–æ‰€éœ€æ•°æ®ï¼ˆåˆ†å¸ƒè§†å›¾ã€è½¨è¿¹è§†å›¾ã€çƒ­åŠ›å›¾ï¼‰ã€‚
    """
    try:
        print(f"=== å¼€å§‹å¤„ç†å¯è§†åŒ–è¯·æ±‚ ===")
        print(f"å‚æ•°: start_time={start_time}, end_time={end_time}, view_type={view_type}, vehicle_id={vehicle_id}")
        
        # éªŒè¯æ—¶é—´èŒƒå›´
        min_valid_time = 1378944000  # 2013-09-12 00:00:00 UTC
        max_valid_time = 1379548799  # 2013-09-18 23:59:59 UTC
        
        if end_time < min_valid_time or start_time > max_valid_time:
            print(f"æ—¶é—´èŒƒå›´éªŒè¯å¤±è´¥: {start_time}-{end_time} è¶…å‡ºæœ‰æ•ˆèŒƒå›´ {min_valid_time}-{max_valid_time}")
            return TrafficDataResponse(
                success=False,
                message="æŸ¥è¯¢æ—¶é—´è¶…å‡ºæ•°æ®é›†èŒƒå›´ï¼ˆ2013å¹´9æœˆ12æ—¥è‡³9æœˆ18æ—¥ï¼‰",
                view_type=view_type,
                data=[]
            )
        
        # åŠ è½½æ•°æ®
        try:
            print(f"å¼€å§‹åŠ è½½æ•°æ®...")
            df = data_processor.load_data(start_time, end_time, vehicle_id)
            print(f"æ•°æ®åŠ è½½å®Œæˆ, å…± {len(df)} æ¡è®°å½•")
        except Exception as load_error:
            print(f"æ•°æ®åŠ è½½é”™è¯¯: {str(load_error)}")
            import traceback
            print(traceback.format_exc())
            return TrafficDataResponse(
                success=False,
                message=f"æ•°æ®åŠ è½½å¤±è´¥: {str(load_error)}",
                view_type=view_type,
                data=[]
            )
        
        if df.empty:
            print("åŠ è½½çš„æ•°æ®ä¸ºç©º")
            return TrafficDataResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                view_type=view_type,
                data=[]
            )
        
        # æ ¹æ®è§†å›¾ç±»å‹å¤„ç†æ•°æ®
        try:
            print(f"å¼€å§‹å¤„ç† {view_type} è§†å›¾æ•°æ®...")
            if view_type == "heatmap":
                # ä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—çš„å¿«é€Ÿçƒ­åŠ›å›¾æ•°æ®
                print("å°è¯•ä½¿ç”¨é¢„è®¡ç®—çƒ­åŠ›å›¾æ•°æ®...")
                fast_heatmap_data = _get_fast_heatmap_data(start_time, end_time)
                
                if fast_heatmap_data and not vehicle_id:  # åªåœ¨æ²¡æœ‰ç‰¹å®šè½¦è¾†è¿‡æ»¤æ—¶ä½¿ç”¨é¢„è®¡ç®—æ•°æ®
                    print(f"ä½¿ç”¨å¿«é€Ÿçƒ­åŠ›å›¾æ•°æ®: {len(fast_heatmap_data)} ä¸ªç‚¹")
                    data = fast_heatmap_data
                else:
                    print("é¢„è®¡ç®—æ•°æ®ä¸å¯ç”¨æˆ–æœ‰è½¦è¾†è¿‡æ»¤ï¼Œä½¿ç”¨å®æ—¶è®¡ç®—...")
                    # ç”Ÿæˆçƒ­åŠ›å›¾æ•°æ®
                    heatmap_points = data_processor.generate_heatmap_data(df)
                    # ç¡®ä¿æ¯ä¸ªç‚¹éƒ½è¢«æ­£ç¡®åºåˆ—åŒ–
                    data = []
                    for point in heatmap_points:
                        try:
                            data.append(point.dict())
                        except Exception as e:
                            print(f"åºåˆ—åŒ–çƒ­åŠ›å›¾ç‚¹æ—¶å‡ºé”™: {e}")
                            # ä½¿ç”¨æ‰‹åŠ¨æ„é€ çš„å­—å…¸ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
                            data.append({
                                "lat": point.lat,
                                "lng": point.lng,
                                "count": point.count
                            })
                print(f"ç”Ÿæˆäº† {len(data)} ä¸ªçƒ­åŠ›å›¾ç‚¹")
            
            elif view_type == "trajectory":
                # ç”Ÿæˆè½¨è¿¹æ•°æ®
                tracks = data_processor.generate_track_data(df, vehicle_id)
                # ç¡®ä¿æ¯ä¸ªè½¨è¿¹éƒ½è¢«æ­£ç¡®åºåˆ—åŒ–
                data = []
                for track in tracks:
                    try:
                        data.append(track.dict())
                    except Exception as e:
                        print(f"åºåˆ—åŒ–è½¨è¿¹æ—¶å‡ºé”™: {e}")
                        # è·³è¿‡æœ‰é—®é¢˜çš„è½¨è¿¹
                        continue
                print(f"ç”Ÿæˆäº† {len(data)} æ¡è½¨è¿¹")
            
            else:  # distribution
                # ç”Ÿæˆåˆ†å¸ƒè§†å›¾æ•°æ®ï¼ˆç®€å•ç‚¹æ ‡è®°ï¼‰
                # æ ¹æ®æ•°æ®é‡æ™ºèƒ½è°ƒæ•´æ˜¾ç¤ºç‚¹æ•°
                if len(df) > 50000:
                    sample_size = 10000  # å¤§æ•°æ®é›†æ˜¾ç¤º1ä¸‡ä¸ªç‚¹
                elif len(df) > 20000:
                    sample_size = 8000   # ä¸­ç­‰æ•°æ®é›†æ˜¾ç¤º8åƒä¸ªç‚¹
                else:
                    sample_size = min(len(df), 5000)  # å°æ•°æ®é›†æ˜¾ç¤ºå…¨éƒ¨æˆ–5åƒä¸ªç‚¹
                
                df_sampled = df.sample(sample_size) if len(df) > sample_size else df
                data = []
                for _, row in df_sampled.iterrows():
                    # è®¡ç®—é€Ÿåº¦ï¼ˆå¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„é€Ÿåº¦ï¼Œåˆ™ä»GPSæ•°æ®è®¡ç®—ï¼‰
                    speed = 0.0
                    if "SPEED" in row and pd.notna(row["SPEED"]):
                        speed = float(row["SPEED"])
                    elif "speed" in row and pd.notna(row["speed"]):
                        speed = float(row["speed"])
                    elif "speed_kmh" in row and pd.notna(row["speed_kmh"]):
                        speed = float(row["speed_kmh"])
                    
                    point = {
                        "lng": float(row["LON"]) / 1e5,
                        "lat": float(row["LAT"]) / 1e5,
                        "vehicle_id": str(row["COMMADDR"]),  # ç¡®ä¿è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        "timestamp": int(row["UTC"]),  # ç¡®ä¿è½¬æ¢ä¸ºPython int
                        "speed": speed  # æ·»åŠ é€Ÿåº¦å­—æ®µ
                    }
                    data.append(point)
                print(f"ç”Ÿæˆäº† {len(data)} ä¸ªåˆ†å¸ƒç‚¹")
        except Exception as process_error:
            print(f"æ•°æ®å¤„ç†é”™è¯¯: {str(process_error)}")
            import traceback
            print(traceback.format_exc())
            return TrafficDataResponse(
                success=False,
                message=f"æ•°æ®å¤„ç†å¤±è´¥: {str(process_error)}",
                view_type=view_type,
                data=[]
            )
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        try:
            print("å¼€å§‹è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
            stats = data_processor.calculate_statistics(df)
            print("ç»Ÿè®¡ä¿¡æ¯è®¡ç®—å®Œæˆ")
            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯å¯åºåˆ—åŒ–çš„
            return TrafficDataResponse(
                success=True,
                message="æ•°æ®è·å–æˆåŠŸ",
                view_type=view_type,
                data=convert_numpy_types(data),
                stats=convert_numpy_types(stats)
            )
        except Exception as stats_error:
            print(f"ç»Ÿè®¡ä¿¡æ¯è®¡ç®—é”™è¯¯: {str(stats_error)}")
            # å³ä½¿ç»Ÿè®¡å¤±è´¥ï¼Œä¹Ÿè¿”å›æ•°æ®
            return TrafficDataResponse(
                success=True,
                message="æ•°æ®è·å–æˆåŠŸï¼ˆç»Ÿè®¡ä¿¡æ¯è®¡ç®—å¤±è´¥ï¼‰",
                view_type=view_type,
                data=convert_numpy_types(data)
            )
        
    except Exception as e:
        print(f"=== å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯ ===")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return TrafficDataResponse(
            success=False,
            message=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}",
            view_type=view_type,
            data=[]
        )

@router.get("/heatmap", response_model=HeatmapResponse)
async def get_heatmap_data(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    resolution: float = Query(0.001, description="çƒ­åŠ›å›¾åˆ†è¾¨ç‡")
):
    """
    è·å–çƒ­åŠ›å›¾æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—æ•°æ®ï¼‰ã€‚
    """
    try:
        # ä¼˜å…ˆå°è¯•é¢„è®¡ç®—æ•°æ®
        print("å°è¯•ä½¿ç”¨é¢„è®¡ç®—çƒ­åŠ›å›¾æ•°æ®...")
        fast_heatmap_data = _get_fast_heatmap_data(start_time, end_time)
        
        if fast_heatmap_data:
            print(f"ä½¿ç”¨å¿«é€Ÿçƒ­åŠ›å›¾æ•°æ®: {len(fast_heatmap_data)} ä¸ªç‚¹")
            return HeatmapResponse(
                success=True,
                data=fast_heatmap_data,
                message=f"å¿«é€Ÿçƒ­åŠ›å›¾ç”ŸæˆæˆåŠŸï¼Œå…± {len(fast_heatmap_data)} ä¸ªç‚¹"
            )
        
        print("é¢„è®¡ç®—æ•°æ®ä¸å¯ç”¨ï¼Œä½¿ç”¨å®æ—¶è®¡ç®—...")
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return HeatmapResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                points=[]
            )
        
        # ç”Ÿæˆçƒ­åŠ›å›¾æ•°æ®
        heatmap_points = data_processor.generate_heatmap_data(df, resolution)
        
        # æ„é€ å“åº”
        return HeatmapResponse(
            success=True,
            points=heatmap_points
        )
    except Exception as e:
        return HeatmapResponse(
            success=False,
            message=f"è·å–çƒ­åŠ›å›¾æ•°æ®å¤±è´¥: {str(e)}",
            points=[]
        )

@router.get("/track", response_model=TracksResponse)
async def get_track(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    vehicle_id: Optional[str] = Query(None, description="è½¦è¾†IDï¼Œå¯é€‰"),
    view_type: str = Query("trajectory", description="è§†å›¾ç±»å‹ï¼štrajectory, path, stops"),
    performance_mode: str = Query("normal", description="æ€§èƒ½æ¨¡å¼ï¼šfast, medium, full"),
    max_points: int = Query(5000, description="æœ€å¤§è¿”å›ç‚¹æ•°")
):
    """
    æŒ‰æ—¶é—´æ®µå’Œè½¦è¾†IDæŸ¥è¯¢è½¦è¾†è½¨è¿¹æ•°æ®ã€‚
    æ”¯æŒæ€§èƒ½æ¨¡å¼å’Œæœ€å¤§ç‚¹æ•°é™åˆ¶ã€‚
    """
    try:
        print(f"ğŸš— è½¨è¿¹æŸ¥è¯¢å‚æ•°: æ—¶é—´={start_time}-{end_time}, è½¦è¾†={vehicle_id}, æ¨¡å¼={performance_mode}, æœ€å¤§ç‚¹æ•°={max_points}")
        
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time, vehicle_id)
        
        if df.empty:
            print(f"âš ï¸ æœªæ‰¾åˆ°è½¦è¾† {vehicle_id} åœ¨æ—¶é—´æ®µ {start_time}-{end_time} çš„æ•°æ®")
            return TracksResponse(
                success=False,
                message=f"æœªæ‰¾åˆ°è½¦è¾† {vehicle_id} åœ¨æŒ‡å®šæ—¶é—´æ®µçš„æ•°æ®",
                tracks=[]
            )
        
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {len(df)} æ¡è®°å½•")
        
        # æ ¹æ®æ€§èƒ½æ¨¡å¼é‡‡æ ·æ•°æ®
        if performance_mode == "fast" and len(df) > max_points:
            # å¿«é€Ÿæ¨¡å¼ï¼šå‡åŒ€é‡‡æ ·
            sample_ratio = max_points / len(df)
            df = df.sample(frac=sample_ratio)
            print(f"âš¡ å¿«é€Ÿæ¨¡å¼: é‡‡æ ·å {len(df)} æ¡è®°å½•")
        elif performance_mode == "medium" and len(df) > max_points:
            # ä¸­ç­‰æ¨¡å¼ï¼šä¿ç•™å…³é”®ç‚¹çš„é‡‡æ ·
            # è¿™é‡Œç®€åŒ–ä¸ºéšæœºé‡‡æ ·ï¼Œå®é™…å¯ä»¥ç”¨æ›´å¤æ‚çš„ç®—æ³•
            sample_ratio = max_points / len(df)
            df = df.sample(frac=sample_ratio)
            print(f"ğŸ“Š ä¸­ç­‰æ¨¡å¼: é‡‡æ ·å {len(df)} æ¡è®°å½•")
        
        # ç”Ÿæˆè½¨è¿¹æ•°æ®
        tracks = data_processor.generate_track_data(df, vehicle_id)
        
        if not tracks:
            print(f"âš ï¸ è½¨è¿¹ç”Ÿæˆå¤±è´¥: è½¦è¾† {vehicle_id}")
            return TracksResponse(
                success=False,
                message=f"æœªèƒ½ç”Ÿæˆè½¦è¾† {vehicle_id} çš„è½¨è¿¹æ•°æ®",
                tracks=[]
            )
        
        print(f"âœ… è½¨è¿¹ç”ŸæˆæˆåŠŸ: {len(tracks)} æ¡è½¨è¿¹, å…± {sum(len(t.points) for t in tracks)} ä¸ªç‚¹")
        
        # æ„é€ å“åº”
        return TracksResponse(
            success=True,
            tracks=tracks
        )
    except Exception as e:
        import traceback
        print(f"âŒ è½¨è¿¹æŸ¥è¯¢å¤±è´¥: {str(e)}")
        print(traceback.format_exc())
        return TracksResponse(
            success=False,
            message=f"è·å–è½¨è¿¹æ•°æ®å¤±è´¥: {str(e)}",
            tracks=[]
        )

@router.get("/clear-cache")
async def clear_cache():
    """
    æ¸…é™¤æ•°æ®å¤„ç†å™¨çš„ç¼“å­˜ã€‚
    """
    data_processor.clear_cache()
    return {"success": True, "message": "ç¼“å­˜å·²æ¸…é™¤"}

@router.get("/orders/analysis")
async def get_orders_analysis(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰")
):
    """
    æ ¹æ®è®¢å•èµ·æ­¢æ—¶é—´ï¼Œåˆ†æä¹˜å®¢ä¹˜è½¦çš„è·ç¦»ä¸æ—¶é—´åˆ†å¸ƒã€‚
    è¿”å›è®¢å•è€—æ—¶ã€è·¯ç¨‹çš„åˆ†å¸ƒæ•°æ®ï¼Œä»¥åŠæŒ‰æ—¶æ®µï¼ˆå¦‚å°æ—¶ï¼‰èšåˆçš„è®¢å•æ•°é‡ã€‚
    """
    try:
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                "data": {}
            }
        
        # æ¨¡æ‹Ÿè®¢å•æ•°æ®ï¼ˆå®é™…åº”ä»è®¢å•è¡¨è·å–ï¼‰
        # è¿™é‡Œæˆ‘ä»¬å‡è®¾æ¯ä¸ªè½¦è¾†çš„è¿ç»­è½¨è¿¹ç‚¹æ„æˆä¸€ä¸ª"è®¢å•"
        orders_data = []
        
        # æŒ‰è½¦è¾†åˆ†ç»„å¤„ç†
        for vehicle_id, group in df.groupby('COMMADDR'):
            # æŒ‰æ—¶é—´æ’åº
            group = group.sort_values('UTC')
            
            # å¦‚æœè½¨è¿¹ç‚¹å¤ªå°‘ï¼Œè·³è¿‡
            if len(group) < 2:
                continue
            
            # è®¡ç®—è®¢å•ä¿¡æ¯
            start_point = group.iloc[0]
            end_point = group.iloc[-1]
            
            duration_min = (end_point['UTC'] - start_point['UTC']) / 60
            
            # ç®€åŒ–çš„è·ç¦»è®¡ç®—
            start_lat, start_lon = start_point['LAT'] / 1e5, start_point['LON'] / 1e5
            end_lat, end_lon = end_point['LAT'] / 1e5, end_point['LON'] / 1e5
            
            # ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»ä½œä¸ºç®€åŒ–ï¼ˆå®é™…åº”ä½¿ç”¨Haversineå…¬å¼ï¼‰
            dist = ((end_lon - start_lon) ** 2 + (end_lat - start_lat) ** 2) ** 0.5
            distance_km = dist * 111  # 1åº¦çº¦ç­‰äº111å…¬é‡Œ
            
            orders_data.append({
                'vehicle_id': vehicle_id,
                'start_time': start_point['UTC'],
                'end_time': end_point['UTC'],
                'duration_min': duration_min,
                'distance_km': distance_km
            })
        
        if not orders_data:
            return {
                "success": False,
                "message": "æœªèƒ½ç”Ÿæˆè®¢å•æ•°æ®",
                "data": {}
            }
        
        # è½¬æ¢ä¸ºDataFrameè¿›è¡Œåˆ†æ
        orders_df = pd.DataFrame(orders_data)
        
        # 1. è€—æ—¶åˆ†å¸ƒ
        duration_bins = [0, 5, 10, 15, 20, 30, 45, 60, 90, 120, 180]
        duration_labels = [f"{bins[i]}-{bins[i+1]}" for i in range(len(duration_bins)-1)]
        
        orders_df['duration_bin'] = pd.cut(
            orders_df['duration_min'], 
            bins=duration_bins,
            labels=duration_labels,
            include_lowest=True
        )
        
        duration_dist = orders_df['duration_bin'].value_counts().sort_index()
        duration_distribution = [
            {"range": str(idx), "count": int(count)}
            for idx, count in duration_dist.items()
        ]
        
        # 2. è·ç¦»åˆ†å¸ƒ
        distance_bins = [0, 1, 2, 3, 5, 10, 15, 20, 30, 50]
        distance_labels = [f"{bins[i]}-{bins[i+1]}" for i in range(len(distance_bins)-1)]
        
        orders_df['distance_bin'] = pd.cut(
            orders_df['distance_km'], 
            bins=distance_bins,
            labels=distance_labels,
            include_lowest=True
        )
        
        distance_dist = orders_df['distance_bin'].value_counts().sort_index()
        distance_distribution = [
            {"range": str(idx), "count": int(count)}
            for idx, count in distance_dist.items()
        ]
        
        # 3. æŒ‰å°æ—¶åˆ†å¸ƒ
        orders_df['hour'] = pd.to_datetime(orders_df['start_time'], unit='s').dt.hour
        hourly_dist = orders_df['hour'].value_counts().sort_index()
        
        hourly_distribution = [
            {"hour": int(hour), "count": int(count)}
            for hour, count in hourly_dist.items()
        ]
        
        return {
            "success": True,
            "data": {
                "duration_distribution": duration_distribution,
                "distance_distribution": distance_distribution,
                "hourly_distribution": hourly_distribution,
                "total_orders": len(orders_data),
                "avg_duration": round(orders_df['duration_min'].mean(), 1),
                "avg_distance": round(orders_df['distance_km'].mean(), 1)
            }
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"åˆ†æè®¢å•æ•°æ®å¤±è´¥: {str(e)}",
            "data": {}
        }

@router.get("/heatmap/time-filtered")
async def get_time_filtered_heatmap(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    resolution: float = Query(0.001, description="çƒ­åŠ›å›¾åˆ†è¾¨ç‡")
):
    """
    è·å–æŒ‰æ—¶é—´æ®µè¿‡æ»¤çš„çƒ­åŠ›å›¾æ•°æ®ã€‚
    è¿”å›æ—©é«˜å³°ã€åˆé¤æ—¶é—´ã€æ™šé«˜å³°å’Œå¤œé—´çš„çƒ­åŠ›å›¾æ•°æ®ã€‚
    """
    try:
        # ç”ŸæˆæŒ‰æ—¶é—´æ®µè¿‡æ»¤çš„çƒ­åŠ›å›¾æ•°æ®
        result = heatmap_generator.generate_time_filtered_heatmap(
            start_time, end_time, resolution=resolution
        )
        
        return {
            "success": True,
            "data": result
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"è·å–æŒ‰æ—¶é—´æ®µè¿‡æ»¤çš„çƒ­åŠ›å›¾æ•°æ®å¤±è´¥: {str(e)}",
            "data": {}
        }

@router.get("/heatmap/pickup")
async def get_pickup_heatmap(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    resolution: float = Query(0.001, description="çƒ­åŠ›å›¾åˆ†è¾¨ç‡")
):
    """
    è·å–ä¸Šå®¢ç‚¹çƒ­åŠ›å›¾æ•°æ®ã€‚
    """
    try:
        # ç”Ÿæˆä¸Šå®¢ç‚¹çƒ­åŠ›å›¾æ•°æ®
        result = heatmap_generator.generate_pickup_heatmap(
            start_time, end_time, resolution=resolution
        )
        
        return {
            "success": True,
            "data": result
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"è·å–ä¸Šå®¢ç‚¹çƒ­åŠ›å›¾æ•°æ®å¤±è´¥: {str(e)}",
            "data": {}
        }

@router.get("/track/metrics")
async def get_track_metrics(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    vehicle_id: Optional[str] = Query(None, description="è½¦è¾†IDï¼Œå¯é€‰")
):
    """
    è·å–è½¨è¿¹æŒ‡æ ‡ã€‚
    """
    try:
        # æŸ¥è¯¢è½¨è¿¹æ•°æ®
        tracks = track_analyzer.query_track(start_time, end_time, vehicle_id)
        
        # è®¡ç®—è½¨è¿¹æŒ‡æ ‡
        metrics = track_analyzer.calculate_track_metrics(tracks)
        
        return {
            "success": True,
            "data": metrics
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"è·å–è½¨è¿¹æŒ‡æ ‡å¤±è´¥: {str(e)}",
            "data": {}
        }

@router.get("/track/similar")
async def get_similar_tracks(
    track_id: str = Query(..., description="å‚è€ƒè½¨è¿¹çš„è½¦è¾†ID"),
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    similarity_threshold: float = Query(0.7, description="ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰")
):
    """
    æŸ¥æ‰¾ç›¸ä¼¼è½¨è¿¹ã€‚
    """
    try:
        # æŸ¥æ‰¾ç›¸ä¼¼è½¨è¿¹
        similar_tracks = track_analyzer.find_similar_tracks(
            track_id, start_time, end_time, similarity_threshold
        )
        
        return {
            "success": True,
            "data": {
                "reference_track_id": track_id,
                "similar_tracks": similar_tracks,
                "total_found": len(similar_tracks)
            }
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"æŸ¥æ‰¾ç›¸ä¼¼è½¨è¿¹å¤±è´¥: {str(e)}",
            "data": {
                "reference_track_id": track_id,
                "similar_tracks": [],
                "total_found": 0
            }
        }

@router.get("/sample-vehicles")
async def get_sample_vehicles(
    start_time: float = Query(1379030400, description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼Œé»˜è®¤2013-09-13 08:00ï¼‰"),
    end_time: float = Query(1379044800, description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼Œé»˜è®¤2013-09-13 12:00ï¼‰"),
    limit: int = Query(15, description="è¿”å›çš„è½¦è¾†æ•°é‡é™åˆ¶")
):
    """
    è·å–æŒ‡å®šæ—¶é—´æ®µå†…çš„ç¤ºä¾‹è½¦è¾†IDåˆ—è¡¨ï¼Œç”¨äºè½¨è¿¹æŸ¥è¯¢æµ‹è¯•ï¼ˆè¶…é«˜é€Ÿç‰ˆæœ¬ï¼‰
    """
    try:
        print(f"ğŸš€ è¶…é«˜é€Ÿè·å–ç¤ºä¾‹è½¦è¾†: {start_time} - {end_time}, é™åˆ¶: {limit}")
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"fast_sample_vehicles_{start_time}_{end_time}_{limit}"
        
        # æ£€æŸ¥ç¼“å­˜
        if hasattr(data_processor, '_sample_cache') and cache_key in data_processor._sample_cache:
            print("âš¡ ä½¿ç”¨ç¼“å­˜çš„ç¤ºä¾‹è½¦è¾†æ•°æ®ï¼ˆç§’çº§å“åº”ï¼‰")
            return data_processor._sample_cache[cache_key]
        
        # ç›´æ¥ä½¿ç”¨é¢„å¤„ç†æ–‡ä»¶è¿›è¡Œè¶…å¿«é€ŸæŸ¥è¯¢
        result = await get_sample_vehicles_from_preprocessed(start_time, end_time, limit)
        
        # ç¼“å­˜ç»“æœ
        if not hasattr(data_processor, '_sample_cache'):
            data_processor._sample_cache = {}
        data_processor._sample_cache[cache_key] = result
        
        # æ¸…ç†ç¼“å­˜
        if len(data_processor._sample_cache) > 20:
            oldest_key = next(iter(data_processor._sample_cache))
            del data_processor._sample_cache[oldest_key]
        
        print(f"âœ… è¶…é«˜é€Ÿç¤ºä¾‹è½¦è¾†è·å–å®Œæˆ: {len(result.get('vehicles', []))} ä¸ªè½¦è¾†")
        return result
        
    except Exception as e:
        logger.error(f"è·å–ç¤ºä¾‹è½¦è¾†æ—¶å‡ºé”™: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return {"success": False, "message": f"è·å–ç¤ºä¾‹è½¦è¾†å¤±è´¥: {str(e)}", "vehicles": []}

async def get_sample_vehicles_from_preprocessed(start_time: float, end_time: float, limit: int):
    """
    ç›´æ¥ä»é¢„å¤„ç†æ–‡ä»¶å¿«é€Ÿè·å–ç¤ºä¾‹è½¦è¾†ï¼ˆä¸ä¾èµ–å®Œæ•´æ•°æ®åŠ è½½ï¼‰
    """
    import pandas as pd
    import os
    import json
    
    try:
        # é¢„å¤„ç†æ–‡ä»¶è·¯å¾„
        processed_dir = os.path.join(data_processor.data_dir, 'processed')
        indexes_dir = os.path.join(data_processor.data_dir, 'indexes')
        
        print(f"ğŸ“ ä½¿ç”¨é¢„å¤„ç†ç›®å½•: {processed_dir}")
        
        # è®¡ç®—éœ€è¦æŸ¥è¯¢çš„å°æ—¶æ–‡ä»¶
        start_hour = (int(start_time) // 3600) * 3600
        end_hour = (int(end_time) // 3600) * 3600
        
        # é™åˆ¶æŸ¥è¯¢èŒƒå›´ï¼ˆæœ€å¤šæŸ¥3ä¸ªå°æ—¶çš„æ–‡ä»¶ä»¥ä¿è¯é€Ÿåº¦ï¼‰
        max_hours = 3
        hour_files = []
        current_hour = start_hour
        
        while current_hour <= end_hour and len(hour_files) < max_hours:
            hour_file = os.path.join(processed_dir, f"hour_{current_hour}.parquet")
            if os.path.exists(hour_file):
                hour_files.append(hour_file)
            current_hour += 3600
        
        if not hour_files:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°å¯¹åº”æ—¶é—´æ®µçš„é¢„å¤„ç†æ•°æ®",
                "vehicles": []
            }
        
        print(f"ğŸ“Š è¯»å– {len(hour_files)} ä¸ªé¢„å¤„ç†æ–‡ä»¶...")
        
        # åªè¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶ä»¥è·å¾—æœ€å¿«é€Ÿåº¦
        sample_file = hour_files[0]
        
        try:
            # è¯»å–parquetæ–‡ä»¶ï¼ˆåªè¯»å–éœ€è¦çš„åˆ—ï¼‰
            df = pd.read_parquet(sample_file, columns=['COMMADDR', 'TIMESTAMP'])
            
            print(f"ğŸ“ˆ æˆåŠŸè¯»å–æ•°æ®: {len(df)} æ¡è®°å½•")
            
            # ç­›é€‰æ—¶é—´èŒƒå›´
            df = df[(df['TIMESTAMP'] >= start_time) & (df['TIMESTAMP'] <= end_time)]
            
            if df.empty:
                # å¦‚æœå½“å‰æ–‡ä»¶æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ–‡ä»¶
                if len(hour_files) > 1:
                    df = pd.read_parquet(hour_files[1], columns=['COMMADDR', 'TIMESTAMP'])
                    df = df[(df['TIMESTAMP'] >= start_time) & (df['TIMESTAMP'] <= end_time)]
            
            if df.empty:
                return {
                    "success": False,
                    "message": "æŒ‡å®šæ—¶é—´æ®µå†…æ²¡æœ‰è½¦è¾†æ•°æ®",
                    "vehicles": []
                }
            
            # å¿«é€Ÿç»Ÿè®¡è½¦è¾†æ•°æ®ç‚¹
            vehicle_counts = df['COMMADDR'].value_counts().head(limit * 2)
            
            vehicles = []
            for vehicle_id, count in vehicle_counts.items():
                if count >= 3:  # è‡³å°‘3ä¸ªæ•°æ®ç‚¹
                    vehicles.append({
                        "vehicle_id": str(vehicle_id),
                        "data_points": int(count),
                        "description": f"è½¦è¾† {vehicle_id} ({count}ä¸ªæ•°æ®ç‚¹)"
                    })
                
                if len(vehicles) >= limit:
                    break
            
            return {
                "success": True,
                "message": f"å¿«é€Ÿæ‰¾åˆ° {len(vehicles)} ä¸ªæ´»è·ƒè½¦è¾†",
                "vehicles": vehicles,
                "time_range": f"{start_time} - {end_time}",
                "total_vehicles": len(df['COMMADDR'].unique()),
                "data_source": "é¢„å¤„ç†æ–‡ä»¶ï¼ˆæé€Ÿæ¨¡å¼ï¼‰"
            }
            
        except Exception as file_error:
            print(f"è¯»å–é¢„å¤„ç†æ–‡ä»¶å¤±è´¥: {file_error}")
            
            # é™çº§åˆ°é™æ€ç¤ºä¾‹è½¦è¾†åˆ—è¡¨
            return get_static_sample_vehicles(start_time, end_time, limit)
            
    except Exception as e:
        print(f"é¢„å¤„ç†æ–‡ä»¶æŸ¥è¯¢å¤±è´¥: {e}")
        return get_static_sample_vehicles(start_time, end_time, limit)

def get_static_sample_vehicles(start_time: float, end_time: float, limit: int):
    """
    æä¾›é™æ€çš„ç¤ºä¾‹è½¦è¾†åˆ—è¡¨ä½œä¸ºåå¤‡æ–¹æ¡ˆ
    ä½¿ç”¨çœŸå®æ•°æ®æ ¼å¼çš„æ•°å­—è½¦è¾†ID
    """
    # åŸºäºçœŸå®æ•°æ®çš„å¸¸è§è½¦è¾†IDï¼ˆæ•°å­—æ ¼å¼ï¼‰
    static_vehicles = [
        {"vehicle_id": "15053114280", "data_points": 150, "description": "è½¦è¾† 15053114280 (150ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114281", "data_points": 120, "description": "è½¦è¾† 15053114281 (120ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114282", "data_points": 200, "description": "è½¦è¾† 15053114282 (200ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114283", "data_points": 180, "description": "è½¦è¾† 15053114283 (180ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114284", "data_points": 160, "description": "è½¦è¾† 15053114284 (160ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114285", "data_points": 140, "description": "è½¦è¾† 15053114285 (140ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114286", "data_points": 190, "description": "è½¦è¾† 15053114286 (190ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114287", "data_points": 170, "description": "è½¦è¾† 15053114287 (170ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114288", "data_points": 130, "description": "è½¦è¾† 15053114288 (130ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114289", "data_points": 210, "description": "è½¦è¾† 15053114289 (210ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114290", "data_points": 155, "description": "è½¦è¾† 15053114290 (155ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114291", "data_points": 175, "description": "è½¦è¾† 15053114291 (175ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114292", "data_points": 165, "description": "è½¦è¾† 15053114292 (165ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114293", "data_points": 145, "description": "è½¦è¾† 15053114293 (145ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114294", "data_points": 185, "description": "è½¦è¾† 15053114294 (185ä¸ªæ•°æ®ç‚¹)"}
    ]
    
    # è¿”å›æŒ‡å®šæ•°é‡çš„è½¦è¾†
    selected_vehicles = static_vehicles[:limit]
    
    return {
        "success": True,
        "message": f"æä¾› {len(selected_vehicles)} ä¸ªç¤ºä¾‹è½¦è¾†ï¼ˆé™æ€åˆ—è¡¨ï¼Œæ•°å­—æ ¼å¼ï¼‰",
        "vehicles": selected_vehicles,
        "time_range": f"{start_time} - {end_time}",
        "total_vehicles": len(static_vehicles),
        "data_source": "é™æ€ç¤ºä¾‹æ•°æ®ï¼ˆåå¤‡æ¨¡å¼ï¼Œå·²ä¿®æ­£ä¸ºæ•°å­—æ ¼å¼ï¼‰"
    }

@router.get("/anomaly/detection", response_model=dict)
async def detect_anomalies(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    detection_types: str = Query("all", description="æ£€æµ‹ç±»å‹ï¼šall, long_stop, abnormal_route, speed_anomaly, cluster_anomaly"),
    threshold_params: Optional[str] = Query(None, description="é˜ˆå€¼å‚æ•°JSONå­—ç¬¦ä¸²")
):
    """
    å¼‚å¸¸æ£€æµ‹API - æ£€æµ‹å„ç§ç±»å‹çš„äº¤é€šå¼‚å¸¸
    """
    try:
        import json
        
        # è§£æé˜ˆå€¼å‚æ•°
        thresholds = {}
        if threshold_params:
            try:
                thresholds = json.loads(threshold_params)
            except json.JSONDecodeError:
                pass
        
        # é»˜è®¤é˜ˆå€¼
        default_thresholds = {
            "long_stop_duration": 300,  # 5åˆ†é’Ÿ
            "speed_threshold_low": 5,   # ä½é€Ÿé˜ˆå€¼ km/h
            "speed_threshold_high": 80, # é«˜é€Ÿé˜ˆå€¼ km/h
            "detour_ratio": 1.5,       # ç»•è·¯æ¯”ä¾‹
            "cluster_density": 50       # èšé›†å¯†åº¦
        }
        thresholds = {**default_thresholds, **thresholds}
        
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                "anomalies": [],
                "statistics": {}
            }
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies = data_processor.detect_anomalies(df, detection_types, thresholds)
        
        # è®¡ç®—å¼‚å¸¸ç»Ÿè®¡
        stats = data_processor.calculate_anomaly_statistics(anomalies)
        
        return {
            "success": True,
            "message": f"æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(anomalies)} ä¸ªå¼‚å¸¸äº‹ä»¶",
            "anomalies": convert_numpy_types(anomalies),
            "statistics": convert_numpy_types(stats),
            "thresholds_used": thresholds
        }
        
    except Exception as e:
        print(f"å¼‚å¸¸æ£€æµ‹é”™è¯¯: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return {
            "success": False,
            "message": f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}",
            "anomalies": [],
            "statistics": {}
        }

@router.get("/anomaly/realtime", response_model=dict)
async def get_realtime_anomalies(
    time_window: int = Query(3600, description="æ—¶é—´çª—å£ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å°æ—¶"),
    limit: int = Query(50, description="è¿”å›å¼‚å¸¸æ•°é‡é™åˆ¶")
):
    """
    è·å–å®æ—¶å¼‚å¸¸äº‹ä»¶
    """
    try:
        import time
        
        # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆæœ€è¿‘time_windowç§’ï¼‰
        end_time = time.time()
        start_time = end_time - time_window
        
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return {
                "success": True,
                "anomalies": [],
                "total_count": 0,
                "time_range": {"start": start_time, "end": end_time}
            }
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies = data_processor.detect_anomalies(df, "all", {})
        
        # æŒ‰æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
        anomalies.sort(key=lambda x: x.get('timestamp', 0), reverse=True)
        limited_anomalies = anomalies[:limit]
        
        return {
            "success": True,
            "anomalies": convert_numpy_types(limited_anomalies),
            "total_count": len(anomalies),
            "time_range": {"start": start_time, "end": end_time}
        }
        
    except Exception as e:
        print(f"è·å–å®æ—¶å¼‚å¸¸å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"è·å–å®æ—¶å¼‚å¸¸å¤±è´¥: {str(e)}",
            "anomalies": [],
            "total_count": 0
        }

@router.get("/anomaly/types", response_model=dict)
async def get_anomaly_types():
    """
    è·å–æ”¯æŒçš„å¼‚å¸¸ç±»å‹åˆ—è¡¨
    """
    anomaly_types = [
        {
            "type": "long_stop",
            "name": "é•¿æ—¶é—´åœè½¦",
            "description": "è½¦è¾†åœ¨åŒä¸€ä½ç½®åœç•™æ—¶é—´è¿‡é•¿",
            "icon": "parking",
            "color": "#ff6b6b",
            "default_threshold": {"duration": 300}
        },
        {
            "type": "abnormal_route",
            "name": "å¼‚å¸¸ç»•è·¯",
            "description": "è½¦è¾†è¡Œé©¶è·¯å¾„æ˜æ˜¾åç¦»æ­£å¸¸è·¯çº¿",
            "icon": "route",
            "color": "#ffa726",
            "default_threshold": {"detour_ratio": 1.5}
        },
        {
            "type": "speed_anomaly",
            "name": "é€Ÿåº¦å¼‚å¸¸",
            "description": "è½¦è¾†é€Ÿåº¦å¼‚å¸¸ï¼ˆè¿‡å¿«æˆ–è¿‡æ…¢ï¼‰",
            "icon": "speed",
            "color": "#42a5f5",
            "default_threshold": {"low": 5, "high": 80}
        },
        {
            "type": "cluster_anomaly",
            "name": "å¼‚å¸¸èšé›†",
            "description": "è½¦è¾†åœ¨ç‰¹å®šåŒºåŸŸå¼‚å¸¸èšé›†",
            "icon": "cluster",
            "color": "#ab47bc",
            "default_threshold": {"density": 50}
        },
        {
            "type": "trajectory_anomaly",
            "name": "è½¨è¿¹å¼‚å¸¸",
            "description": "è½¦è¾†è½¨è¿¹æ¨¡å¼å¼‚å¸¸",
            "icon": "trajectory",
            "color": "#66bb6a",
            "default_threshold": {"pattern_threshold": 0.7}
        }
    ]
    
    return {
        "success": True,
        "anomaly_types": anomaly_types
    }

@router.get("/anomaly/heatmap", response_model=dict)
async def get_anomaly_heatmap(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    anomaly_type: str = Query("all", description="å¼‚å¸¸ç±»å‹"),
    resolution: float = Query(0.002, description="çƒ­åŠ›å›¾åˆ†è¾¨ç‡")
):
    """
    è·å–å¼‚å¸¸äº‹ä»¶çƒ­åŠ›å›¾æ•°æ®
    """
    try:
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                "heatmap_points": []
            }
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies = data_processor.detect_anomalies(df, anomaly_type, {})
        
        # ç”Ÿæˆå¼‚å¸¸çƒ­åŠ›å›¾
        heatmap_points = data_processor.generate_anomaly_heatmap(anomalies, resolution)
        
        return {
            "success": True,
            "heatmap_points": convert_numpy_types(heatmap_points),
            "total_anomalies": len(anomalies),
            "resolution": resolution
        }
        
    except Exception as e:
        print(f"ç”Ÿæˆå¼‚å¸¸çƒ­åŠ›å›¾å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"ç”Ÿæˆå¼‚å¸¸çƒ­åŠ›å›¾å¤±è´¥: {str(e)}",
            "heatmap_points": []
        }

@router.get("/spatiotemporal/dynamic-heatmap", response_model=DynamicHeatmapResponse)
async def get_dynamic_heatmap(
    start_time: str,
    end_time: str,
    temporal_resolution: int = 15,
    spatial_resolution: float = 0.001,
    smoothing: bool = True
):
    """
    è·å–åŠ¨æ€çƒ­åŠ›å›¾æ•°æ®
    
    Args:
        start_time: å¼€å§‹æ—¶é—´ (ISOæ ¼å¼)
        end_time: ç»“æŸæ—¶é—´ (ISOæ ¼å¼)
        temporal_resolution: æ—¶é—´åˆ†è¾¨ç‡ï¼ˆåˆ†é’Ÿï¼‰
        spatial_resolution: ç©ºé—´åˆ†è¾¨ç‡ï¼ˆåº¦ï¼‰
        smoothing: æ˜¯å¦å¹³æ»‘å¤„ç†
    """
    try:
        logger.info(f"è·å–åŠ¨æ€çƒ­åŠ›å›¾: {start_time} to {end_time}, æ—¶é—´åˆ†è¾¨ç‡: {temporal_resolution}åˆ†é’Ÿ")
        
        # æ—¶é—´æ ¼å¼è½¬æ¢
        start_timestamp = convert_time_to_timestamp(start_time)
        end_timestamp = convert_time_to_timestamp(end_time)
        
        # è·å–æ•°æ®
        processor = TrafficDataProcessor()
        df = processor.get_data_by_time_range(start_timestamp, end_timestamp)
        
        if df.empty:
            return DynamicHeatmapResponse(
                success=True,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®",
                frames=[],
                time_series_stats={},
                spatial_stats={}
            )
        
        # ç”ŸæˆåŠ¨æ€çƒ­åŠ›å›¾
        frames = processor.generate_dynamic_heatmap(
            df,
            temporal_resolution=temporal_resolution,
            spatial_resolution=spatial_resolution,
            smoothing=smoothing
        )
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        time_series_stats = processor._calculate_time_series_stats(frames)
        spatial_stats = processor._calculate_spatial_stats(df)
        
        # è½¬æ¢numpyç±»å‹
        frames = convert_numpy_types(frames)
        time_series_stats = convert_numpy_types(time_series_stats)
        spatial_stats = convert_numpy_types(spatial_stats)
        
        return DynamicHeatmapResponse(
            success=True,
            message=f"æˆåŠŸç”Ÿæˆ{len(frames)}ä¸ªæ—¶é—´å¸§çš„åŠ¨æ€çƒ­åŠ›å›¾",
            frames=frames,
            time_series_stats=time_series_stats,
            spatial_stats=spatial_stats
        )
        
    except Exception as e:
        logger.error(f"è·å–åŠ¨æ€çƒ­åŠ›å›¾æ—¶å‡ºé”™: {str(e)}")
        return DynamicHeatmapResponse(
            success=False,
            message=f"è·å–åŠ¨æ€çƒ­åŠ›å›¾å¤±è´¥: {str(e)}",
            frames=[],
            time_series_stats={},
            spatial_stats={}
        )

@router.post("/spatiotemporal/clustering", response_model=ClusteringResponse)
async def perform_clustering_analysis(
    start_time: str,
    end_time: str,
    request: ClusteringRequest
):
    """
    æ‰§è¡Œèšç±»åˆ†æ
    
    Args:
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        request: èšç±»è¯·æ±‚å‚æ•°
    """
    try:
        logger.info(f"æ‰§è¡Œèšç±»åˆ†æ: {request.algorithm}, æ•°æ®ç±»å‹: {request.data_type}")
        
        # æ—¶é—´è½¬æ¢
        start_timestamp = convert_time_to_timestamp(start_time)
        end_timestamp = convert_time_to_timestamp(end_time)
        
        # è·å–æ•°æ®
        processor = TrafficDataProcessor()
        df = processor.get_data_by_time_range(start_timestamp, end_timestamp)
        
        if df.empty:
            return ClusteringResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®",
                clusters=[],
                algorithm_used=request.algorithm,
                parameters=request.params,
                statistics={}
            )
        
        # æ‰§è¡Œèšç±»åˆ†æ
        clusters, metrics = processor.perform_clustering_analysis(
            df,
            data_type=request.data_type,
            algorithm=request.algorithm,
            params=request.params
        )
        
        # è½¬æ¢æ•°æ®ç±»å‹
        clusters = convert_numpy_types(clusters)
        metrics = convert_numpy_types(metrics)
        
        return ClusteringResponse(
            success=True,
            message=f"èšç±»åˆ†æå®Œæˆï¼Œå‘ç°{len(clusters)}ä¸ªèšç±»",
            clusters=clusters,
            algorithm_used=request.algorithm,
            parameters=request.params,
            statistics=metrics
        )
        
    except Exception as e:
        logger.error(f"èšç±»åˆ†ææ—¶å‡ºé”™: {str(e)}")
        return ClusteringResponse(
            success=False,
            message=f"èšç±»åˆ†æå¤±è´¥: {str(e)}",
            clusters=[],
            algorithm_used=request.algorithm,
            parameters=request.params,
            statistics={}
        )

@router.post("/spatiotemporal/od-analysis", response_model=ODFlowResponse)
async def perform_od_analysis(
    start_time: str,
    end_time: str,
    request: ODAnalysisRequest
):
    """
    æ‰§è¡ŒODå¯¹åˆ†æ
    
    Args:
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        request: ODåˆ†æè¯·æ±‚å‚æ•°
    """
    try:
        logger.info(f"æ‰§è¡ŒODå¯¹åˆ†æ: æœ€å°è¡Œç¨‹æ—¶é—´ {request.min_trip_duration}ç§’")
        
        # æ—¶é—´è½¬æ¢
        start_timestamp = convert_time_to_timestamp(start_time)
        end_timestamp = convert_time_to_timestamp(end_time)
        
        # è·å–æ•°æ®
        processor = TrafficDataProcessor()
        df = processor.get_data_by_time_range(start_timestamp, end_timestamp)
        
        if df.empty:
            return ODFlowResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®",
                od_pairs=[],
                top_flows=[],
                statistics={}
            )
        
        # æå–ODå¯¹
        od_pairs = processor.extract_od_pairs_from_data(
            df,
            min_trip_duration=request.min_trip_duration,
            max_trip_duration=request.max_trip_duration,
            min_trip_distance=request.min_trip_distance
        )
        
        if not od_pairs:
            return ODFlowResponse(
                success=True,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ODå¯¹",
                od_pairs=[],
                top_flows=[],
                statistics={}
            )
        
        # ODæµé‡åˆ†æ
        from .od_analysis_engine import ODAnalysisEngine
        od_engine = ODAnalysisEngine()
        od_engine.od_pairs = od_pairs
        
        # åˆ†æé¡¶çº§æµé‡
        top_flows = od_engine.analyze_top_flows(od_pairs, top_k=20)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        statistics = od_engine.calculate_od_statistics(od_pairs)
        
        # ç”Ÿæˆæµé‡çŸ©é˜µï¼ˆå¦‚æœè¯·æ±‚èšåˆçº§åˆ«éœ€è¦ï¼‰
        flow_matrix = None
        if request.aggregate_level == "grid":
            matrix, grid_info = od_engine.generate_flow_matrix(od_pairs)
            if matrix.size > 0:
                flow_matrix = matrix.tolist()
                statistics['grid_info'] = grid_info
        
        # è½¬æ¢æ•°æ®ç±»å‹
        od_pairs = convert_numpy_types(od_pairs)
        top_flows = convert_numpy_types(top_flows)
        statistics = convert_numpy_types(statistics)
        
        return ODFlowResponse(
            success=True,
            message=f"ODåˆ†æå®Œæˆï¼Œæ‰¾åˆ°{len(od_pairs)}ä¸ªODå¯¹",
            od_pairs=od_pairs,
            flow_matrix=flow_matrix,
            top_flows=top_flows,
            statistics=statistics
        )
        
    except Exception as e:
        logger.error(f"ODåˆ†ææ—¶å‡ºé”™: {str(e)}")
        return ODFlowResponse(
            success=False,
            message=f"ODåˆ†æå¤±è´¥: {str(e)}",
            od_pairs=[],
            top_flows=[],
            statistics={}
        )

@router.post("/spatiotemporal/comprehensive", response_model=SpatioTemporalResponse)
async def perform_comprehensive_analysis(
    start_time: str,
    end_time: str,
    heatmap_request: HeatmapRequest
):
    """
    æ‰§è¡Œç»¼åˆæ—¶ç©ºåˆ†æ
    
    Args:
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        heatmap_request: çƒ­åŠ›å›¾åˆ†æå‚æ•°
    """
    try:
        logger.info(f"æ‰§è¡Œç»¼åˆæ—¶ç©ºåˆ†æ: {start_time} to {end_time}")
        
        # æ—¶é—´è½¬æ¢
        start_timestamp = convert_time_to_timestamp(start_time)
        end_timestamp = convert_time_to_timestamp(end_time)
        
        # è·å–æ•°æ®
        processor = TrafficDataProcessor()
        df = processor.get_data_by_time_range(start_timestamp, end_timestamp)
        
        if df.empty:
            return SpatioTemporalResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®",
                analysis_type="comprehensive",
                data=SpatioTemporalAnalysis(
                    analysis_type="comprehensive",
                    time_range={},
                    spatial_bounds={},
                    data=[],
                    statistics={},
                    algorithm_params={}
                )
            )
        
        # ç”Ÿæˆæ—¶ç©ºçƒ­åŠ›å›¾åˆ†æ
        analysis_result = processor.generate_spatiotemporal_heatmap(
            df,
            analysis_type="comprehensive",
            temporal_resolution=heatmap_request.temporal_resolution,
            spatial_resolution=heatmap_request.spatial_resolution
        )
        
        # æ„é€ å“åº”æ•°æ®
        spatiotemporal_data = SpatioTemporalAnalysis(
            analysis_type="comprehensive",
            time_range=analysis_result['time_range'],
            spatial_bounds=analysis_result['spatial_bounds'],
            data=analysis_result['heatmap_frames'],  # æ—¶é—´å¸§æ•°æ®
            statistics={
                'time_series_stats': analysis_result['time_series_stats'],
                'spatial_stats': analysis_result['spatial_stats']
            },
            algorithm_params=analysis_result['algorithm_params']
        )
        
        # è½¬æ¢æ•°æ®ç±»å‹
        spatiotemporal_data_dict = convert_numpy_types(spatiotemporal_data.dict())
        
        return SpatioTemporalResponse(
            success=True,
            message="ç»¼åˆæ—¶ç©ºåˆ†æå®Œæˆ",
            analysis_type="comprehensive",
            data=SpatioTemporalAnalysis(**spatiotemporal_data_dict),
            processing_time=None
        )
        
    except Exception as e:
        logger.error(f"ç»¼åˆæ—¶ç©ºåˆ†ææ—¶å‡ºé”™: {str(e)}")
        return SpatioTemporalResponse(
            success=False,
            message=f"ç»¼åˆæ—¶ç©ºåˆ†æå¤±è´¥: {str(e)}",
            analysis_type="comprehensive",
            data=SpatioTemporalAnalysis(
                analysis_type="comprehensive",
                time_range={},
                spatial_bounds={},
                data=[],
                statistics={},
                algorithm_params={}
            )
        )

@router.get("/spatiotemporal/algorithms")
async def get_available_algorithms():
    """è·å–å¯ç”¨çš„èšç±»ç®—æ³•åˆ—è¡¨"""
    try:
        from .clustering_engine import ClusteringEngine
        
        clustering_engine = ClusteringEngine()
        algorithms = clustering_engine.get_available_algorithms()
        
        algorithm_info = {}
        for algo in algorithms:
            algorithm_info[algo] = clustering_engine.get_algorithm_params(algo)
        
        return {
            "success": True,
            "message": "è·å–ç®—æ³•åˆ—è¡¨æˆåŠŸ",
            "algorithms": algorithms,
            "algorithm_params": algorithm_info
        }
        
    except Exception as e:
        logger.error(f"è·å–ç®—æ³•åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
        return {
            "success": False,
            "message": f"è·å–ç®—æ³•åˆ—è¡¨å¤±è´¥: {str(e)}",
            "algorithms": [],
            "algorithm_params": {}
        }

# è·¯æ®µåˆ†æç›¸å…³APIæ¥å£

@router.post("/road/analysis", response_model=RoadAnalysisResponse)
async def analyze_road_segments(request: RoadAnalysisRequest):
    """
    è·¯æ®µåˆ†æAPI
    åˆ†æé“è·¯ç½‘ç»œçš„é€šè¡ŒçŠ¶å†µã€é€Ÿåº¦åˆ†å¸ƒã€æ‹¥å µæƒ…å†µç­‰
    """
    try:
        start_time = time.time()
        
        # è·å–å½“å‰æ—¶é—´ä½œä¸ºæ—¶é—´èŒƒå›´ï¼ˆæ¼”ç¤ºç”¨ï¼‰
        current_time = time.time()
        start_timestamp = current_time - 24 * 3600  # 24å°æ—¶å‰
        end_timestamp = current_time
        
        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return RoadAnalysisResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„äº¤é€šæ•°æ®",
                analysis=RoadNetworkAnalysis(
                    analysis_type=request.analysis_type,
                    time_range={"start": start_timestamp, "end": end_timestamp},
                    total_segments=0,
                    segments_data=[],
                    network_summary={},
                    bottleneck_segments=[]
                ),
                speed_distributions=[],
                flow_patterns=[],
                processing_time=time.time() - start_time
            )
        
        # æ‰§è¡Œè·¯æ®µåˆ†æ
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type=request.analysis_type,
            segment_types=request.segment_types,
            aggregation_level=request.aggregation_level,
            min_vehicles=request.min_vehicles
        )
        
        if "error" in analysis_result:
            return RoadAnalysisResponse(
                success=False,
                message=analysis_result["error"],
                analysis=RoadNetworkAnalysis(
                    analysis_type=request.analysis_type,
                    time_range={"start": start_timestamp, "end": end_timestamp},
                    total_segments=0,
                    segments_data=[],
                    network_summary={},
                    bottleneck_segments=[]
                ),
                speed_distributions=[],
                flow_patterns=[],
                processing_time=time.time() - start_time
            )
        
        # æ„å»ºåˆ†æç»“æœ
        metadata = analysis_result.get("analysis_metadata", {})
        time_range = metadata.get("time_range", {"start": start_timestamp, "end": end_timestamp})
        
        # è½¬æ¢è·¯æ®µç»Ÿè®¡æ•°æ®
        segment_stats_data = analysis_result.get("segment_statistics", [])
        segments_data = []
        for stats in segment_stats_data:
            segment_stat = RoadSegmentStatistics(**stats)
            segments_data.append(segment_stat)
        
        # æ„å»ºç½‘ç»œåˆ†æå¯¹è±¡
        network_analysis = RoadNetworkAnalysis(
            analysis_type=request.analysis_type,
            time_range=time_range,
            total_segments=metadata.get("total_segments", 0),
            segments_data=segments_data,
            network_summary=analysis_result.get("network_summary", {}),
            bottleneck_segments=analysis_result.get("bottlenecks", [])
        )
        
        # è½¬æ¢é€Ÿåº¦åˆ†å¸ƒæ•°æ®
        speed_distributions = []
        if "speed_distributions" in analysis_result:
            for dist_data in analysis_result["speed_distributions"]:
                speed_dist = SpeedDistribution(**dist_data)
                speed_distributions.append(speed_dist)
        
        # è½¬æ¢æµé‡æ¨¡å¼æ•°æ®
        flow_patterns = []
        if "flow_patterns" in analysis_result:
            for pattern_data in analysis_result["flow_patterns"]:
                flow_pattern = TrafficFlowPattern(**pattern_data)
                flow_patterns.append(flow_pattern)
        
        processing_time = time.time() - start_time
        
        return RoadAnalysisResponse(
            success=True,
            message=f"æˆåŠŸåˆ†æäº† {metadata.get('active_segments', 0)} ä¸ªè·¯æ®µ",
            analysis=network_analysis,
            speed_distributions=speed_distributions,
            flow_patterns=flow_patterns,
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"è·¯æ®µåˆ†æAPIé”™è¯¯: {str(e)}")
        return RoadAnalysisResponse(
            success=False,
            message=f"åˆ†æå¤±è´¥: {str(e)}",
            analysis=RoadNetworkAnalysis(
                analysis_type=request.analysis_type,
                time_range={"start": 0, "end": 0},
                total_segments=0,
                segments_data=[],
                network_summary={},
                bottleneck_segments=[]
            ),
            speed_distributions=[],
            flow_patterns=[]
        )

@router.get("/road/segments", response_model=RoadSegmentResponse)
async def get_road_segments():
    """
    è·å–è·¯æ®µä¿¡æ¯API
    è¿”å›å½“å‰ç³»ç»Ÿè¯†åˆ«çš„æ‰€æœ‰è·¯æ®µåŸºç¡€ä¿¡æ¯
    """
    try:
        # è·å–æœ€è¿‘çš„æ•°æ®æ¥æå–è·¯æ®µ
        current_time = time.time()
        start_timestamp = current_time - 3600  # 1å°æ—¶å‰
        end_timestamp = current_time
        
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return RoadSegmentResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„æ•°æ®æ¥æå–è·¯æ®µä¿¡æ¯",
                segments=[],
                total_segments=0
            )
        
        # æ‰§è¡Œç®€å•çš„è·¯æ®µåˆ†ææ¥è·å–è·¯æ®µä¿¡æ¯
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type="comprehensive",
            min_vehicles=1  # é™ä½é˜ˆå€¼ä»¥è·å–æ›´å¤šè·¯æ®µ
        )
        
        if "error" in analysis_result:
            return RoadSegmentResponse(
                success=False,
                message=analysis_result["error"],
                segments=[],
                total_segments=0
            )
        
        # è½¬æ¢è·¯æ®µæ•°æ®
        segments_data = analysis_result.get("segments", [])
        segments = []
        for segment_data in segments_data:
            segment = RoadSegment(**segment_data)
            segments.append(segment)
        
        return RoadSegmentResponse(
            success=True,
            message=f"æˆåŠŸè·å– {len(segments)} ä¸ªè·¯æ®µä¿¡æ¯",
            segments=segments,
            total_segments=len(segments)
        )
        
    except Exception as e:
        logger.error(f"è·å–è·¯æ®µä¿¡æ¯APIé”™è¯¯: {str(e)}")
        return RoadSegmentResponse(
            success=False,
            message=f"è·å–å¤±è´¥: {str(e)}",
            segments=[],
            total_segments=0
        )

@router.post("/road/traffic", response_model=RoadTrafficResponse)
async def get_road_traffic_data(time_range: Dict[str, float]):
    """
    è·å–è·¯æ®µäº¤é€šæ•°æ®API
    è¿”å›æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„è·¯æ®µäº¤é€šçŠ¶å†µæ•°æ®
    """
    try:
        start_timestamp = time_range.get("start", time.time() - 3600)
        end_timestamp = time_range.get("end", time.time())
        
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return RoadTrafficResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰äº¤é€šæ•°æ®",
                traffic_data=[],
                statistics={}
            )
        
        # æ‰§è¡Œè·¯æ®µåˆ†æ
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type="comprehensive",
            min_vehicles=5
        )
        
        if "error" in analysis_result:
            return RoadTrafficResponse(
                success=False,
                message=analysis_result["error"],
                traffic_data=[],
                statistics={}
            )
        
        # è½¬æ¢äº¤é€šæ•°æ®
        traffic_data_list = analysis_result.get("traffic_data", [])
        traffic_data = []
        for data in traffic_data_list:
            traffic_item = RoadTrafficData(**data)
            traffic_data.append(traffic_item)
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        statistics = {
            "total_records": len(traffic_data),
            "time_range": {"start": start_timestamp, "end": end_timestamp},
            "avg_speed": np.mean([t.avg_speed for t in traffic_data]) if traffic_data else 0,
            "avg_flow": np.mean([t.flow_rate for t in traffic_data]) if traffic_data else 0,
            "congestion_summary": {}
        }
        
        # æ‹¥å µåˆ†å¸ƒç»Ÿè®¡
        congestion_counts = {}
        for t in traffic_data:
            level = t.congestion_level
            congestion_counts[level] = congestion_counts.get(level, 0) + 1
        statistics["congestion_summary"] = congestion_counts
        
        return RoadTrafficResponse(
            success=True,
            message=f"æˆåŠŸè·å– {len(traffic_data)} æ¡äº¤é€šæ•°æ®è®°å½•",
            traffic_data=traffic_data,
            statistics=statistics
        )
        
    except Exception as e:
        logger.error(f"è·å–äº¤é€šæ•°æ®APIé”™è¯¯: {str(e)}")
        return RoadTrafficResponse(
            success=False,
            message=f"è·å–å¤±è´¥: {str(e)}",
            traffic_data=[],
            statistics={}
        )

@router.post("/road/visualization", response_model=RoadVisualizationResponse)
async def get_road_visualization_data(request: Dict[str, Any]):
    """
    è·å–è·¯æ®µå¯è§†åŒ–æ•°æ®API
    ç”Ÿæˆç”¨äºåœ°å›¾å±•ç¤ºçš„è·¯æ®µå¯è§†åŒ–æ•°æ®
    """
    try:
        visualization_type = request.get("visualization_type", "speed")
        time_range = request.get("time_range", {})
        
        start_timestamp = time_range.get("start", time.time() - 3600)
        end_timestamp = time_range.get("end", time.time())
        
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return RoadVisualizationResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„æ•°æ®è¿›è¡Œå¯è§†åŒ–",
                visualization_data={},
                segment_colors={},
                legend_info={}
            )
        
        # æ‰§è¡Œè·¯æ®µåˆ†æ
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type="comprehensive",
            min_vehicles=3
        )
        
        if "error" in analysis_result:
            return RoadVisualizationResponse(
                success=False,
                message=analysis_result["error"],
                visualization_data={},
                segment_colors={},
                legend_info={}
            )
        
        # ç”Ÿæˆå¯è§†åŒ–æ•°æ®
        segments_data = analysis_result.get("segments", [])
        traffic_data = analysis_result.get("traffic_data", [])
        
        visualization_data = data_processor.generate_road_visualization_data(
            segments_data,
            traffic_data,
            visualization_type
        )
        
        if "error" in visualization_data:
            return RoadVisualizationResponse(
                success=False,
                message=visualization_data["error"],
                visualization_data={},
                segment_colors={},
                legend_info={}
            )
        
        return RoadVisualizationResponse(
            success=True,
            message=f"æˆåŠŸç”Ÿæˆ {len(visualization_data.get('segments', []))} ä¸ªè·¯æ®µçš„å¯è§†åŒ–æ•°æ®",
            visualization_data=visualization_data,
            segment_colors=visualization_data.get("color_mapping", {}),
            legend_info=visualization_data.get("legend", {})
        )
        
    except Exception as e:
        logger.error(f"è·¯æ®µå¯è§†åŒ–APIé”™è¯¯: {str(e)}")
        return RoadVisualizationResponse(
            success=False,
            message=f"ç”Ÿæˆå¯è§†åŒ–æ•°æ®å¤±è´¥: {str(e)}",
            visualization_data={},
            segment_colors={},
            legend_info={}
        )

@router.get("/road/metrics", response_model=Dict[str, Any])
async def get_road_network_metrics():
    """
    è·å–è·¯ç½‘æ•´ä½“æŒ‡æ ‡API
    è¿”å›é“è·¯ç½‘ç»œçš„ç»¼åˆæ€§èƒ½æŒ‡æ ‡
    """
    try:
        # ä½¿ç”¨å½“å‰æ—¶é—´èŒƒå›´ï¼ˆæ¼”ç¤ºç”¨ï¼‰
        current_time = time.time()
        start_timestamp = current_time - 24 * 3600  # 24å°æ—¶å‰
        end_timestamp = current_time
        
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return {
                "success": False,
                "message": "æ²¡æœ‰å¯ç”¨çš„äº¤é€šæ•°æ®",
                "metrics": {}
            }
        
        # æ‰§è¡Œè·¯æ®µåˆ†æè·å–åŸºç¡€æ•°æ®
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type="comprehensive",
            min_vehicles=3
        )
        
        if "error" in analysis_result:
            return {
                "success": False,
                "message": analysis_result["error"],
                "metrics": {}
            }
        
        # è®¡ç®—ç½‘ç»œæŒ‡æ ‡
        segments_data = analysis_result.get("segments", [])
        traffic_data = analysis_result.get("traffic_data", [])
        
        metrics = data_processor.calculate_road_network_metrics(segments_data, traffic_data)
        
        return {
            "success": True,
            "metrics": metrics,
            "timestamp": time.time()
        }
    
    except Exception as e:
        logger.error(f"è·å–è·¯ç½‘æŒ‡æ ‡å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"è·å–è·¯ç½‘æŒ‡æ ‡å¤±è´¥: {str(e)}",
            "metrics": {}
        }

@router.get("/weekly-passenger-flow", response_model=Dict[str, Any])
async def get_weekly_passenger_flow_analysis(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰")
):
    """
    è·å–å‘¨å®¢æµé‡åˆ†ææ•°æ®
    
    Args:
        start_time: å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰
        end_time: ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰
    
    Returns:
        å‘¨å®¢æµé‡åˆ†æç»“æœï¼ŒåŒ…æ‹¬æ—¥æµé‡å¯¹æ¯”ã€å·¥ä½œæ—¥vså‘¨æœ«ã€é«˜å³°æ—¶æ®µç­‰
    """
    try:
        logger.info(f"å¼€å§‹å‘¨å®¢æµé‡åˆ†æ: {start_time} - {end_time}")
        
        # éªŒè¯æ—¶é—´èŒƒå›´ï¼ˆç¡®ä¿è‡³å°‘æœ‰3å¤©æ•°æ®ï¼‰
        time_span = end_time - start_time
        if time_span < 3 * 24 * 3600:  # å°‘äº3å¤©
            return {
                "success": False,
                "message": "åˆ†æå‘¨å®¢æµé‡éœ€è¦è‡³å°‘3å¤©çš„æ•°æ®",
                "data": {}
            }
        
        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return {
                "success": False,
                "message": "æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰äº¤é€šæ•°æ®",
                "data": {}
            }
        
        # æ‰§è¡Œå‘¨å®¢æµé‡åˆ†æ
        analysis_result = data_processor.analyze_weekly_passenger_flow(df)
        
        if not analysis_result.get('success', False):
            return {
                "success": False,
                "message": analysis_result.get('message', 'åˆ†æå¤±è´¥'),
                "data": {}
            }
        
        # æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
        analysis_result['metadata'] = {
            'query_start_time': start_time,
            'query_end_time': end_time,
            'analysis_timestamp': time.time(),
            'data_quality': {
                'total_records': len(df),
                'unique_vehicles': df['COMMADDR'].nunique() if 'COMMADDR' in df.columns else 0,
                'time_coverage': analysis_result.get('analysis_period', {})
            }
        }
        
        logger.info(f"å‘¨å®¢æµé‡åˆ†æå®Œæˆï¼Œæ•°æ®è´¨é‡: {analysis_result['metadata']['data_quality']}")
        
        return {
            "success": True,
            "message": "å‘¨å®¢æµé‡åˆ†æå®Œæˆ",
            "data": analysis_result
        }
        
    except Exception as e:
        logger.error(f"å‘¨å®¢æµé‡åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        return {
            "success": False,
            "message": f"åˆ†æå¤±è´¥: {str(e)}",
            "data": {}
        }


# æ™ºèƒ½å®¢è¿ç›‘æ§ç›¸å…³APIæ¥å£

@router.post("/smart-passenger/analysis", response_model=SmartPassengerResponse)
async def analyze_smart_passenger(request: SmartPassengerRequest):
    """
    æ™ºèƒ½å®¢è¿ç›‘æ§åˆ†æAPI
    åˆ†æå¤©æ°”å¯¹å®¢æµçš„å½±å“ï¼Œè½½å®¢å‡ºç§Ÿè½¦éœ€æ±‚ç­‰
    """
    try:
        start_time = time.time()
        
        # è·å–å½“å‰æ—¶é—´ä½œä¸ºæ—¶é—´èŒƒå›´ï¼ˆæ¼”ç¤ºç”¨ï¼‰
        current_time = time.time()
        start_timestamp = current_time - 24 * 3600  # 24å°æ—¶å‰
        end_timestamp = current_time
        
        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return SmartPassengerResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„äº¤é€šæ•°æ®",
                analysis_type=request.analysis_type,
                statistics=None,
                processing_time=time.time() - start_time
            )
        
        # åˆå§‹åŒ–æ™ºèƒ½å®¢è¿åˆ†æå¼•æ“
        from .smart_passenger_engine import SmartPassengerEngine
        smart_engine = SmartPassengerEngine()
        
        # è·å–å¤©æ°”æ•°æ®
        weather_data = []
        if request.include_weather:
            weather_data = smart_engine.get_weather_data(start_timestamp, end_timestamp)
        
        # è¯†åˆ«è½½å®¢è½¦è¾†å’Œå®¢æµæ•°æ®
        passenger_flows = smart_engine.identify_passenger_vehicles(df)
        
        # åˆ†æå‡ºç§Ÿè½¦éœ€æ±‚
        taxi_demand_data = []
        if request.include_taxi_analysis:
            taxi_demand_data = smart_engine.analyze_taxi_demand(df, request.time_resolution)
        
        # åˆ†æå¤©æ°”å½±å“
        weather_impact = []
        if request.weather_correlation and weather_data:
            weather_impact = smart_engine.analyze_weather_impact(passenger_flows, weather_data)
        
        # è®¡ç®—å‡ºç§Ÿè½¦ä¾›éœ€
        taxi_supply_demand = []
        if taxi_demand_data:
            taxi_supply_demand = smart_engine.calculate_taxi_supply_demand(taxi_demand_data)
        
        # ç”Ÿæˆç»Ÿè®¡æ•°æ®
        statistics = smart_engine.generate_smart_passenger_statistics(
            passenger_flows, weather_data, taxi_demand_data, weather_impact,
            (start_timestamp, end_timestamp)
        )
        
        return SmartPassengerResponse(
            success=True,
            message=f"æ™ºèƒ½å®¢è¿ç›‘æ§åˆ†æå®Œæˆï¼Œåˆ†æäº† {len(passenger_flows)} æ¡å®¢æµæ•°æ®",
            analysis_type=request.analysis_type,
            statistics=statistics,
            weather_impact=weather_impact if weather_impact else None,
            taxi_demand=taxi_supply_demand if taxi_supply_demand else None,
            processing_time=time.time() - start_time
        )
        
    except Exception as e:
        logger.error(f"æ™ºèƒ½å®¢è¿ç›‘æ§åˆ†æå¤±è´¥: {str(e)}")
        return SmartPassengerResponse(
            success=False,
            message=f"åˆ†æå¤±è´¥: {str(e)}",
            analysis_type=request.analysis_type,
            statistics=None,
            processing_time=time.time() - start_time if 'start_time' in locals() else 0
        )

@router.post("/smart-passenger/weather-impact", response_model=WeatherImpactResponse)
async def analyze_weather_impact(request: WeatherImpactRequest):
    """
    å¤©æ°”å½±å“åˆ†æAPI
    ä¸“é—¨åˆ†æå¤©æ°”å˜åŒ–å¯¹å®¢æµé‡çš„å½±å“
    """
    try:
        # è·å–æ—¶é—´èŒƒå›´
        current_time = time.time()
        start_timestamp = current_time - 7 * 24 * 3600  # 7å¤©å‰ï¼Œè·å–æ›´å¤šå¤©æ°”æ ·æœ¬
        end_timestamp = current_time
        
        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return WeatherImpactResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„äº¤é€šæ•°æ®",
                weather_impact_analysis=[],
                correlation_matrix={},
                weather_stats={}
            )
        
        # åˆå§‹åŒ–æ™ºèƒ½å®¢è¿åˆ†æå¼•æ“
        from .smart_passenger_engine import SmartPassengerEngine
        smart_engine = SmartPassengerEngine()
        
        # è·å–å¤©æ°”æ•°æ®
        weather_data = smart_engine.get_weather_data(start_timestamp, end_timestamp)
        
        # è¯†åˆ«å®¢æµæ•°æ®
        passenger_flows = smart_engine.identify_passenger_vehicles(df)
        
        # åˆ†æå¤©æ°”å½±å“
        weather_impact_analysis = smart_engine.analyze_weather_impact(passenger_flows, weather_data)
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix = {}
        for impact in weather_impact_analysis:
            correlation_matrix[impact.weather_condition] = impact.correlation_coefficient
        
        # ç”Ÿæˆå¤©æ°”ç»Ÿè®¡
        weather_stats = {
            "total_weather_records": len(weather_data),
            "weather_type_distribution": {},
            "avg_temperature": np.mean([w.temperature for w in weather_data]) if weather_data else 0,
            "avg_precipitation": np.mean([w.precipitation for w in weather_data]) if weather_data else 0
        }
        
        # ç»Ÿè®¡å¤©æ°”ç±»å‹åˆ†å¸ƒ
        for weather in weather_data:
            weather_type = weather.weather_type
            weather_stats["weather_type_distribution"][weather_type] = \
                weather_stats["weather_type_distribution"].get(weather_type, 0) + 1
        
        return WeatherImpactResponse(
            success=True,
            message=f"å¤©æ°”å½±å“åˆ†æå®Œæˆï¼Œåˆ†æäº† {len(weather_data)} æ¡å¤©æ°”æ•°æ®",
            weather_impact_analysis=weather_impact_analysis,
            correlation_matrix=correlation_matrix,
            weather_stats=weather_stats,
            prediction_data=None  # æš‚ä¸å®ç°é¢„æµ‹åŠŸèƒ½
        )
        
    except Exception as e:
        logger.error(f"å¤©æ°”å½±å“åˆ†æå¤±è´¥: {str(e)}")
        return WeatherImpactResponse(
            success=False,
            message=f"åˆ†æå¤±è´¥: {str(e)}",
            weather_impact_analysis=[],
            correlation_matrix={},
            weather_stats={}
        )

@router.post("/smart-passenger/taxi-demand", response_model=TaxiDemandResponse)
async def analyze_taxi_demand(request: TaxiDemandRequest):
    """
    å‡ºç§Ÿè½¦éœ€æ±‚åˆ†æAPI
    åŠ¨æ€ç›‘æ§è½½å®¢å‡ºç§Ÿè½¦æ•°é‡å’Œéœ€æ±‚æƒ…å†µ
    """
    try:
        # è·å–æ—¶é—´èŒƒå›´ï¼ˆå®æ—¶ç›‘æ§ä½¿ç”¨è¾ƒçŸ­æ—¶é—´èŒƒå›´ï¼‰
        current_time = time.time()
        if request.real_time_monitoring:
            start_timestamp = current_time - 3600  # 1å°æ—¶å‰
        else:
            start_timestamp = current_time - 24 * 3600  # 24å°æ—¶å‰
        end_timestamp = current_time
        
        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return TaxiDemandResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„äº¤é€šæ•°æ®",
                supply_demand_analysis=[],
                real_time_status={},
                hotspot_visualization={}
            )
        
        # åˆå§‹åŒ–æ™ºèƒ½å®¢è¿åˆ†æå¼•æ“
        from .smart_passenger_engine import SmartPassengerEngine
        smart_engine = SmartPassengerEngine()
        
        # åˆ†æå‡ºç§Ÿè½¦éœ€æ±‚
        taxi_demand_data = smart_engine.analyze_taxi_demand(df)
        
        # è®¡ç®—ä¾›éœ€åˆ†æ
        supply_demand_analysis = smart_engine.calculate_taxi_supply_demand(taxi_demand_data)
        
        # ç”Ÿæˆå®æ—¶çŠ¶æ€
        current_data = [td for td in taxi_demand_data 
                       if abs(td.timestamp - current_time) < 900]  # 15åˆ†é’Ÿå†…
        
        real_time_status = {
            "current_time": current_time,
            "active_loaded_taxis": sum(td.loaded_taxis for td in current_data),
            "active_empty_taxis": sum(td.empty_taxis for td in current_data),
            "current_demand": sum(td.total_orders for td in current_data),
            "overall_supply_ratio": 0,
            "demand_level": "normal"
        }
        
        if real_time_status["current_demand"] > 0:
            real_time_status["overall_supply_ratio"] = \
                real_time_status["active_loaded_taxis"] / real_time_status["current_demand"]
        
        # åˆ¤æ–­éœ€æ±‚ç­‰çº§
        if real_time_status["overall_supply_ratio"] < 0.5:
            real_time_status["demand_level"] = "high"
        elif real_time_status["overall_supply_ratio"] > 1.5:
            real_time_status["demand_level"] = "low"
        
        # ç”Ÿæˆçƒ­ç‚¹å¯è§†åŒ–æ•°æ®
        hotspot_visualization = {
            "high_demand_zones": [],
            "low_supply_zones": [],
            "heatmap_data": []
        }
        
        for td in current_data:
            if td.demand_index > 0.7:
                hotspot_visualization["high_demand_zones"].append({
                    "location": td.location,
                    "demand_index": td.demand_index,
                    "orders": td.total_orders
                })
            
            if td.supply_ratio < 0.5:
                hotspot_visualization["low_supply_zones"].append({
                    "location": td.location,
                    "supply_ratio": td.supply_ratio,
                    "waiting_orders": td.waiting_orders
                })
            
            hotspot_visualization["heatmap_data"].append({
                "lat": td.location["lat"],
                "lng": td.location["lng"],
                "intensity": td.demand_index
            })
        
        return TaxiDemandResponse(
            success=True,
            message=f"å‡ºç§Ÿè½¦éœ€æ±‚åˆ†æå®Œæˆï¼Œåˆ†æäº† {len(taxi_demand_data)} ä¸ªåŒºåŸŸ",
            supply_demand_analysis=supply_demand_analysis,
            real_time_status=real_time_status,
            demand_forecasting=None,  # æš‚ä¸å®ç°é¢„æµ‹åŠŸèƒ½
            hotspot_visualization=hotspot_visualization
        )
        
    except Exception as e:
        logger.error(f"å‡ºç§Ÿè½¦éœ€æ±‚åˆ†æå¤±è´¥: {str(e)}")
        return TaxiDemandResponse(
            success=False,
            message=f"åˆ†æå¤±è´¥: {str(e)}",
            supply_demand_analysis=[],
            real_time_status={},
            hotspot_visualization={}
        )

@router.post("/smart-passenger/visualization", response_model=PassengerVisualizationResponse)
async def get_passenger_visualization_data(request: Dict[str, Any]):
    """
    å®¢è¿å¯è§†åŒ–æ•°æ®API
    ç”Ÿæˆå®¢æµçƒ­åŠ›å›¾ã€å¤©æ°”å…³è”å›¾è¡¨ã€å‡ºç§Ÿè½¦éœ€æ±‚åœ°å›¾ç­‰å¯è§†åŒ–æ•°æ®
    """
    try:
        visualization_type = request.get("visualization_type", "comprehensive")
        time_range = request.get("time_range", {})
        
        start_timestamp = time_range.get("start", time.time() - 3600)
        end_timestamp = time_range.get("end", time.time())
        
        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return PassengerVisualizationResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„æ•°æ®è¿›è¡Œå¯è§†åŒ–",
                passenger_heatmap={},
                weather_correlation_chart={},
                taxi_demand_map={},
                time_series_data={}
            )
        
        # åˆå§‹åŒ–æ™ºèƒ½å®¢è¿åˆ†æå¼•æ“
        from .smart_passenger_engine import SmartPassengerEngine
        smart_engine = SmartPassengerEngine()
        
        # è·å–åŸºç¡€æ•°æ®
        weather_data = smart_engine.get_weather_data(start_timestamp, end_timestamp)
        passenger_flows = smart_engine.identify_passenger_vehicles(df)
        taxi_demand_data = smart_engine.analyze_taxi_demand(df)
        
        # ç”Ÿæˆå®¢æµçƒ­åŠ›å›¾æ•°æ®
        passenger_heatmap = {
            "heatmap_points": [
                {
                    "lat": pf.location["lat"],
                    "lng": pf.location["lng"],
                    "intensity": pf.passenger_count,
                    "type": "pickup" if pf.is_pickup else "dropoff"
                }
                for pf in passenger_flows
            ],
            "legend": {
                "pickup": "ä¸Šå®¢ç‚¹",
                "dropoff": "ä¸‹å®¢ç‚¹"
            }
        }
        
        # ç”Ÿæˆå¤©æ°”å…³è”å›¾è¡¨æ•°æ®
        weather_correlation_chart = {
            "weather_passenger_correlation": [],
            "time_series": []
        }
        
        if weather_data and passenger_flows:
            weather_impact = smart_engine.analyze_weather_impact(passenger_flows, weather_data)
            for impact in weather_impact:
                weather_correlation_chart["weather_passenger_correlation"].append({
                    "weather_type": impact.weather_condition,
                    "impact_percentage": impact.impact_percentage,
                    "correlation": impact.correlation_coefficient
                })
        
        # ç”Ÿæˆå‡ºç§Ÿè½¦éœ€æ±‚åœ°å›¾æ•°æ®
        taxi_demand_map = {
            "demand_zones": [
                {
                    "lat": td.location["lat"],
                    "lng": td.location["lng"],
                    "loaded_taxis": td.loaded_taxis,
                    "demand_index": td.demand_index,
                    "supply_ratio": td.supply_ratio
                }
                for td in taxi_demand_data
            ],
            "supply_demand_legend": {
                "high_demand": {"color": "#ff4444", "threshold": 0.7},
                "medium_demand": {"color": "#ffaa44", "threshold": 0.4},
                "low_demand": {"color": "#44ff44", "threshold": 0.0}
            }
        }
        
        # ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
        time_series_data = {
            "passenger_flow_trend": [],
            "weather_trend": [],
            "taxi_demand_trend": []
        }
        
        # æŒ‰å°æ—¶èšåˆæ—¶é—´åºåˆ—
        hourly_passenger = defaultdict(int)
        hourly_weather = defaultdict(list)
        hourly_taxi = defaultdict(list)
        
        for pf in passenger_flows:
            hour = datetime.fromtimestamp(pf.timestamp).strftime("%Y-%m-%d %H:00")
            hourly_passenger[hour] += pf.passenger_count
        
        for w in weather_data:
            hour = datetime.fromtimestamp(w.timestamp).strftime("%Y-%m-%d %H:00")
            hourly_weather[hour].append(w)
        
        for td in taxi_demand_data:
            hour = datetime.fromtimestamp(td.timestamp).strftime("%Y-%m-%d %H:00")
            hourly_taxi[hour].append(td)
        
        for hour in sorted(set(list(hourly_passenger.keys()) + list(hourly_weather.keys()))):
            time_series_data["passenger_flow_trend"].append({
                "time": hour,
                "passenger_count": hourly_passenger.get(hour, 0)
            })
            
            if hour in hourly_weather:
                avg_temp = np.mean([w.temperature for w in hourly_weather[hour]])
                avg_precip = np.mean([w.precipitation for w in hourly_weather[hour]])
                time_series_data["weather_trend"].append({
                    "time": hour,
                    "temperature": avg_temp,
                    "precipitation": avg_precip
                })
            
            if hour in hourly_taxi:
                avg_demand = np.mean([td.demand_index for td in hourly_taxi[hour]])
                total_loaded = sum(td.loaded_taxis for td in hourly_taxi[hour])
                time_series_data["taxi_demand_trend"].append({
                    "time": hour,
                    "demand_index": avg_demand,
                    "loaded_taxis": total_loaded
                })
        
        return PassengerVisualizationResponse(
            success=True,
            message=f"æˆåŠŸç”Ÿæˆå¯è§†åŒ–æ•°æ®ï¼ŒåŒ…å« {len(passenger_flows)} ä¸ªå®¢æµç‚¹",
            passenger_heatmap=passenger_heatmap,
            weather_correlation_chart=weather_correlation_chart,
            taxi_demand_map=taxi_demand_map,
            time_series_data=time_series_data
        )
        
    except Exception as e:
        logger.error(f"å®¢è¿å¯è§†åŒ–æ•°æ®ç”Ÿæˆå¤±è´¥: {str(e)}")
        return PassengerVisualizationResponse(
            success=False,
            message=f"ç”Ÿæˆå¯è§†åŒ–æ•°æ®å¤±è´¥: {str(e)}",
            passenger_heatmap={},
            weather_correlation_chart={},
            taxi_demand_map={},
            time_series_data={}
        )

@router.get("/api/smart-passenger/real-time", response_model=Dict[str, Any])
async def get_real_time_passenger_monitoring():
    """
    å®æ—¶å®¢è¿ç›‘æ§API
    è·å–å½“å‰æ—¶æ®µçš„å®æ—¶å®¢æµå’Œè½½å®¢è½¦è¾†çŠ¶æ€
    """
    try:
        current_time = time.time()
        start_timestamp = current_time - 900  # 15åˆ†é’Ÿå‰
        end_timestamp = current_time
        
        # åŠ è½½å®æ—¶æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return {
                "success": False,
                "message": "æ²¡æœ‰å®æ—¶æ•°æ®",
                "real_time_data": {}
            }
        
        # åˆå§‹åŒ–æ™ºèƒ½å®¢è¿åˆ†æå¼•æ“
        from .smart_passenger_engine import SmartPassengerEngine
        smart_engine = SmartPassengerEngine()
        
        # è·å–å®æ—¶æ•°æ®
        passenger_flows = smart_engine.identify_passenger_vehicles(df)
        taxi_demand_data = smart_engine.analyze_taxi_demand(df, 5)  # 5åˆ†é’Ÿåˆ†è¾¨ç‡
        
        # è®¡ç®—å®æ—¶æŒ‡æ ‡
        real_time_data = {
            "current_timestamp": current_time,
            "time_window": "15min",
            "passenger_stats": {
                "active_passengers": sum(pf.passenger_count for pf in passenger_flows),
                "pickup_points": len([pf for pf in passenger_flows if pf.is_pickup]),
                "dropoff_points": len([pf for pf in passenger_flows if not pf.is_pickup])
            },
            "taxi_stats": {
                "loaded_taxis": sum(td.loaded_taxis for td in taxi_demand_data),
                "empty_taxis": sum(td.empty_taxis for td in taxi_demand_data),
                "total_demand": sum(td.total_orders for td in taxi_demand_data),
                "avg_demand_index": np.mean([td.demand_index for td in taxi_demand_data]) if taxi_demand_data else 0
            },
            "status_indicators": {
                "demand_level": "normal",
                "supply_status": "adequate",
                "traffic_flow": "smooth"
            }
        }
        
        # åˆ¤æ–­çŠ¶æ€æŒ‡æ ‡
        if real_time_data["taxi_stats"]["avg_demand_index"] > 0.7:
            real_time_data["status_indicators"]["demand_level"] = "high"
        elif real_time_data["taxi_stats"]["avg_demand_index"] < 0.3:
            real_time_data["status_indicators"]["demand_level"] = "low"
        
        supply_ratio = (real_time_data["taxi_stats"]["loaded_taxis"] / 
                       max(real_time_data["taxi_stats"]["total_demand"], 1))
        if supply_ratio < 0.5:
            real_time_data["status_indicators"]["supply_status"] = "shortage"
        elif supply_ratio > 1.5:
            real_time_data["status_indicators"]["supply_status"] = "surplus"
        
        return {
            "success": True,
            "message": "å®æ—¶ç›‘æ§æ•°æ®è·å–æˆåŠŸ",
            "real_time_data": real_time_data
        }
        
    except Exception as e:
        logger.error(f"å®æ—¶å®¢è¿ç›‘æ§å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"å®æ—¶ç›‘æ§å¤±è´¥: {str(e)}",
            "real_time_data": {}
        }

# ===== è·¯ç¨‹åˆ†æå’Œè®¢å•é€Ÿåº¦åˆ†æAPIæ¥å£ =====

@router.post("/road/trip-analysis", response_model=TripAnalysisResponse)
async def analyze_trip_distance_classification(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    request: TripAnalysisRequest = None
):
    """
    è·¯ç¨‹åˆ†æåŠŸèƒ½
    è§„å®šä»¥å°äº4åƒç±³ä¸ºçŸ­é€”ï¼Œ4åƒç±³è‡³8åƒç±³ä¸ºä¸­é€”ï¼Œå¤§äº8åƒç±³ä¸ºé•¿é€”
    æŸ¥çœ‹æ¯å¤©ä¸‰ç§è·ç¦»è¿è¾“çš„å æ¯”ï¼Œå®Œæˆè·¯ç¨‹åˆ†æçš„å¯è§†åŒ–å±•ç¤º
    """
    try:
        start_processing = time.time()
        
        # é»˜è®¤è¯·æ±‚å‚æ•°
        if request is None:
            request = TripAnalysisRequest()
        
        # æ ¹æ®é€‰æ‹©çš„æ—¥æœŸè°ƒæ•´æ—¶é—´èŒƒå›´
        if request.selected_date and request.selected_date != 'all':
            # å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³
            from datetime import datetime
            date_obj = datetime.strptime(request.selected_date, '%Y-%m-%d')
            # è®¾ç½®ä¸ºè¯¥æ—¥æœŸçš„å¼€å§‹å’Œç»“æŸæ—¶é—´ï¼ˆUTCï¼‰
            day_start = int(date_obj.timestamp())
            day_end = day_start + 86400 - 1  # 86400ç§’ = 24å°æ—¶
            
            logger.info(f"é€‰æ‹©ç‰¹å®šæ—¥æœŸ {request.selected_date}: {day_start} - {day_end}")
            actual_start_time = day_start
            actual_end_time = day_end
        else:
            logger.info(f"é€‰æ‹©å…¨éƒ¨æ—¥æœŸ: {start_time} - {end_time}")
            actual_start_time = start_time
            actual_end_time = end_time
        
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(actual_start_time, actual_end_time)
        
        if df.empty:
            return TripAnalysisResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                analysis_result=TripAnalysisStatistics(
                    time_range={"start": actual_start_time, "end": actual_end_time},
                    daily_classifications=[],
                    overall_stats={},
                    trend_analysis={}
                ),
                visualization_data={}
            )
        
        # åˆ›å»ºè·¯æ®µåˆ†æå¼•æ“
        from .road_analysis_engine import RoadAnalysisEngine
        road_engine = RoadAnalysisEngine()
        
        # æ ‡å‡†åŒ–æ•°æ®åˆ—åå¹¶è°ƒè¯•
        logger.info(f"åŸå§‹æ•°æ®åˆ—å: {list(df.columns)}")
        logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        logger.info(f"å‰å‡ è¡Œæ•°æ®: {df.head(2).to_dict()}")
        
        if 'COMMADDR' in df.columns:
            df['vehicle_id'] = df['COMMADDR']
        if 'UTC' in df.columns:
            df['timestamp'] = df['UTC']
        if 'LAT' in df.columns and 'LON' in df.columns:
            df['latitude'] = df['LAT'] / 1e5
            df['longitude'] = df['LON'] / 1e5
        
        logger.info(f"æ ‡å‡†åŒ–åæ•°æ®åˆ—å: {list(df.columns)}")
        logger.info(f"è½¦è¾†IDæ ·æœ¬: {df['vehicle_id'].head(3).tolist() if 'vehicle_id' in df.columns else 'N/A'}")
        logger.info(f"æ—¶é—´æˆ³æ ·æœ¬: {df['timestamp'].head(3).tolist() if 'timestamp' in df.columns else 'N/A'}")
        logger.info(f"åæ ‡æ ·æœ¬: lat={df['latitude'].head(3).tolist() if 'latitude' in df.columns else 'N/A'}, lng={df['longitude'].head(3).tolist() if 'longitude' in df.columns else 'N/A'}")
        
        # è¿›è¡Œè·¯ç¨‹åˆ†æ
        analysis_result = road_engine.analyze_trip_distance_classification(df)
        
        # ç”Ÿæˆå¯è§†åŒ–æ•°æ®
        visualization_data = {
            "daily_chart": {
                "type": "stacked_bar",
                "title": "æ¯æ—¥è·¯ç¨‹åˆ†ç±»ç»Ÿè®¡",
                "data": [
                    {
                        "date": daily.date,
                        "short_trips": daily.short_trips,
                        "medium_trips": daily.medium_trips,
                        "long_trips": daily.long_trips,
                        "short_percentage": daily.short_percentage,
                        "medium_percentage": daily.medium_percentage,
                        "long_percentage": daily.long_percentage
                    }
                    for daily in analysis_result.daily_classifications
                ]
            },
            "pie_chart": {
                "type": "pie",
                "title": "æ€»ä½“è·¯ç¨‹åˆ†ç±»å æ¯”",
                "data": [
                    {
                        "name": "çŸ­é€” (<4km)",
                        "value": analysis_result.overall_stats.get("overall_short_percentage", 0),
                        "count": analysis_result.overall_stats.get("short_trips_total", 0)
                    },
                    {
                        "name": "ä¸­é€” (4-8km)",
                        "value": analysis_result.overall_stats.get("overall_medium_percentage", 0),
                        "count": analysis_result.overall_stats.get("medium_trips_total", 0)
                    },
                    {
                        "name": "é•¿é€” (>8km)",
                        "value": analysis_result.overall_stats.get("overall_long_percentage", 0),
                        "count": analysis_result.overall_stats.get("long_trips_total", 0)
                    }
                ]
            },
            "trend_chart": {
                "type": "line",
                "title": "è·¯ç¨‹åˆ†ç±»è¶‹åŠ¿åˆ†æ",
                "data": {
                    "short_trip_trend": analysis_result.trend_analysis.get("short_trip_trend", "stable"),
                    "medium_trip_trend": analysis_result.trend_analysis.get("medium_trip_trend", "stable"),
                    "long_trip_trend": analysis_result.trend_analysis.get("long_trip_trend", "stable"),
                    "dominant_category": analysis_result.trend_analysis.get("most_common_distance_category", "unknown")
                }
            },
            "statistics_summary": {
                "total_trips": analysis_result.overall_stats.get("total_trips", 0),
                "avg_daily_trips": analysis_result.overall_stats.get("avg_daily_trips", 0),
                "overall_avg_distance": analysis_result.overall_stats.get("overall_avg_distance", 0),
                "analysis_days": len(analysis_result.daily_classifications)
            }
        }
        
        processing_time = time.time() - start_processing
        
        logger.info(f"è·¯ç¨‹åˆ†æå®Œæˆï¼Œå¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        
        return TripAnalysisResponse(
            success=True,
            message=f"è·¯ç¨‹åˆ†æå®Œæˆï¼Œå…±åˆ†æäº† {analysis_result.overall_stats.get('total_trips', 0)} ä¸ªè®¢å•",
            analysis_result=analysis_result,
            visualization_data=visualization_data,
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"è·¯ç¨‹åˆ†æå¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        
        return TripAnalysisResponse(
            success=False,
            message=f"è·¯ç¨‹åˆ†æå¤±è´¥: {str(e)}",
            analysis_result=TripAnalysisStatistics(
                time_range={"start": actual_start_time if 'actual_start_time' in locals() else start_time, 
                           "end": actual_end_time if 'actual_end_time' in locals() else end_time},
                daily_classifications=[],
                overall_stats={"error": str(e)},
                trend_analysis={}
            ),
            visualization_data={}
        )

@router.post("/road/order-speed-analysis", response_model=OrderSpeedAnalysisResponse)
async def analyze_order_based_road_speed(
    start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
    request: OrderSpeedAnalysisRequest = None
):
    """
    åŸºäºè®¢å•çš„é“è·¯é€Ÿåº¦åˆ†æ
    åˆ©ç”¨ä¸­çŸ­é€”è®¢å•æ•°æ®ä¸­çš„é¢„ä¼°è·ç¦»ä¸èµ·æ­¢æ—¶é—´ï¼Œè®¡ç®—è®¢å•çš„å¹³å‡é€Ÿåº¦
    å¤§æ•°æ®èƒŒæ™¯ä¸‹ï¼Œå¤§é‡è®¢å•çš„å¹³å‡é€Ÿåº¦å¯ä»¥å®æ—¶åæ˜ é“è·¯çš„æ‹¥å µçŠ¶å†µ
    å®Œæˆé“è·¯é€Ÿåº¦çš„å¯è§†åŒ–å±•ç¤º
    """
    try:
        start_processing = time.time()
        
        # é»˜è®¤è¯·æ±‚å‚æ•°
        if request is None:
            request = OrderSpeedAnalysisRequest()
        
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)
        
        if df.empty:
            return OrderSpeedAnalysisResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                speed_analysis=RoadSpeedAnalysisResult(
                    time_range={"start": start_time, "end": end_time},
                    speed_data=[],
                    heatmap_data=[],
                    congestion_summary={},
                    road_speed_trends=[]
                ),
                visualization_data={}
            )
        
        # åˆ›å»ºè·¯æ®µåˆ†æå¼•æ“
        from .road_analysis_engine import RoadAnalysisEngine
        road_engine = RoadAnalysisEngine()
        
        # æ ‡å‡†åŒ–æ•°æ®åˆ—å
        if 'COMMADDR' in df.columns:
            df['vehicle_id'] = df['COMMADDR']
        if 'UTC' in df.columns:
            df['timestamp'] = df['UTC']
        if 'LAT' in df.columns and 'LON' in df.columns:
            df['latitude'] = df['LAT'] / 1e5
            df['longitude'] = df['LON'] / 1e5
        
        # è¿›è¡Œè®¢å•é€Ÿåº¦åˆ†æ
        speed_analysis = road_engine.analyze_order_based_road_speed(
            df,
            include_short_medium_only=request.include_short_medium_only,
            spatial_resolution=request.spatial_resolution,
            min_orders_per_location=request.min_orders_per_location,
            congestion_threshold=request.congestion_threshold
        )
        
        # ç”Ÿæˆå¯è§†åŒ–æ•°æ®
        visualization_data = {
            "speed_heatmap": {
                "type": "heatmap",
                "title": "é“è·¯é€Ÿåº¦çƒ­åŠ›å›¾",
                "data": [
                    {
                        "lat": heatmap.lat,
                        "lng": heatmap.lng,
                        "speed": heatmap.speed,
                        "intensity": heatmap.intensity,
                        "order_count": heatmap.order_count,
                        "congestion_level": heatmap.congestion_level
                    }
                    for heatmap in speed_analysis.heatmap_data
                ]
            },
            "congestion_distribution": {
                "type": "pie",
                "title": "æ‹¥å µç­‰çº§åˆ†å¸ƒ",
                "data": [
                    {
                        "name": f"{level} (æ‹¥å µç­‰çº§)",
                        "value": info["percentage"],
                        "count": info["count"]
                    }
                    for level, info in speed_analysis.congestion_summary.get("congestion_distribution", {}).items()
                ]
            },
            "speed_trends": {
                "type": "line",
                "title": "24å°æ—¶é“è·¯é€Ÿåº¦è¶‹åŠ¿",
                "data": [
                    {
                        "hour": trend["hour"],
                        "avg_speed": trend["avg_speed"],
                        "order_count": trend["order_count"],
                        "is_peak_hour": trend["is_peak_hour"],
                        "speed_category": trend["speed_category"]
                    }
                    for trend in speed_analysis.road_speed_trends
                ]
            },
            "speed_statistics": {
                "overall_avg_speed": speed_analysis.congestion_summary.get("overall_avg_speed", 0),
                "total_analysis_locations": speed_analysis.congestion_summary.get("total_analysis_locations", 0),
                "high_confidence_locations": speed_analysis.congestion_summary.get("high_confidence_locations", 0),
                "total_orders_analyzed": speed_analysis.congestion_summary.get("total_orders_analyzed", 0),
                "speed_range": {
                    "min": speed_analysis.congestion_summary.get("speed_statistics", {}).get("min_speed", 0),
                    "max": speed_analysis.congestion_summary.get("speed_statistics", {}).get("max_speed", 0),
                    "median": speed_analysis.congestion_summary.get("speed_statistics", {}).get("median_speed", 0)
                }
            },
            "congestion_zones": {
                "type": "scatter",
                "title": "æ‹¥å µåŒºåŸŸåˆ†å¸ƒ",
                "data": [
                    {
                        "lat": data.location["lat"],
                        "lng": data.location["lng"],
                        "speed": data.avg_speed,
                        "congestion_level": data.congestion_level,
                        "order_count": data.order_count,
                        "confidence": data.confidence_score
                    }
                    for data in speed_analysis.speed_data
                    if data.congestion_level in ["heavy", "jam"]  # åªæ˜¾ç¤ºæ‹¥å µåŒºåŸŸ
                ]
            }
        }
        
        processing_time = time.time() - start_processing
        
        logger.info(f"è®¢å•é€Ÿåº¦åˆ†æå®Œæˆï¼Œå¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        
        return OrderSpeedAnalysisResponse(
            success=True,
            message=f"è®¢å•é€Ÿåº¦åˆ†æå®Œæˆï¼Œåˆ†æäº† {len(speed_analysis.speed_data)} ä¸ªä½ç½®çš„é€Ÿåº¦æ•°æ®",
            speed_analysis=speed_analysis,
            visualization_data=visualization_data,
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"è®¢å•é€Ÿåº¦åˆ†æå¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        
        return OrderSpeedAnalysisResponse(
            success=False,
            message=f"è®¢å•é€Ÿåº¦åˆ†æå¤±è´¥: {str(e)}",
            speed_analysis=RoadSpeedAnalysisResult(
                time_range={"start": start_time, "end": end_time},
                speed_data=[],
                heatmap_data=[],
                congestion_summary={"error": str(e)},
                road_speed_trends=[]
            ),
            visualization_data={}
        )
    
class DailyTrafficResponse(BaseModel):
    success: bool
    message: str
    data: List[int]  # 24å°æ—¶çš„è½¦è¾†æ•°æ•°ç»„

class WeeklyTrafficResponse(BaseModel):
    success: bool
    message: str
    data: List[Dict[str, Union[str, int]]]  # æŒ‰å¤©çš„æ—¥æœŸå’Œè½¦è¾†æ€»æ•°

class KeyMetric(BaseModel):
    title: str
    value: str
    trend: float

class MetricsResponse(BaseModel):
    success: bool
    message: str
    data: List[KeyMetric]

class AreaStat(BaseModel):
    id: int
    name: str
    totalVehicles: int
    avgSpeed: float
    congestionRate: float
    trafficLevel: str

class AreaStatsResponse(BaseModel):
    success: bool
    message: str
    data: List[AreaStat]

class PeriodStat(BaseModel):
    name: str
    timeRange: str
    avgVehicles: int
    avgSpeed: float
    status: str
    statusClass: str

class PeriodStatsResponse(BaseModel):
    success: bool
    message: str
    data: List[PeriodStat]

# --- è¾…åŠ©å‡½æ•° ---
def load_spatial_grid():
    grid_file = Path(os.path.join(os.path.dirname(__file__), 'data', 'indexes', 'spatial_grid_0.001.json'))
    if grid_file.exists():
        with open(grid_file, 'r') as f:
            return json.load(f)
    # é»˜è®¤ç½‘æ ¼ï¼ˆç¤ºä¾‹æ•°æ®ï¼Œéœ€æ›¿æ¢ä¸ºå®é™…ç½‘æ ¼ï¼‰
    return {
        "1": {"name": "å¸‚ä¸­å¿ƒæ ¸å¿ƒåŒº", "bounds": [36.65, 36.67, 117.00, 117.02]},
        "2": {"name": "å•†ä¸šè´­ç‰©åŒº", "bounds": [36.67, 36.69, 117.02, 117.04]},
        "3": {"name": "ä½å®…å±…æ°‘åŒº", "bounds": [36.69, 36.71, 117.04, 117.06]},
        "4": {"name": "å·¥ä¸šå¼€å‘åŒº", "bounds": [36.71, 36.73, 117.06, 117.08]},
        "5": {"name": "æ–‡æ•™ç§‘ç ”åŒº", "bounds": [36.73, 36.75, 117.08, 117.10]},
        "6": {"name": "äº¤é€šæ¢çº½åŒº", "bounds": [36.75, 36.77, 117.10, 117.12]},
        "7": {"name": "ä¼‘é—²å¨±ä¹åŒº", "bounds": [36.77, 36.79, 117.12, 117.14]}
    }

def calculate_congestion_rate(vehicle_count: int, area_size: float = 1.2321) -> tuple[float, str]:
    """è®¡ç®—æ‹¥å µç‡å’Œæµé‡ç­‰çº§ï¼Œé¢ç§¯å•ä½ä¸ºå¹³æ–¹å…¬é‡Œ"""
    density = vehicle_count / area_size
    congestion_rate = min(density * 100, 100)  # ç®€å•å¯†åº¦å…¬å¼ï¼Œéœ€è°ƒæ•´
    if congestion_rate >= 80:
        return congestion_rate, "ä¸¥é‡æ‹¥å µ"
    elif congestion_rate >= 60:
        return congestion_rate, "é‡åº¦æ‹¥å µ"
    elif congestion_rate >= 40:
        return congestion_rate, "ä¸­åº¦æ‹¥å µ"
    elif congestion_rate >= 20:
        return congestion_rate, "è½»åº¦æ‹¥å µ"
    else:
        return congestion_rate, "åŸºæœ¬ç•…é€š"

# --- æ–°å¢ API ç«¯ç‚¹ ---
@router.get("/daily", response_model=DailyTrafficResponse)
async def get_daily_traffic(
    date: str = Query(None, description="æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œé»˜è®¤ä¸ºä»Šå¤©")
):
    """è·å–æ¯æ—¥æµé‡è¶‹åŠ¿ï¼ˆ24å°æ—¶è½¦è¾†æ•°ï¼‰- ä½¿ç”¨å®Œæ•´æ•°æ®ç»Ÿè®¡"""
    try:
        if not date:
            date = datetime.now().strftime("%Y-%m-%d")
        
        logger.info(f"å¼€å§‹æŸ¥è¯¢æ¯æ—¥å®Œæ•´æµé‡è¶‹åŠ¿: {date}")
        
        start_time = convert_time_to_timestamp(f"{date} 00:00:00")
        end_time = start_time + 24 * 3600 - 1
        
        logger.info(f"æ—¶é—´èŒƒå›´: {start_time} - {end_time}")
        
        # ä½¿ç”¨ä¸“ç”¨çš„æµé‡ç»Ÿè®¡åŠ è½½å™¨è·å–å®Œæ•´çš„å°æ—¶ç»Ÿè®¡
        hourly_counts = traffic_stats_loader.get_hourly_traffic_counts(start_time, end_time)
        
        if not hourly_counts or all(count == 0 for count in hourly_counts):
            logger.warning(f"æœªæ‰¾åˆ°æ—¥æœŸ {date} çš„æµé‡æ•°æ®")
            return DailyTrafficResponse(
                success=False,
                message="æœªæ‰¾åˆ°æŒ‡å®šæ—¥æœŸçš„æµé‡æ•°æ®",
                data=[0] * 24
            )
        
        total_vehicles = sum(hourly_counts)
        logger.info(f"å°æ—¶ç»Ÿè®¡å®Œæˆ: æ€»è½¦è¾†æ•° {total_vehicles}")
        
        return DailyTrafficResponse(
            success=True,
            message=f"æˆåŠŸè·å– {date} çš„æ¯æ—¥å®Œæ•´æµé‡æ•°æ®ï¼Œæ€»è½¦è¾†æ•°: {total_vehicles}",
            data=convert_numpy_types(hourly_counts)
        )
    except Exception as e:
        logger.error(f"æ¯æ—¥æµé‡æŸ¥è¯¢å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return DailyTrafficResponse(
            success=False,
            message=f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
            data=[0] * 24
        )

@router.get("/weekly", response_model=WeeklyTrafficResponse)
async def get_weekly_traffic(
    start_date: str = Query(None, description="å‘¨èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œé»˜è®¤ä¸ºæ•°æ®é›†çš„èµ·å§‹å‘¨")
):
    """è·å–æ¯å‘¨æµé‡è¶‹åŠ¿ï¼ˆ7å¤©æ¯æ—¥è½¦è¾†æ€»æ•°ï¼‰- ä½¿ç”¨å®Œæ•´æ•°æ®ç»Ÿè®¡"""
    try:
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ•°æ®é›†çš„å®é™…æ—¥æœŸèŒƒå›´
        if not start_date:
            # ä½¿ç”¨æ•°æ®é›†çš„èµ·å§‹æ—¥æœŸï¼š2013-09-12 (å‘¨å››)
            start_date = "2013-09-12"
        
        # éªŒè¯æ—¥æœŸæ˜¯å¦åœ¨æ•°æ®é›†èŒƒå›´å†…
        dataset_start = "2013-09-12"  # æ•°æ®é›†èµ·å§‹æ—¥æœŸ
        dataset_end = "2013-09-18"    # æ•°æ®é›†ç»“æŸæ—¥æœŸ
        
        # æ£€æŸ¥è¯·æ±‚çš„æ—¥æœŸæ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
        requested_date = datetime.strptime(start_date, "%Y-%m-%d")
        dataset_start_date = datetime.strptime(dataset_start, "%Y-%m-%d")
        dataset_end_date = datetime.strptime(dataset_end, "%Y-%m-%d")
        
        if requested_date < dataset_start_date or requested_date > dataset_end_date:
            logger.warning(f"è¯·æ±‚çš„æ—¥æœŸ {start_date} è¶…å‡ºæ•°æ®é›†èŒƒå›´ {dataset_start} åˆ° {dataset_end}")
            return WeeklyTrafficResponse(
                success=False,
                message=f"è¯·æ±‚çš„æ—¥æœŸè¶…å‡ºæ•°æ®é›†èŒƒå›´ï¼Œå¯ç”¨æ—¥æœŸï¼š{dataset_start} åˆ° {dataset_end}",
                data=[]
            )
        
        start_time = convert_time_to_timestamp(f"{start_date} 00:00:00")
        end_time = start_time + 7 * 24 * 3600 - 1
        
        logger.info(f"æŸ¥è¯¢æ¯å‘¨å®Œæ•´æµé‡è¶‹åŠ¿: {start_date} ({start_time} - {end_time})")
        
        # ä½¿ç”¨ä¸“ç”¨çš„æµé‡ç»Ÿè®¡åŠ è½½å™¨è·å–å®Œæ•´çš„æ¯æ—¥ç»Ÿè®¡
        daily_counts = traffic_stats_loader.get_daily_traffic_counts(start_time, end_time, days=7)
        
        if not daily_counts or all(item['totalVehicles'] == 0 for item in daily_counts):
            logger.warning(f"æœªæ‰¾åˆ°å‘¨ {start_date} çš„æµé‡æ•°æ®")
            # æ„é€ é»˜è®¤çš„7å¤©æ•°æ®
            result = []
            current_date = datetime.fromtimestamp(start_time)
            for i in range(7):
                date_str = (current_date + timedelta(days=i)).strftime("%Y-%m-%d")
                result.append({"date": date_str, "totalVehicles": 0})
            return WeeklyTrafficResponse(
                success=False,
                message="æœªæ‰¾åˆ°æŒ‡å®šå‘¨çš„æµé‡æ•°æ®",
                data=result
            )
        
        total_week_vehicles = sum(item['totalVehicles'] for item in daily_counts)
        logger.info(f"æ¯æ—¥ç»Ÿè®¡å®Œæˆ: 7å¤©æ€»è½¦è¾†æ•° {total_week_vehicles}")
        
        return WeeklyTrafficResponse(
            success=True,
            message=f"æˆåŠŸè·å– {start_date} å¼€å§‹çš„å‘¨å®Œæ•´æµé‡æ•°æ®ï¼Œæ€»è½¦è¾†æ•°: {total_week_vehicles}",
            data=convert_numpy_types(daily_counts)
        )
    except Exception as e:
        logger.error(f"æ¯å‘¨æµé‡æŸ¥è¯¢å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        # æ„é€ é»˜è®¤çš„7å¤©æ•°æ®ä½œä¸ºé”™è¯¯å›é€€
        result = []
        try:
            if 'start_time' in locals():
                current_date = datetime.fromtimestamp(start_time)
                for i in range(7):
                    date_str = (current_date + timedelta(days=i)).strftime("%Y-%m-%d")
                    result.append({"date": date_str, "totalVehicles": 0})
        except:
            # å¦‚æœè¿æ—¶é—´æˆ³éƒ½æœ‰é—®é¢˜ï¼Œä½¿ç”¨æ•°æ®é›†çš„èµ·å§‹æ—¥æœŸæ„é€ 
            start_date_obj = datetime.strptime("2013-09-12", "%Y-%m-%d")
            for i in range(7):
                date_str = (start_date_obj + timedelta(days=i)).strftime("%Y-%m-%d")
                result.append({"date": date_str, "totalVehicles": 0})
        
        return WeeklyTrafficResponse(
            success=False,
            message=f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
            data=result
        )

@router.get("/metrics", response_model=MetricsResponse)
async def get_metrics(
    date: str = Query(None, description="æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œé»˜è®¤ä¸ºæ•°æ®é›†èµ·å§‹æ—¥æœŸ"),
    period: str = Query("today", description="æ—¶é—´èŒƒå›´ï¼štoday, weekï¼ˆå…¼å®¹æ€§å‚æ•°ï¼‰")
):
    """è·å–å…³é”®æŒ‡æ ‡ï¼ˆæ€»æµé‡ã€å¹³å‡é€Ÿåº¦ã€é«˜å³°æ—¶é•¿ã€æ´»è·ƒç”¨æˆ·ï¼‰- ä½¿ç”¨å®Œæ•´æ•°æ®ç»Ÿè®¡"""
    try:
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ•°æ®é›†çš„å®é™…æ—¥æœŸèŒƒå›´
        if date:
            # éªŒè¯æ—¥æœŸæ˜¯å¦åœ¨æ•°æ®é›†èŒƒå›´å†…
            dataset_start = "2013-09-12"
            dataset_end = "2013-09-18"

            requested_date = datetime.strptime(date, "%Y-%m-%d")
            dataset_start_date = datetime.strptime(dataset_start, "%Y-%m-%d")
            dataset_end_date = datetime.strptime(dataset_end, "%Y-%m-%d")

            if requested_date < dataset_start_date or requested_date > dataset_end_date:
                return MetricsResponse(
                    success=False,
                    message=f"è¯·æ±‚çš„æ—¥æœŸè¶…å‡ºæ•°æ®é›†èŒƒå›´ï¼Œå¯ç”¨æ—¥æœŸï¼š{dataset_start} åˆ° {dataset_end}",
                    data=[
                        KeyMetric(title="æ€»æµé‡", value="0", trend=0),
                        KeyMetric(title="å¹³å‡é€Ÿåº¦", value="0.0km/h", trend=0),
                        KeyMetric(title="é«˜å³°æ—¶é•¿", value="0.0h", trend=0),
                        KeyMetric(title="æ´»è·ƒç”¨æˆ·", value="0", trend=0)
                    ]
                )

            # ä½¿ç”¨ä¼ å…¥çš„å…·ä½“æ—¥æœŸ
            start_time = datetime.strptime(date, "%Y-%m-%d")
            end_time = start_time + timedelta(days=1) - timedelta(seconds=1)
            hours = 24
            period_desc = f"æ—¥æœŸ {date}"
        else:
            # ä½¿ç”¨æ•°æ®é›†çš„é»˜è®¤æ—¥æœŸè€Œä¸æ˜¯å½“å‰æ—¶é—´
            if period == "today":
                start_time = datetime.strptime("2013-09-12", "%Y-%m-%d")  # æ•°æ®é›†èµ·å§‹æ—¥æœŸ
                end_time = start_time + timedelta(days=1) - timedelta(seconds=1)
                hours = 24
                period_desc = "æ•°æ®é›†èµ·å§‹æ—¥æœŸ(2013-09-12)"
            else:  # week
                start_time = datetime.strptime("2013-09-12", "%Y-%m-%d")  # æ•°æ®é›†èµ·å§‹æ—¥æœŸ
                end_time = start_time + timedelta(days=7) - timedelta(seconds=1)
                hours = 7 * 24
                period_desc = "æ•°æ®é›†çš„å®Œæ•´å‘¨(2013-09-12 åˆ° 2013-09-18)"

        start_timestamp = int(start_time.timestamp())
        end_timestamp = int(end_time.timestamp())

        logger.info(f"æŸ¥è¯¢å…³é”®æŒ‡æ ‡: {period_desc} ({start_timestamp} - {end_timestamp})")

        # ğŸš€ ä½¿ç”¨æ–°çš„æµé‡ç»Ÿè®¡åŠ è½½å™¨è®¡ç®—æŒ‡æ ‡
        metrics_data = traffic_stats_loader.calculate_key_metrics(
            start_timestamp, 
            end_timestamp, 
            hours
        )

        if not metrics_data["data_available"]:
            logger.warning(f"æœªæ‰¾åˆ° {period_desc} çš„æ•°æ®")
            return MetricsResponse(
                success=False,
                message=f"æœªæ‰¾åˆ°{period_desc}çš„æµé‡æ•°æ®",
                data=[
                    KeyMetric(title="æ€»æµé‡", value="0", trend=0),
                    KeyMetric(title="å¹³å‡é€Ÿåº¦", value="0.0km/h", trend=0),
                    KeyMetric(title="é«˜å³°æ—¶é•¿", value="0.0h", trend=0),
                    KeyMetric(title="æ´»è·ƒç”¨æˆ·", value="0", trend=0)
                ]
            )

        # ğŸ“Š æ„å»ºæŒ‡æ ‡ç»“æœ
        metrics = [
            KeyMetric(
                title="æ€»æµé‡", 
                value=f"{metrics_data['total_vehicles']:,}", 
                trend=0
            ),
            KeyMetric(
                title="å¹³å‡é€Ÿåº¦", 
                value=f"{metrics_data['avg_speed']:.1f}km/h", 
                trend=0
            ),
            KeyMetric(
                title="é«˜å³°æ—¶é•¿", 
                value=f"{metrics_data['peak_hours']:.1f}h", 
                trend=0
            ),
            KeyMetric(
                title="æ´»è·ƒç”¨æˆ·", 
                value=f"{metrics_data['unique_vehicles']:,}", 
                trend=0
            )
        ]

        # ğŸ“ˆ æ·»åŠ è¯¦ç»†çš„æˆåŠŸæ¶ˆæ¯
        success_message = (
            f"æˆåŠŸè·å–{period_desc}çš„å…³é”®æŒ‡æ ‡ - "
            f"æ€»æµé‡: {metrics_data['total_vehicles']:,}, "
            f"æ´»è·ƒè½¦è¾†: {metrics_data['unique_vehicles']:,}, "
            f"å¹³å‡é€Ÿåº¦: {metrics_data['avg_speed']:.1f}km/h"
        )

        return MetricsResponse(
            success=True,
            message=success_message,
            data=convert_numpy_types(metrics)
        )

    except ValueError as ve:
        # å¤„ç†æ—¥æœŸæ ¼å¼é”™è¯¯
        logger.error(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {str(ve)}")
        return MetricsResponse(
            success=False,
            message=f"æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨YYYY-MM-DDæ ¼å¼: {str(ve)}",
            data=[
                KeyMetric(title="æ€»æµé‡", value="0", trend=0),
                KeyMetric(title="å¹³å‡é€Ÿåº¦", value="0.0km/h", trend=0),
                KeyMetric(title="é«˜å³°æ—¶é•¿", value="0.0h", trend=0),
                KeyMetric(title="æ´»è·ƒç”¨æˆ·", value="0", trend=0)
            ]
        )
    except Exception as e:
        logger.error(f"å…³é”®æŒ‡æ ‡æŸ¥è¯¢å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return MetricsResponse(
            success=False,
            message=f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
            data=[
                KeyMetric(title="æ€»æµé‡", value="0", trend=0),
                KeyMetric(title="å¹³å‡é€Ÿåº¦", value="0.0km/h", trend=0),
                KeyMetric(title="é«˜å³°æ—¶é•¿", value="0.0h", trend=0),
                KeyMetric(title="æ´»è·ƒç”¨æˆ·", value="0", trend=0)
            ]
        )
    
@router.get("/areas", response_model=AreaStatsResponse)
async def get_area_stats(
    date: str = Query(None, description="æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD"),
    period: str = Query("today", description="æ—¶é—´èŒƒå›´ï¼štoday, weekï¼ˆå…¼å®¹æ€§å‚æ•°ï¼‰")
):
    """è·å–åŒºåŸŸç»Ÿè®¡ï¼ˆè½¦è¾†æ•°ã€å¹³å‡é€Ÿåº¦ã€æ‹¥å µç‡ã€æµé‡ç­‰çº§ï¼‰"""
    try:
        spatial_grid = load_spatial_grid()
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¼ å…¥çš„æ—¥æœŸ
        if date:
            start_time = datetime.strptime(date, "%Y-%m-%d")
            end_time = start_time + timedelta(days=1) - timedelta(seconds=1)
            period_desc = f"æ—¥æœŸ {date}"
        else:
            start_time = datetime.now()
            if period == "today":
                start_time = start_time.replace(hour=0, minute=0, second=0, microsecond=0)
                end_time = start_time + timedelta(days=1) - timedelta(seconds=1)
                period_desc = "ä»Šæ—¥"
            else:  # week
                start_time = start_time - timedelta(days=start_time.weekday())
                end_time = start_time + timedelta(days=7) - timedelta(seconds=1)
                period_desc = "æœ¬å‘¨"
        
        start_timestamp = int(start_time.timestamp())
        end_timestamp = int(end_time.timestamp())
        
        logger.info(f"æŸ¥è¯¢åŒºåŸŸç»Ÿè®¡: {period_desc} ({start_timestamp} - {end_timestamp})")
        
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return AreaStatsResponse(
                success=False,
                message="æœªæ‰¾åˆ°æŒ‡å®šæ—¶é—´èŒƒå›´çš„æµé‡æ•°æ®",
                data=[]
            )
        
        # æŒ‰åŒºåŸŸèšåˆ
        area_data = {grid_id: {"total_vehicles": 0, "total_speed": 0, "count": 0} for grid_id in spatial_grid}
        
        for _, row in df.iterrows():
            # æ­£ç¡®å¤„ç†åæ ‡
            lat = row['LAT'] / 1e5 if 'LAT' in row else 0
            lon = row['LON'] / 1e5 if 'LON' in row else 0
            
            for grid_id, grid in spatial_grid.items():
                min_lat, max_lat, min_lon, max_lon = grid['bounds']
                if min_lat <= lat <= max_lat and min_lon <= lon <= max_lon:
                    area_data[grid_id]['total_vehicles'] += 1
                    # æ­£ç¡®å¤„ç†é€Ÿåº¦
                    speed = 0
                    if 'SPEED' in row and pd.notna(row['SPEED']):
                        speed = row['SPEED']
                    elif 'speed' in row and pd.notna(row['speed']):
                        speed = row['speed']
                    area_data[grid_id]['total_speed'] += speed
                    area_data[grid_id]['count'] += 1
                    break
        
        # æ„é€ ç»“æœ
        result = []
        for grid_id, data in area_data.items():
            if data['count'] > 0:
                avg_speed = data['total_speed'] / data['count']
                congestion_rate, traffic_level = calculate_congestion_rate(data['total_vehicles'])
                result.append(AreaStat(
                    id=int(grid_id),
                    name=spatial_grid[grid_id]['name'],
                    totalVehicles=data['total_vehicles'],
                    avgSpeed=round(avg_speed, 1),
                    congestionRate=round(congestion_rate, 1),
                    trafficLevel=traffic_level
                ))
        
        return AreaStatsResponse(
            success=True,
            message=f"æˆåŠŸè·å– {period} çš„åŒºåŸŸç»Ÿè®¡æ•°æ®ï¼Œå…± {len(result)} ä¸ªåŒºåŸŸ",
            data=convert_numpy_types(result)
        )
    except Exception as e:
        logger.error(f"åŒºåŸŸç»Ÿè®¡æŸ¥è¯¢å¤±è´¥: {str(e)}")
        return AreaStatsResponse(
            success=False,
            message=f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
            data=[]
        )

@router.get("/periods", response_model=PeriodStatsResponse)
async def get_period_stats(
    date: str = Query(None, description="æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD"),
    period: str = Query("today", description="æ—¶é—´èŒƒå›´ï¼štoday, weekï¼ˆå…¼å®¹æ€§å‚æ•°ï¼‰")
):
    """è·å–æ—¶é—´æ®µç»Ÿè®¡ï¼ˆæ—©é«˜å³°ã€æ™šé«˜å³°ã€å¹³å³°ï¼‰"""
    try:
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¼ å…¥çš„æ—¥æœŸ
        if date:
            start_time = datetime.strptime(date, "%Y-%m-%d")
            days = 1
            period_desc = f"æ—¥æœŸ {date}"
        else:
            start_time = datetime.now()
            if period == "today":
                start_time = start_time.replace(hour=0, minute=0, second=0, microsecond=0)
                days = 1
                period_desc = "ä»Šæ—¥"
            else:  # week
                start_time = start_time - timedelta(days=start_time.weekday())
                days = 7
                period_desc = "æœ¬å‘¨"
        
        periods = [
            {"name": "æ—©é«˜å³°", "start_hour": 7, "end_hour": 9, "timeRange": "07:00-09:00"},
            {"name": "æ™šé«˜å³°", "start_hour": 17, "end_hour": 19, "timeRange": "17:00-19:00"},
            {"name": "å¹³å³°æ—¶æ®µ", "start_hour": 10, "end_hour": 16, "timeRange": "10:00-16:00"}
        ]
        
        start_timestamp = int(start_time.timestamp())
        end_timestamp = int((start_time + timedelta(days=days)).timestamp() - 1)
        
        logger.info(f"æŸ¥è¯¢æ—¶é—´æ®µç»Ÿè®¡: {period_desc} ({start_timestamp} - {end_timestamp})")
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_timestamp, end_timestamp)
        
        if df.empty:
            return PeriodStatsResponse(
                success=False,
                message="æœªæ‰¾åˆ°æŒ‡å®šæ—¶é—´èŒƒå›´çš„æµé‡æ•°æ®",
                data=[PeriodStat(
                    name=p['name'],
                    timeRange=p['timeRange'],
                    avgVehicles=0,
                    avgSpeed=0.0,
                    status="ç•…é€š",
                    statusClass="bg-green-500/20 text-green-400"
                ) for p in periods]
            )
        
        # æŒ‰æ—¶é—´æ®µèšåˆ
        result = []
        time_col = 'UTC' if 'UTC' in df.columns else 'timestamp'
        df['hour'] = pd.to_datetime(df[time_col], unit='s').dt.hour
        df['date'] = pd.to_datetime(df[time_col], unit='s').dt.date
        
        for p in periods:
            total_vehicles = 0
            total_speed = 0
            count = 0
            
            for day in range(days):
                day_start = start_time + datetime.timedelta(days=day)
                day_df = df[df['date'] == day_start.date()]
                period_df = day_df[day_df['hour'].between(p['start_hour'], p['end_hour'] - 1)]
                
                total_vehicles += len(period_df)
                # æ­£ç¡®è®¡ç®—é€Ÿåº¦
                speed_sum = 0
                if 'SPEED' in period_df.columns:
                    speed_sum = period_df['SPEED'].fillna(0).sum()
                elif 'speed' in period_df.columns:
                    speed_sum = period_df['speed'].fillna(0).sum()
                total_speed += speed_sum
                count += len(period_df) if not period_df.empty else 0
            
            avg_vehicles = total_vehicles / (days * (p['end_hour'] - p['start_hour'])) if count > 0 else 0
            avg_speed = total_speed / total_vehicles if total_vehicles > 0 else 0
            status = "ç•…é€š" if avg_speed > 50 else "æ‹¥å µ" if avg_speed < 35 else "ä¸­åº¦æ‹¥å µ"
            status_class = (
                "bg-green-500/20 text-green-400" if status == "ç•…é€š" else
                "bg-red-500/20 text-red-400" if status == "æ‹¥å µ" else
                "bg-orange-500/20 text-orange-400"
            )
            
            result.append(PeriodStat(
                name=p['name'],
                timeRange=p['timeRange'],
                avgVehicles=round(avg_vehicles),
                avgSpeed=round(avg_speed, 1),
                status=status,
                statusClass=status_class
            ))
        
        return PeriodStatsResponse(
            success=True,
            message=f"æˆåŠŸè·å– {period} çš„æ—¶é—´æ®µç»Ÿè®¡æ•°æ®",
            data=convert_numpy_types(result)
        )
    except Exception as e:
        logger.error(f"æ—¶é—´æ®µç»Ÿè®¡æŸ¥è¯¢å¤±è´¥: {str(e)}")
        return PeriodStatsResponse(
            success=False,
            message=f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
            data=[PeriodStat(
                name=p['name'],
                timeRange=p['timeRange'],
                avgVehicles=0,
                avgSpeed=0.0,
                status="ç•…é€š",
                statusClass="bg-green-500/20 text-green-400"
            ) for p in periods]
        )