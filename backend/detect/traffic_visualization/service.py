from typing import Tuple
from fastapi import APIRouter, Query, Depends, HTTPException, Request, Body
import pandas as pd
import os
from datetime import datetime, timedelta
from collections import defaultdict
from typing import List, Dict, Optional, Any, Union
from .data_processor import TrafficDataProcessor
from .heatmap import HeatmapGenerator
from .track import TrackAnalyzer
from .traffic_analysis_cache import get_traffic_cache
from .log import logger  # å¯¼å…¥è‡ªå®šä¹‰æ—¥å¿—è®°å½•å™¨
# from .data_cleaner import TrafficDataCleaner, DataQualityAnalyzer  # å·²åˆ é™¤æ•°æ®æ¸…æ´—åŠŸèƒ½
from .traffic_statistics_loader import TrafficStatisticsLoader
from .models import (
    TimeRangeRequest, TrafficQueryRequest, HeatmapRequest,
    TrackQueryRequest, StatisticsRequest, TrafficResponse,
    TrafficDataResponse, HeatmapResponse, TracksResponse,
    StatisticsResponse, TrafficOverview, TimeDistribution,
    DynamicHeatmapResponse, ClusteringRequest, ClusteringResponse,
    ODAnalysisRequest, ODFlowResponse, SpatioTemporalResponse,
    SpatioTemporalAnalysis, RoadAnalysisRequest, RoadAnalysisResponse,
    RoadSegmentResponse, RoadTrafficResponse, RoadVisualizationResponse,
    RoadSegment, RoadTrafficData, RoadSegmentStatistics,
    RoadNetworkAnalysis, SpeedDistribution, TrafficFlowPattern,
    SmartPassengerRequest, SmartPassengerResponse, WeatherImpactRequest,
    WeatherImpactResponse, TaxiDemandRequest, TaxiDemandResponse,
    PassengerVisualizationResponse, WeatherData, PassengerFlowData,
    TaxiDemandData, WeatherImpactAnalysis, TaxiSupplyDemand,
    TripAnalysisRequest, TripAnalysisResponse, TripAnalysisStatistics,
    OrderSpeedAnalysisRequest, OrderSpeedAnalysisResponse,
    TripDistanceClassification, OrderSpeedAnalysis
)
import numpy as np
import logging
import traceback
import time
import json
from pydantic import BaseModel
from .traffic_analysis_cache import get_traffic_cache
from functools import lru_cache

CACHE_DIR = os.path.join(os.path.dirname(__file__), 'data', 'cache')

router = APIRouter()
print("[DEBUG] traffic_visualization.service.py loaded")
print("[DEBUG] router id:", id(router))

# è®¾ç½®æ ‡å‡†æ—¥å¿—ï¼ˆç”¨äºè°ƒè¯•ï¼‰
logging.basicConfig(level=logging.INFO)
standard_logger = logging.getLogger(__name__)

# åˆ›å»ºæ•°æ®å¤„ç†å™¨å®ä¾‹
data_processor = TrafficDataProcessor()
heatmap_generator = HeatmapGenerator()
track_analyzer = TrackAnalyzer()
traffic_stats_loader = TrafficStatisticsLoader()

# data_cleaner = TrafficDataCleaner()  # å·²åˆ é™¤æ•°æ®æ¸…æ´—åŠŸèƒ½
# quality_analyzer = DataQualityAnalyzer()  # å·²åˆ é™¤æ•°æ®æ¸…æ´—åŠŸèƒ½

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ä¸€ä¸ªæ–°çš„è¾…åŠ©å‡½æ•°æ¥å¿«é€Ÿç”Ÿæˆçƒ­åŠ›å›¾
def _get_fast_heatmap_data(start_time: float, end_time: float) -> List[Dict]:
    """ä½¿ç”¨é¢„è®¡ç®—æ•°æ®å¿«é€Ÿç”Ÿæˆçƒ­åŠ›å›¾"""
    try:
        # ç›´æ¥ä»é¢„è®¡ç®—çš„çƒ­åŠ›å›¾æ–‡ä»¶ä¸­åŠ è½½æ•°æ®
        data_dir = os.path.join(os.path.dirname(__file__), 'data')
        index_dir = os.path.join(data_dir, 'indexes')

        if not os.path.exists(index_dir):
            return []

        # è®¡ç®—éœ€è¦çš„å¤©æ•°èŒƒå›´
        start_day = (int(start_time) // (24 * 3600)) * (24 * 3600)
        end_day = (int(end_time) // (24 * 3600)) * (24 * 3600)

        combined_heatmap = defaultdict(int)
        current_day = start_day

        while current_day <= end_day:
            filename = f"heatmap_day_{int(current_day)}.json"
            filepath = os.path.join(index_dir, filename)

            if os.path.exists(filepath):
                try:
                    with open(filepath, 'r') as f:
                        day_heatmap = json.load(f)

                    for grid_key, count in day_heatmap.items():
                        combined_heatmap[grid_key] += count
                except Exception as e:
                    logger.error("çƒ­åŠ›å›¾ç”Ÿæˆ", f"è¯»å–çƒ­åŠ›å›¾æ–‡ä»¶ {filename} å¤±è´¥: {e}", source="æ–‡ä»¶è¯»å–", user="ç³»ç»Ÿ")

            current_day += 24 * 3600  # ä¸‹ä¸€å¤©

        # è½¬æ¢ä¸ºçƒ­åŠ›å›¾ç‚¹æ ¼å¼
        heatmap_points = []
        for grid_key, count in combined_heatmap.items():
            try:
                lat, lng = map(float, grid_key.split(','))
                heatmap_points.append({
                    'lat': lat,
                    'lng': lng,
                    'count': count
                })
            except:
                continue

        # æŒ‰å¯†åº¦æ’åºï¼Œå–å‰10000ä¸ªç‚¹é¿å…å‰ç«¯æ€§èƒ½é—®é¢˜
        heatmap_points.sort(key=lambda x: x['count'], reverse=True)
        return heatmap_points[:10000]

    except Exception as e:
        logger.error("çƒ­åŠ›å›¾ç”Ÿæˆ", f"å¿«é€Ÿçƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}", source="æ–‡ä»¶è¯»å–", user="ç³»ç»Ÿ")
        return []


@router.get("/test")
async def test_endpoint():
    """æµ‹è¯•ç«¯ç‚¹ï¼Œç¡®ä¿è·¯ç”±æ­£å¸¸å·¥ä½œ"""
    logger.info("ç³»ç»Ÿç®¡ç†", "äº¤é€šå¯è§†åŒ–è·¯ç”±æµ‹è¯•", source="APIè°ƒç”¨", user="ç³»ç»Ÿ")
    return {"message": "Traffic router is working!", "status": "ok"}


@router.get("/summary")
async def get_traffic_summary():
    """è·å–äº¤é€šæ•°æ®æ¦‚è¦ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # è·å–åŸºæœ¬çš„æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        data_dir = os.path.join(os.path.dirname(__file__), 'data')

        # è®¡ç®—æ–‡ä»¶æ•°é‡å’Œæ€»å¤§å°
        file_count = 0
        total_size = 0

        for root, dirs, files in os.walk(data_dir):
            for file in files:
                if file.endswith('.csv') or file.endswith('.parquet'):
                    file_path = os.path.join(root, file)
                    file_count += 1
                    total_size += os.path.getsize(file_path)

        # è·å–æ•°æ®æ—¶é—´èŒƒå›´
        time_range = {
            "start": 1378944000,  # 2013-09-12 00:00:00 UTC
            "end": 1379548799  # 2013-09-18 23:59:59 UTC
        }

        # è·å–è½¦è¾†æ•°é‡ä¼°è®¡
        vehicle_count = data_processor.get_vehicle_count_estimate()

        # è·å–æ•°æ®ç‚¹æ•°é‡ä¼°è®¡
        point_count = data_processor.get_point_count_estimate()

        # æ„é€ å“åº”
        return {
            "success": True,
            "message": "è·å–äº¤é€šæ•°æ®æ¦‚è¦æˆåŠŸ",
            "summary": {
                "time_range": time_range,
                "vehicle_count": vehicle_count,
                "point_count": point_count,
                "file_count": file_count,
                "total_size_mb": round(total_size / (1024 * 1024), 2),
                "data_source": "T-Driveè½¨è¿¹æ•°æ®é›†",
                "description": "åŒ—äº¬å¸‚å‡ºç§Ÿè½¦è½¨è¿¹æ•°æ®é›†ï¼ŒåŒ…å«å¤šå¤©çš„GPSè½¨è¿¹æ•°æ®"
            }
        }
    except Exception as e:
        logger.error("ç³»ç»Ÿç®¡ç†", f"è·å–äº¤é€šæ•°æ®æ¦‚è¦å¤±è´¥: {str(e)}", source="APIè°ƒç”¨", user="ç³»ç»Ÿ")
        return {
            "success": False,
            "message": f"è·å–äº¤é€šæ•°æ®æ¦‚è¦å¤±è´¥: {str(e)}",
            "summary": {
                "time_range": {"start": 0, "end": 0},
                "vehicle_count": 0,
                "point_count": 0,
                "file_count": 0,
                "total_size_mb": 0,
                "data_source": "æœªçŸ¥",
                "description": "æ•°æ®åŠ è½½å¤±è´¥"
            }
        }


@router.get("/files/info")
async def get_data_files_info():
    """è·å–æ•°æ®æ–‡ä»¶ä¿¡æ¯"""
    try:
        data_dir = os.path.join(os.path.dirname(__file__), 'data')

        # è·å–æ–‡ä»¶åˆ—è¡¨
        files_info = []

        for root, dirs, files in os.walk(data_dir):
            for file in files:
                if file.endswith('.csv') or file.endswith('.parquet'):
                    file_path = os.path.join(root, file)
                    file_size = os.path.getsize(file_path)
                    file_modified = os.path.getmtime(file_path)

                    # è®¡ç®—æ–‡ä»¶ä¸­çš„è®°å½•æ•°ï¼ˆä¼°è®¡å€¼ï¼‰
                    record_count = 0
                    if file.endswith('.csv'):
                        try:
                            with open(file_path, 'r') as f:
                                for i, _ in enumerate(f):
                                    if i >= 10:  # åªè¯»å–å‰10è¡Œæ¥ä¼°è®¡
                                        break
                                    record_count += 1
                            # æ ¹æ®æ–‡ä»¶å¤§å°ä¼°ç®—æ€»è¡Œæ•°
                            if record_count > 0:
                                avg_line_size = os.path.getsize(file_path) / record_count
                                record_count = int(file_size / avg_line_size)
                        except:
                            record_count = 0

                    # æ„é€ æ–‡ä»¶ä¿¡æ¯
                    rel_path = os.path.relpath(file_path, data_dir)
                    files_info.append({
                        "filename": file,
                        "path": rel_path,
                        "size": file_size,
                        "size_formatted": f"{file_size / (1024 * 1024):.2f} MB" if file_size > 1024 * 1024 else f"{file_size / 1024:.2f} KB",
                        "modified": file_modified,
                        "modified_formatted": datetime.fromtimestamp(file_modified).strftime('%Y-%m-%d %H:%M:%S'),
                        "record_count_estimate": record_count
                    })

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        files_info.sort(key=lambda x: x["modified"], reverse=True)

        return {
            "success": True,
            "message": "è·å–æ•°æ®æ–‡ä»¶ä¿¡æ¯æˆåŠŸ",
            "files": files_info,
            "total_files": len(files_info),
            "total_size": sum(f["size"] for f in files_info),
            "total_size_formatted": f"{sum(f['size'] for f in files_info) / (1024 * 1024):.2f} MB"
        }
    except Exception as e:
        logger.error(f"è·å–æ•°æ®æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"è·å–æ•°æ®æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}",
            "files": [],
            "total_files": 0,
            "total_size": 0,
            "total_size_formatted": "0 MB"
        }


@router.get("/stats", response_model=StatisticsResponse)
async def get_traffic_stats(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        group_by: str = Query("hour", description="æ—¶é—´åˆ†ç»„æ–¹å¼ï¼ˆhour, day, week, monthï¼‰")
):
    """
    è·å–äº¤é€šæ•°æ®ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    try:
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)

        if df.empty:
            return StatisticsResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                overview=TrafficOverview(
                    total_vehicles=0,
                    total_points=0,
                    active_vehicles=0,
                    time_span="0å°æ—¶",
                    coverage_area="æœªçŸ¥",
                    average_speed=0.0
                ),
                time_distribution=[]
            )

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = data_processor.calculate_statistics(df, group_by)

        # æ„é€ å“åº”
        return StatisticsResponse(
            success=True,
            overview=TrafficOverview(
                total_vehicles=stats['total_vehicles'],
                total_points=stats['total_points'],
                active_vehicles=stats['active_vehicles'],
                time_span=stats['time_span'],
                coverage_area=stats['coverage_area'],
                average_speed=stats['average_speed']
            ),
            time_distribution=[
                TimeDistribution(time_key=item['time_key'], count=item['count'])
                for item in stats['time_distribution']
            ]
        )
    except Exception as e:
        return StatisticsResponse(
            success=False,
            message=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}",
            overview=TrafficOverview(
                total_vehicles=0,
                total_points=0,
                active_vehicles=0,
                time_span="0å°æ—¶",
                coverage_area="æœªçŸ¥",
                average_speed=0.0
            ),
            time_distribution=[]
        )


def convert_time_to_timestamp(time_str: str) -> float:
    """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºUTCæ—¶é—´æˆ³"""
    try:
        if 'T' in time_str:
            dt = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
        else:
            dt = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
        return dt.timestamp()
    except Exception as e:
        logger.error(f"æ—¶é—´è½¬æ¢å¤±è´¥: {time_str}, é”™è¯¯: {e}")
        raise HTTPException(status_code=400, detail=f"æ—¶é—´æ ¼å¼é”™è¯¯: {time_str}")


def convert_numpy_types(obj):
    """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif hasattr(obj, '__dict__'):
        # å¤„ç†pydanticæ¨¡å‹ç­‰å¯¹è±¡
        if hasattr(obj, 'dict'):
            return convert_numpy_types(obj.dict())
        else:
            return convert_numpy_types(obj.__dict__)
    return obj


@router.get("/visualization", response_model=TrafficDataResponse)
async def get_traffic_visualization(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        view_type: str = Query("distribution", description="è§†å›¾ç±»å‹ï¼šdistribution, trajectory, heatmap"),
        vehicle_id: Optional[str] = Query(None, description="è½¦è¾†IDï¼Œå¯é€‰"),
        map_style: Optional[str] = Query("blue", description="åœ°å›¾æ ·å¼")
):
    """
    è·å–äº¤é€šæ•°æ®å¯è§†åŒ–æ‰€éœ€æ•°æ®ï¼ˆåˆ†å¸ƒè§†å›¾ã€è½¨è¿¹è§†å›¾ã€çƒ­åŠ›å›¾ï¼‰ã€‚
    """
    try:
        print(f"=== å¼€å§‹å¤„ç†å¯è§†åŒ–è¯·æ±‚ ===")
        print(f"å‚æ•°: start_time={start_time}, end_time={end_time}, view_type={view_type}, vehicle_id={vehicle_id}")

        # è®°å½•APIè°ƒç”¨æ—¥å¿—
        try:
            logger.traffic(
                "æ•°æ®æ€»è§ˆå¯è§†åŒ–",
                f"æ•°æ®æ€»è§ˆå¯è§†åŒ–APIè°ƒç”¨",
                source="APIè°ƒç”¨",
                user="ç³»ç»Ÿ",
                details={
                    "start_time": start_time,
                    "end_time": end_time,
                    "view_type": view_type,
                    "vehicle_id": vehicle_id,
                    "map_style": map_style
                }
            )
        except Exception as log_error:
            print(f"æ—¥å¿—è®°å½•å¤±è´¥: {log_error}")

        # éªŒè¯æ—¶é—´èŒƒå›´
        min_valid_time = 1378944000  # 2013-09-12 00:00:00 UTC
        max_valid_time = 1379548799  # 2013-09-18 23:59:59 UTC

        if end_time < min_valid_time or start_time > max_valid_time:
            print(f"æ—¶é—´èŒƒå›´éªŒè¯å¤±è´¥: {start_time}-{end_time} è¶…å‡ºæœ‰æ•ˆèŒƒå›´ {min_valid_time}-{max_valid_time}")

            # è®°å½•æ—¶é—´èŒƒå›´é”™è¯¯æ—¥å¿—
            try:
                logger.warning(
                    "æ•°æ®æ€»è§ˆå¯è§†åŒ–",
                    f"æ•°æ®æ€»è§ˆæŸ¥è¯¢æ—¶é—´èŒƒå›´è¶…å‡ºæ•°æ®é›†èŒƒå›´",
                    source="APIè°ƒç”¨",
                    user="ç³»ç»Ÿ",
                    details={
                        "start_time": start_time,
                        "end_time": end_time,
                        "min_valid_time": min_valid_time,
                        "max_valid_time": max_valid_time
                    }
                )
            except Exception as log_error:
                print(f"æ—¥å¿—è®°å½•å¤±è´¥: {log_error}")

            return TrafficDataResponse(
                success=False,
                message="æŸ¥è¯¢æ—¶é—´è¶…å‡ºæ•°æ®é›†èŒƒå›´ï¼ˆ2013å¹´9æœˆ12æ—¥è‡³9æœˆ18æ—¥ï¼‰",
                view_type=view_type,
                data=[],
                points=0
            )

        # åŠ è½½æ•°æ®
        try:
            print(f"å¼€å§‹åŠ è½½æ•°æ®...")
            # å¤„ç†vehicle_idå‚æ•°ï¼Œç¡®ä¿ä¸ä¸ºNone
            vehicle_id_param = vehicle_id if vehicle_id else ""
            df = data_processor.load_data(start_time, end_time, vehicle_id_param)
            print(f"æ•°æ®åŠ è½½å®Œæˆ, å…± {len(df)} æ¡è®°å½•")

            # è®°å½•æ•°æ®åŠ è½½æˆåŠŸæ—¥å¿—
            try:
                logger.info(
                    "æ•°æ®æ€»è§ˆå¯è§†åŒ–",
                    f"æ•°æ®æ€»è§ˆæ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•",
                    source="APIè°ƒç”¨",
                    user="ç³»ç»Ÿ",
                    details={
                        "start_time": start_time,
                        "end_time": end_time,
                        "data_count": len(df),
                        "view_type": view_type
                    }
                )
            except Exception as log_error:
                print(f"æ—¥å¿—è®°å½•å¤±è´¥: {log_error}")

        except Exception as load_error:
            print(f"æ•°æ®åŠ è½½é”™è¯¯: {str(load_error)}")
            import traceback
            print(traceback.format_exc())

            # è®°å½•æ•°æ®åŠ è½½å¤±è´¥æ—¥å¿—
            try:
                logger.error(
                    "æ•°æ®æ€»è§ˆå¯è§†åŒ–",
                    f"æ•°æ®æ€»è§ˆæ•°æ®åŠ è½½å¤±è´¥: {str(load_error)}",
                    source="APIè°ƒç”¨",
                    user="ç³»ç»Ÿ",
                    details={
                        "start_time": start_time,
                        "end_time": end_time,
                        "error_message": str(load_error),
                        "error_stack": traceback.format_exc()
                    }
                )
            except Exception as log_error:
                print(f"æ—¥å¿—è®°å½•å¤±è´¥: {log_error}")

            return TrafficDataResponse(
                success=False,
                message=f"æ•°æ®åŠ è½½å¤±è´¥: {str(load_error)}",
                view_type=view_type,
                data=[],
                points=0
            )

        if df.empty:
            print("åŠ è½½çš„æ•°æ®ä¸ºç©º")

            # è®°å½•æ•°æ®ä¸ºç©ºæ—¥å¿—
            try:
                logger.warning(
                    "æ•°æ®æ€»è§ˆå¯è§†åŒ–",
                    f"æ•°æ®æ€»è§ˆæŸ¥è¯¢æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                    source="APIè°ƒç”¨",
                    user="ç³»ç»Ÿ",
                    details={
                        "start_time": start_time,
                        "end_time": end_time,
                        "view_type": view_type
                    }
                )
            except Exception as log_error:
                print(f"æ—¥å¿—è®°å½•å¤±è´¥: {log_error}")

            return TrafficDataResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                view_type=view_type,
                data=[],
                points=0
            )

        # æ ¹æ®è§†å›¾ç±»å‹å¤„ç†æ•°æ®
        try:
            print(f"å¼€å§‹å¤„ç† {view_type} è§†å›¾æ•°æ®...")
            if view_type == "heatmap":
                # ä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—çš„å¿«é€Ÿçƒ­åŠ›å›¾æ•°æ®
                print("å°è¯•ä½¿ç”¨é¢„è®¡ç®—çƒ­åŠ›å›¾æ•°æ®...")
                fast_heatmap_data = _get_fast_heatmap_data(start_time, end_time)

                if fast_heatmap_data and not vehicle_id:  # åªåœ¨æ²¡æœ‰ç‰¹å®šè½¦è¾†è¿‡æ»¤æ—¶ä½¿ç”¨é¢„è®¡ç®—æ•°æ®
                    print(f"ä½¿ç”¨å¿«é€Ÿçƒ­åŠ›å›¾æ•°æ®: {len(fast_heatmap_data)} ä¸ªç‚¹")
                    data = fast_heatmap_data
                else:
                    print("é¢„è®¡ç®—æ•°æ®ä¸å¯ç”¨æˆ–æœ‰è½¦è¾†è¿‡æ»¤ï¼Œä½¿ç”¨å®æ—¶è®¡ç®—...")
                    # ç”Ÿæˆçƒ­åŠ›å›¾æ•°æ®
                    heatmap_points = data_processor.generate_heatmap_data(df)
                    # ç¡®ä¿æ¯ä¸ªç‚¹éƒ½è¢«æ­£ç¡®åºåˆ—åŒ–
                    data = []
                    for point in heatmap_points:
                        try:
                            data.append(point.dict())
                        except Exception as e:
                            print(f"åºåˆ—åŒ–çƒ­åŠ›å›¾ç‚¹æ—¶å‡ºé”™: {e}")
                            # ä½¿ç”¨æ‰‹åŠ¨æ„é€ çš„å­—å…¸ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
                            data.append({
                                "lat": point.lat,
                                "lng": point.lng,
                                "count": point.count
                            })
                print(f"ç”Ÿæˆäº† {len(data)} ä¸ªçƒ­åŠ›å›¾ç‚¹")

            elif view_type == "trajectory":
                # ç”Ÿæˆè½¨è¿¹æ•°æ®
                vehicle_id_param = vehicle_id if vehicle_id else ""
                tracks = data_processor.generate_track_data(df, vehicle_id_param)
                # ç¡®ä¿æ¯ä¸ªè½¨è¿¹éƒ½è¢«æ­£ç¡®åºåˆ—åŒ–
                data = []
                for track in tracks:
                    try:
                        data.append(track.dict())
                    except Exception as e:
                        print(f"åºåˆ—åŒ–è½¨è¿¹æ—¶å‡ºé”™: {e}")
                        # è·³è¿‡æœ‰é—®é¢˜çš„è½¨è¿¹
                        continue
                print(f"ç”Ÿæˆäº† {len(data)} æ¡è½¨è¿¹")

            else:  # distribution
                # ç”Ÿæˆåˆ†å¸ƒè§†å›¾æ•°æ®ï¼ˆç®€å•ç‚¹æ ‡è®°ï¼‰
                # æ ¹æ®æ•°æ®é‡æ™ºèƒ½è°ƒæ•´æ˜¾ç¤ºç‚¹æ•°
                if len(df) > 50000:
                    sample_size = 10000  # å¤§æ•°æ®é›†æ˜¾ç¤º1ä¸‡ä¸ªç‚¹
                elif len(df) > 20000:
                    sample_size = 8000  # ä¸­ç­‰æ•°æ®é›†æ˜¾ç¤º8åƒä¸ªç‚¹
                else:
                    sample_size = min(len(df), 5000)  # å°æ•°æ®é›†æ˜¾ç¤ºå…¨éƒ¨æˆ–5åƒä¸ªç‚¹
                df_sampled = df.sample(sample_size) if len(df) > sample_size else df

                data = []
                for _, row in df_sampled.iterrows():
                    # è®¡ç®—é€Ÿåº¦ï¼ˆå¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„é€Ÿåº¦ï¼Œåˆ™ä»GPSæ•°æ®è®¡ç®—ï¼‰
                    speed = 0.0
                    if "SPEED" in row and pd.notna(row["SPEED"]):
                        speed = float(row["SPEED"])
                    elif "speed" in row and pd.notna(row["speed"]):
                        speed = float(row["speed"])
                    elif "speed_kmh" in row and pd.notna(row["speed_kmh"]):
                        speed = float(row["speed_kmh"])

                    point = {
                        "lng": float(row["LON"]) / 1e5,
                        "lat": float(row["LAT"]) / 1e5,
                        "vehicle_id": str(row["COMMADDR"]),  # ç¡®ä¿è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        "timestamp": int(row["UTC"]),  # ç¡®ä¿è½¬æ¢ä¸ºPython int
                        "speed": speed  # æ·»åŠ é€Ÿåº¦å­—æ®µ
                    }
                    data.append(point)
                print(f"ç”Ÿæˆäº† {len(data)} ä¸ªåˆ†å¸ƒç‚¹")
        except Exception as process_error:
            print(f"æ•°æ®å¤„ç†é”™è¯¯: {str(process_error)}")
            import traceback
            print(traceback.format_exc())
            return TrafficDataResponse(
                success=False,
                message=f"æ•°æ®å¤„ç†å¤±è´¥: {str(process_error)}",
                view_type=view_type,
                data=[],
                points=0
            )

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        try:
            print("å¼€å§‹è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
            stats = data_processor.calculate_statistics(df)
            print("ç»Ÿè®¡ä¿¡æ¯è®¡ç®—å®Œæˆ")
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯å¯åºåˆ—åŒ–çš„
            # ä¿®å¤ï¼šåªä¼ é€’TrafficOverviewå¯¹è±¡æˆ–Noneç»™stats
            stats_obj = None
            if isinstance(stats, dict) and all(k in stats for k in
                                               ["total_vehicles", "total_points", "active_vehicles", "time_span",
                                                "coverage_area", "average_speed"]):
                try:
                    from .models import TrafficOverview
                    stats_obj = TrafficOverview(**stats)
                except ImportError:
                    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨None
                    stats_obj = None
            elif hasattr(stats, '__class__') and stats.__class__.__name__ == 'TrafficOverview':
                stats_obj = stats
            # å…¶ä½™æƒ…å†µä¸ä¼ é€’stats

            # è®°å½•æŸ¥è¯¢æˆåŠŸæ—¥å¿—
            try:
                logger.info(
                    "æ•°æ®æ€»è§ˆå¯è§†åŒ–",
                    f"æ•°æ®æ€»è§ˆæŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(data)} æ¡æ•°æ®",
                    source="APIè°ƒç”¨",
                    user="ç³»ç»Ÿ",
                    details={
                        "start_time": start_time,
                        "end_time": end_time,
                        "data_count": len(data),
                        "view_type": view_type,
                        "map_style": map_style
                    }
                )
            except Exception as log_error:
                print(f"æ—¥å¿—è®°å½•å¤±è´¥: {log_error}")

            return TrafficDataResponse(
                success=True,
                message="æ•°æ®è·å–æˆåŠŸ",
                view_type=view_type,
                data=convert_numpy_types(data),
                stats=stats_obj
            )
        except Exception as stats_error:
            print(f"ç»Ÿè®¡ä¿¡æ¯è®¡ç®—é”™è¯¯: {str(stats_error)}")

            # è®°å½•ç»Ÿè®¡è®¡ç®—å¤±è´¥æ—¥å¿—
            try:
                logger.warning(
                    "æ•°æ®æ€»è§ˆå¯è§†åŒ–",
                    f"æ•°æ®æ€»è§ˆç»Ÿè®¡ä¿¡æ¯è®¡ç®—å¤±è´¥ï¼Œä½†æ•°æ®è·å–æˆåŠŸ",
                    source="APIè°ƒç”¨",
                    user="ç³»ç»Ÿ",
                    details={
                        "start_time": start_time,
                        "end_time": end_time,
                        "data_count": len(data),
                        "view_type": view_type,
                        "error_message": str(stats_error)
                    }
                )
            except Exception as log_error:
                print(f"æ—¥å¿—è®°å½•å¤±è´¥: {log_error}")

            # å³ä½¿ç»Ÿè®¡å¤±è´¥ï¼Œä¹Ÿè¿”å›æ•°æ®
            return TrafficDataResponse(
                success=True,
                message="æ•°æ®è·å–æˆåŠŸï¼ˆç»Ÿè®¡ä¿¡æ¯è®¡ç®—å¤±è´¥ï¼‰",
                view_type=view_type,
                data=convert_numpy_types(data)
            )

    except Exception as e:
        print(f"=== å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯ ===")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        import traceback
        print(traceback.format_exc())

        # è®°å½•æœªé¢„æœŸé”™è¯¯æ—¥å¿—
        try:
            logger.error(
                "æ•°æ®æ€»è§ˆå¯è§†åŒ–",
                f"æ•°æ®æ€»è§ˆæŸ¥è¯¢å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}",
                source="APIè°ƒç”¨",
                user="ç³»ç»Ÿ",
                details={
                    "start_time": start_time,
                    "end_time": end_time,
                    "view_type": view_type,
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "error_stack": traceback.format_exc()
                }
            )
        except Exception as log_error:
            print(f"æ—¥å¿—è®°å½•å¤±è´¥: {log_error}")

        return TrafficDataResponse(
            success=False,
            message=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}",
            view_type=view_type,
            data=[],
            points=0
        )


@router.get("/heatmap", response_model=HeatmapResponse)
async def get_heatmap_data(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        resolution: float = Query(0.001, description="çƒ­åŠ›å›¾åˆ†è¾¨ç‡")
):
    """
    è·å–çƒ­åŠ›å›¾æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—æ•°æ®ï¼‰ã€‚
    """
    try:
        # è®°å½•çƒ­åŠ›å›¾è¯·æ±‚
        print(f"ğŸ”¥ çƒ­åŠ›å›¾APIå¼€å§‹: {start_time} - {end_time}")
        logger.traffic("çƒ­åŠ›å›¾ç”Ÿæˆ", f"å¼€å§‹ç”Ÿæˆçƒ­åŠ›å›¾ï¼Œæ—¶é—´èŒƒå›´: {start_time} - {end_time}",
                       source="APIè°ƒç”¨", user="ç”¨æˆ·",
                       details={"start_time": start_time, "end_time": end_time, "resolution": resolution})

        # ä¼˜å…ˆå°è¯•é¢„è®¡ç®—æ•°æ®
        print("å°è¯•ä½¿ç”¨é¢„è®¡ç®—çƒ­åŠ›å›¾æ•°æ®...")
        fast_heatmap_data = _get_fast_heatmap_data(start_time, end_time)

        if fast_heatmap_data:
            print(f"âœ… ä½¿ç”¨å¿«é€Ÿçƒ­åŠ›å›¾æ•°æ®: {len(fast_heatmap_data)} ä¸ªç‚¹")
            logger.traffic("çƒ­åŠ›å›¾ç”Ÿæˆ", f"ä½¿ç”¨é¢„è®¡ç®—çƒ­åŠ›å›¾æ•°æ®ï¼Œå…± {len(fast_heatmap_data)} ä¸ªç‚¹",
                           source="APIè°ƒç”¨", user="ç”¨æˆ·",
                           details={"point_count": len(fast_heatmap_data), "method": "precomputed"})
            return HeatmapResponse(
                success=True,
                data=fast_heatmap_data,
                message=f"å¿«é€Ÿçƒ­åŠ›å›¾ç”ŸæˆæˆåŠŸï¼Œå…± {len(fast_heatmap_data)} ä¸ªç‚¹",
                points=fast_heatmap_data
            )

        print("âš ï¸ é¢„è®¡ç®—æ•°æ®ä¸å¯ç”¨ï¼Œä½¿ç”¨å®æ—¶è®¡ç®—...")
        logger.traffic("çƒ­åŠ›å›¾ç”Ÿæˆ", "é¢„è®¡ç®—æ•°æ®ä¸å¯ç”¨ï¼Œä½¿ç”¨å®æ—¶è®¡ç®—", source="APIè°ƒç”¨", user="ç”¨æˆ·")

        # åŠ è½½æ•°æ®
        print("ğŸ“Š å¼€å§‹åŠ è½½æ•°æ®...")
        df = data_processor.load_data(start_time, end_time)

        if df.empty:
            logger.warning("çƒ­åŠ›å›¾ç”Ÿæˆ", "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®", source="APIè°ƒç”¨", user="ç”¨æˆ·")
            return HeatmapResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                points=[]
            )

        # ç”Ÿæˆçƒ­åŠ›å›¾æ•°æ®
        heatmap_points = data_processor.generate_heatmap_data(df, resolution)

        # è®°å½•æˆåŠŸæ—¥å¿—
        logger.traffic("çƒ­åŠ›å›¾ç”Ÿæˆ", f"å®æ—¶çƒ­åŠ›å›¾ç”ŸæˆæˆåŠŸï¼Œå…± {len(heatmap_points)} ä¸ªç‚¹",
                       source="APIè°ƒç”¨", user="ç”¨æˆ·",
                       details={"point_count": len(heatmap_points), "method": "realtime"})

        # æ„é€ å“åº”
        return HeatmapResponse(
            success=True,
            points=heatmap_points
        )
    except Exception as e:
        logger.error("çƒ­åŠ›å›¾ç”Ÿæˆ", f"è·å–çƒ­åŠ›å›¾æ•°æ®å¤±è´¥: {str(e)}", source="APIè°ƒç”¨", user="ç”¨æˆ·")
        return HeatmapResponse(
            success=False,
            message=f"è·å–çƒ­åŠ›å›¾æ•°æ®å¤±è´¥: {str(e)}",
            points=[]
        )


@router.get("/track", response_model=TracksResponse)
async def get_track(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        vehicle_id: Optional[str] = Query(None, description="è½¦è¾†IDï¼Œå¯é€‰"),
        view_type: str = Query("trajectory", description="è§†å›¾ç±»å‹ï¼štrajectory, path, stops"),
        performance_mode: str = Query("normal", description="æ€§èƒ½æ¨¡å¼ï¼šfast, medium, full"),
        max_points: int = Query(5000, description="æœ€å¤§è¿”å›ç‚¹æ•°")
):
    """
    æŒ‰æ—¶é—´æ®µå’Œè½¦è¾†IDæŸ¥è¯¢è½¦è¾†è½¨è¿¹æ•°æ®ã€‚
    æ”¯æŒæ€§èƒ½æ¨¡å¼å’Œæœ€å¤§ç‚¹æ•°é™åˆ¶ã€‚
    """
    try:
        # è®°å½•è½¨è¿¹æŸ¥è¯¢è¯·æ±‚
        logger.traffic("è½¨è¿¹æŸ¥è¯¢", f"å¼€å§‹æŸ¥è¯¢è½¨è¿¹ï¼Œæ—¶é—´èŒƒå›´: {start_time} - {end_time}, è½¦è¾†: {vehicle_id}",
                       source="APIè°ƒç”¨", user="ç”¨æˆ·",
                       details={"start_time": start_time, "end_time": end_time, "vehicle_id": vehicle_id,
                                "performance_mode": performance_mode, "max_points": max_points})

        print(
            f"ğŸš— è½¨è¿¹æŸ¥è¯¢å‚æ•°: æ—¶é—´={start_time}-{end_time}, è½¦è¾†={vehicle_id}, æ¨¡å¼={performance_mode}, æœ€å¤§ç‚¹æ•°={max_points}")

        # åŠ è½½æ•°æ®
        if vehicle_id:
            df = data_processor.load_data(start_time, end_time, vehicle_id)
        else:
            df = data_processor.load_data(start_time, end_time)

        if df.empty:
            print(f"âš ï¸ æœªæ‰¾åˆ°è½¦è¾† {vehicle_id} åœ¨æ—¶é—´æ®µ {start_time}-{end_time} çš„æ•°æ®")
            logger.warning("è½¨è¿¹æŸ¥è¯¢", f"æœªæ‰¾åˆ°è½¦è¾† {vehicle_id} åœ¨æŒ‡å®šæ—¶é—´æ®µçš„æ•°æ®", source="APIè°ƒç”¨", user="ç”¨æˆ·")
            return TracksResponse(
                success=False,
                message=f"æœªæ‰¾åˆ°è½¦è¾† {vehicle_id} åœ¨æŒ‡å®šæ—¶é—´æ®µçš„æ•°æ®",
                tracks=[],
                points=0
            )

        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {len(df)} æ¡è®°å½•")

        # æ ¹æ®æ€§èƒ½æ¨¡å¼é‡‡æ ·æ•°æ®
        if performance_mode == "fast" and len(df) > max_points:
            # å¿«é€Ÿæ¨¡å¼ï¼šå‡åŒ€é‡‡æ ·
            sample_ratio = max_points / len(df)
            df = df.sample(frac=sample_ratio)
            print(f"âš¡ å¿«é€Ÿæ¨¡å¼: é‡‡æ ·å {len(df)} æ¡è®°å½•")
        elif performance_mode == "medium" and len(df) > max_points:
            # ä¸­ç­‰æ¨¡å¼ï¼šä¿ç•™å…³é”®ç‚¹çš„é‡‡æ ·
            # è¿™é‡Œç®€åŒ–ä¸ºéšæœºé‡‡æ ·ï¼Œå®é™…å¯ä»¥ç”¨æ›´å¤æ‚çš„ç®—æ³•
            sample_ratio = max_points / len(df)
            df = df.sample(frac=sample_ratio)
            print(f"ğŸ“Š ä¸­ç­‰æ¨¡å¼: é‡‡æ ·å {len(df)} æ¡è®°å½•")

        # ç”Ÿæˆè½¨è¿¹æ•°æ®
        tracks = data_processor.generate_track_data(df, vehicle_id)

        if not tracks:
            print(f"âš ï¸ è½¨è¿¹ç”Ÿæˆå¤±è´¥: è½¦è¾† {vehicle_id}")
            return TracksResponse(
                success=False,
                message=f"æœªèƒ½ç”Ÿæˆè½¦è¾† {vehicle_id} çš„è½¨è¿¹æ•°æ®",
                tracks=[],
                points=0
            )

        print(f"âœ… è½¨è¿¹ç”ŸæˆæˆåŠŸ: {len(tracks)} æ¡è½¨è¿¹, å…± {sum(len(t.points) for t in tracks)} ä¸ªç‚¹")

        # è®°å½•æˆåŠŸæ—¥å¿—
        logger.traffic("è½¨è¿¹æŸ¥è¯¢", f"è½¨è¿¹æŸ¥è¯¢æˆåŠŸï¼Œå…± {len(tracks)} æ¡è½¨è¿¹, {sum(len(t.points) for t in tracks)} ä¸ªç‚¹",
                       source="APIè°ƒç”¨", user="ç”¨æˆ·",
                       details={"track_count": len(tracks), "point_count": sum(len(t.points) for t in tracks),
                                "vehicle_id": vehicle_id, "performance_mode": performance_mode})

        # æ„é€ å“åº”
        return TracksResponse(
            success=True,
            tracks=tracks
        )
    except Exception as e:
        import traceback
        print(f"âŒ è½¨è¿¹æŸ¥è¯¢å¤±è´¥: {str(e)}")
        print(traceback.format_exc())
        logger.error("è½¨è¿¹æŸ¥è¯¢", f"è·å–è½¨è¿¹æ•°æ®å¤±è´¥: {str(e)}", source="APIè°ƒç”¨", user="ç”¨æˆ·")
        return TracksResponse(
            success=False,
            message=f"è·å–è½¨è¿¹æ•°æ®å¤±è´¥: {str(e)}",
            tracks=[],
            points=0
        )


@router.get("/clear-cache")
async def clear_cache():
    """
    æ¸…é™¤æ•°æ®å¤„ç†å™¨çš„ç¼“å­˜ã€‚
    """
    data_processor.clear_cache()
    return {"success": True, "message": "ç¼“å­˜å·²æ¸…é™¤"}


@router.get("/orders/analysis")
async def get_orders_analysis(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰")
):
    """
    æ ¹æ®è®¢å•èµ·æ­¢æ—¶é—´ï¼Œåˆ†æä¹˜å®¢ä¹˜è½¦çš„è·ç¦»ä¸æ—¶é—´åˆ†å¸ƒã€‚
    è¿”å›è®¢å•è€—æ—¶ã€è·¯ç¨‹çš„åˆ†å¸ƒæ•°æ®ï¼Œä»¥åŠæŒ‰æ—¶æ®µï¼ˆå¦‚å°æ—¶ï¼‰èšåˆçš„è®¢å•æ•°é‡ã€‚
    """
    try:
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)

        if df.empty:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                "data": {}
            }

        # æ¨¡æ‹Ÿè®¢å•æ•°æ®ï¼ˆå®é™…åº”ä»è®¢å•è¡¨è·å–ï¼‰
        # è¿™é‡Œæˆ‘ä»¬å‡è®¾æ¯ä¸ªè½¦è¾†çš„è¿ç»­è½¨è¿¹ç‚¹æ„æˆä¸€ä¸ª"è®¢å•"
        orders_data = []

        # æŒ‰è½¦è¾†åˆ†ç»„å¤„ç†
        for vehicle_id, group in df.groupby('COMMADDR'):
            # æŒ‰æ—¶é—´æ’åº
            group = group.sort_values('UTC')

            # å¦‚æœè½¨è¿¹ç‚¹å¤ªå°‘ï¼Œè·³è¿‡
            if len(group) < 2:
                continue

            # è®¡ç®—è®¢å•ä¿¡æ¯
            start_point = group.iloc[0]
            end_point = group.iloc[-1]

            duration_min = (end_point['UTC'] - start_point['UTC']) / 60

            # ç®€åŒ–çš„è·ç¦»è®¡ç®—
            start_lat, start_lon = start_point['LAT'] / 1e5, start_point['LON'] / 1e5
            end_lat, end_lon = end_point['LAT'] / 1e5, end_point['LON'] / 1e5

            # ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»ä½œä¸ºç®€åŒ–ï¼ˆå®é™…åº”ä½¿ç”¨Haversineå…¬å¼ï¼‰
            dist = ((end_lon - start_lon) ** 2 + (end_lat - start_lat) ** 2) ** 0.5
            distance_km = dist * 111  # 1åº¦çº¦ç­‰äº111å…¬é‡Œ

            orders_data.append({
                'vehicle_id': vehicle_id,
                'start_time': start_point['UTC'],
                'end_time': end_point['UTC'],
                'duration_min': duration_min,
                'distance_km': distance_km
            })

        if not orders_data:
            return {
                "success": False,
                "message": "æœªèƒ½ç”Ÿæˆè®¢å•æ•°æ®",
                "data": {}
            }

        # è½¬æ¢ä¸ºDataFrameè¿›è¡Œåˆ†æ
        orders_df = pd.DataFrame(orders_data)

        # 1. è€—æ—¶åˆ†å¸ƒ
        duration_bins = [0, 5, 10, 15, 20, 30, 45, 60, 90, 120, 180]
        duration_labels = [f"{bins[i]}-{bins[i + 1]}" for i in range(len(duration_bins) - 1)]

        orders_df['duration_bin'] = pd.cut(
            orders_df['duration_min'],
            bins=duration_bins,
            labels=duration_labels,
            include_lowest=True
        )

        duration_dist = orders_df['duration_bin'].value_counts().sort_index()
        duration_distribution = [
            {"range": str(idx), "count": int(count)}
            for idx, count in duration_dist.items()
        ]

        # 2. è·ç¦»åˆ†å¸ƒ
        distance_bins = [0, 1, 2, 3, 5, 10, 15, 20, 30, 50]
        distance_labels = [f"{bins[i]}-{bins[i + 1]}" for i in range(len(distance_bins) - 1)]

        orders_df['distance_bin'] = pd.cut(
            orders_df['distance_km'],
            bins=distance_bins,
            labels=distance_labels,
            include_lowest=True
        )

        distance_dist = orders_df['distance_bin'].value_counts().sort_index()
        distance_distribution = [
            {"range": str(idx), "count": int(count)}
            for idx, count in distance_dist.items()
        ]

        # 3. æŒ‰å°æ—¶åˆ†å¸ƒ
        orders_df['hour'] = pd.to_datetime(orders_df['start_time'], unit='s').dt.hour
        hourly_dist = orders_df['hour'].value_counts().sort_index()

        hourly_distribution = [
            {"hour": int(hour), "count": int(count)}
            for hour, count in hourly_dist.items()
        ]

        return {
            "success": True,
            "data": {
                "duration_distribution": duration_distribution,
                "distance_distribution": distance_distribution,
                "hourly_distribution": hourly_distribution,
                "total_orders": len(orders_data),
                "avg_duration": round(orders_df['duration_min'].mean(), 1),
                "avg_distance": round(orders_df['distance_km'].mean(), 1)
            }
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"åˆ†æè®¢å•æ•°æ®å¤±è´¥: {str(e)}",
            "data": {}
        }


@router.get("/heatmap/time-filtered")
async def get_time_filtered_heatmap(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        resolution: float = Query(0.001, description="çƒ­åŠ›å›¾åˆ†è¾¨ç‡")
):
    """
    è·å–æŒ‰æ—¶é—´æ®µè¿‡æ»¤çš„çƒ­åŠ›å›¾æ•°æ®ã€‚
    è¿”å›æ—©é«˜å³°ã€åˆé¤æ—¶é—´ã€æ™šé«˜å³°å’Œå¤œé—´çš„çƒ­åŠ›å›¾æ•°æ®ã€‚
    """
    try:
        # ç”ŸæˆæŒ‰æ—¶é—´æ®µè¿‡æ»¤çš„çƒ­åŠ›å›¾æ•°æ®
        result = heatmap_generator.generate_time_filtered_heatmap(
            start_time, end_time, resolution=resolution
        )

        return {
            "success": True,
            "data": result
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"è·å–æŒ‰æ—¶é—´æ®µè¿‡æ»¤çš„çƒ­åŠ›å›¾æ•°æ®å¤±è´¥: {str(e)}",
            "data": {}
        }


@router.get("/heatmap/pickup")
async def get_pickup_heatmap(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        resolution: float = Query(0.001, description="çƒ­åŠ›å›¾åˆ†è¾¨ç‡")
):
    """
    è·å–ä¸Šå®¢ç‚¹çƒ­åŠ›å›¾æ•°æ®ã€‚
    """
    try:
        # ç”Ÿæˆä¸Šå®¢ç‚¹çƒ­åŠ›å›¾æ•°æ®
        result = heatmap_generator.generate_pickup_heatmap(
            start_time, end_time, resolution=resolution
        )

        return {
            "success": True,
            "data": result
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"è·å–ä¸Šå®¢ç‚¹çƒ­åŠ›å›¾æ•°æ®å¤±è´¥: {str(e)}",
            "data": {}
        }


@router.get("/track/metrics")
async def get_track_metrics(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        vehicle_id: Optional[str] = Query(None, description="è½¦è¾†IDï¼Œå¯é€‰")
):
    """
    è·å–è½¨è¿¹æŒ‡æ ‡ã€‚
    """
    try:
        # æŸ¥è¯¢è½¨è¿¹æ•°æ®
        tracks = track_analyzer.query_track(start_time, end_time, vehicle_id)

        # è®¡ç®—è½¨è¿¹æŒ‡æ ‡
        metrics = track_analyzer.calculate_track_metrics(tracks)

        return {
            "success": True,
            "data": metrics
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"è·å–è½¨è¿¹æŒ‡æ ‡å¤±è´¥: {str(e)}",
            "data": {}
        }


@router.get("/track/similar")
async def get_similar_tracks(
        track_id: str = Query(..., description="å‚è€ƒè½¨è¿¹çš„è½¦è¾†ID"),
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        similarity_threshold: float = Query(0.7, description="ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰")
):
    """
    æŸ¥æ‰¾ç›¸ä¼¼è½¨è¿¹ã€‚
    """
    try:
        # æŸ¥æ‰¾ç›¸ä¼¼è½¨è¿¹
        similar_tracks = track_analyzer.find_similar_tracks(
            track_id, start_time, end_time, similarity_threshold
        )

        return {
            "success": True,
            "data": {
                "reference_track_id": track_id,
                "similar_tracks": similar_tracks,
                "total_found": len(similar_tracks)
            }
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"æŸ¥æ‰¾ç›¸ä¼¼è½¨è¿¹å¤±è´¥: {str(e)}",
            "data": {
                "reference_track_id": track_id,
                "similar_tracks": [],
                "total_found": 0
            }
        }


@router.get("/sample-vehicles")
async def get_sample_vehicles(
        start_time: float = Query(1379030400, description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼Œé»˜è®¤2013-09-13 08:00ï¼‰"),
        end_time: float = Query(1379044800, description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼Œé»˜è®¤2013-09-13 12:00ï¼‰"),
        limit: int = Query(15, description="è¿”å›çš„è½¦è¾†æ•°é‡é™åˆ¶")
):
    """
    è·å–æŒ‡å®šæ—¶é—´æ®µå†…çš„ç¤ºä¾‹è½¦è¾†IDåˆ—è¡¨ï¼Œç”¨äºè½¨è¿¹æŸ¥è¯¢æµ‹è¯•ï¼ˆè¶…é«˜é€Ÿç‰ˆæœ¬ï¼‰
    """
    try:
        print(f"ğŸš€ è¶…é«˜é€Ÿè·å–ç¤ºä¾‹è½¦è¾†: {start_time} - {end_time}, é™åˆ¶: {limit}")

        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"fast_sample_vehicles_{start_time}_{end_time}_{limit}"

        # æ£€æŸ¥ç¼“å­˜
        if hasattr(data_processor, '_sample_cache') and cache_key in data_processor._sample_cache:
            print("âš¡ ä½¿ç”¨ç¼“å­˜çš„ç¤ºä¾‹è½¦è¾†æ•°æ®ï¼ˆç§’çº§å“åº”ï¼‰")
            return data_processor._sample_cache[cache_key]

        # ç›´æ¥ä½¿ç”¨é¢„å¤„ç†æ–‡ä»¶è¿›è¡Œè¶…å¿«é€ŸæŸ¥è¯¢
        result = await get_sample_vehicles_from_preprocessed(start_time, end_time, limit)

        # ç¼“å­˜ç»“æœ
        if not hasattr(data_processor, '_sample_cache'):
            data_processor._sample_cache = {}
        data_processor._sample_cache[cache_key] = result

        # æ¸…ç†ç¼“å­˜
        if len(data_processor._sample_cache) > 20:
            oldest_key = next(iter(data_processor._sample_cache))
            del data_processor._sample_cache[oldest_key]

        print(f"âœ… è¶…é«˜é€Ÿç¤ºä¾‹è½¦è¾†è·å–å®Œæˆ: {len(result.get('vehicles', []))} ä¸ªè½¦è¾†")
        return result

    except Exception as e:
        logger.error(f"è·å–ç¤ºä¾‹è½¦è¾†æ—¶å‡ºé”™: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return {"success": False, "message": f"è·å–ç¤ºä¾‹è½¦è¾†å¤±è´¥: {str(e)}", "vehicles": []}


async def get_sample_vehicles_from_preprocessed(start_time: float, end_time: float, limit: int):
    """
    ç›´æ¥ä»é¢„å¤„ç†æ–‡ä»¶å¿«é€Ÿè·å–ç¤ºä¾‹è½¦è¾†ï¼ˆä¸ä¾èµ–å®Œæ•´æ•°æ®åŠ è½½ï¼‰
    """
    import pandas as pd
    import os
    import json

    try:
        # é¢„å¤„ç†æ–‡ä»¶è·¯å¾„
        processed_dir = os.path.join(data_processor.data_dir, 'processed')
        indexes_dir = os.path.join(data_processor.data_dir, 'indexes')

        print(f"ğŸ“ ä½¿ç”¨é¢„å¤„ç†ç›®å½•: {processed_dir}")

        # è®¡ç®—éœ€è¦æŸ¥è¯¢çš„å°æ—¶æ–‡ä»¶
        start_hour = (int(start_time) // 3600) * 3600
        end_hour = (int(end_time) // 3600) * 3600

        # é™åˆ¶æŸ¥è¯¢èŒƒå›´ï¼ˆæœ€å¤šæŸ¥3ä¸ªå°æ—¶çš„æ–‡ä»¶ä»¥ä¿è¯é€Ÿåº¦ï¼‰
        max_hours = 3
        hour_files = []
        current_hour = start_hour

        while current_hour <= end_hour and len(hour_files) < max_hours:
            hour_file = os.path.join(processed_dir, f"hour_{current_hour}.parquet")
            if os.path.exists(hour_file):
                hour_files.append(hour_file)
            current_hour += 3600

        if not hour_files:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°å¯¹åº”æ—¶é—´æ®µçš„é¢„å¤„ç†æ•°æ®",
                "vehicles": []
            }

        print(f"ğŸ“Š è¯»å– {len(hour_files)} ä¸ªé¢„å¤„ç†æ–‡ä»¶...")

        # åªè¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶ä»¥è·å¾—æœ€å¿«é€Ÿåº¦
        sample_file = hour_files[0]

        try:
            # è¯»å–parquetæ–‡ä»¶ï¼ˆåªè¯»å–éœ€è¦çš„åˆ—ï¼‰
            df = pd.read_parquet(sample_file, columns=['COMMADDR', 'TIMESTAMP'])

            print(f"ğŸ“ˆ æˆåŠŸè¯»å–æ•°æ®: {len(df)} æ¡è®°å½•")

            # ç­›é€‰æ—¶é—´èŒƒå›´
            df = df[(df['TIMESTAMP'] >= start_time) & (df['TIMESTAMP'] <= end_time)]

            if df.empty:
                # å¦‚æœå½“å‰æ–‡ä»¶æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ–‡ä»¶
                if len(hour_files) > 1:
                    df = pd.read_parquet(hour_files[1], columns=['COMMADDR', 'TIMESTAMP'])
                    df = df[(df['TIMESTAMP'] >= start_time) & (df['TIMESTAMP'] <= end_time)]

            if df.empty:
                return {
                    "success": False,
                    "message": "æŒ‡å®šæ—¶é—´æ®µå†…æ²¡æœ‰è½¦è¾†æ•°æ®",
                    "vehicles": []
                }

            # å¿«é€Ÿç»Ÿè®¡è½¦è¾†æ•°æ®ç‚¹
            vehicle_counts = df['COMMADDR'].value_counts().head(limit * 2)

            vehicles = []
            for vehicle_id, count in vehicle_counts.items():
                if count >= 3:  # è‡³å°‘3ä¸ªæ•°æ®ç‚¹
                    vehicles.append({
                        "vehicle_id": str(vehicle_id),
                        "data_points": int(count),
                        "description": f"è½¦è¾† {vehicle_id} ({count}ä¸ªæ•°æ®ç‚¹)"
                    })

                if len(vehicles) >= limit:
                    break

            return {
                "success": True,
                "message": f"å¿«é€Ÿæ‰¾åˆ° {len(vehicles)} ä¸ªæ´»è·ƒè½¦è¾†",
                "vehicles": vehicles,
                "time_range": f"{start_time} - {end_time}",
                "total_vehicles": len(df['COMMADDR'].unique()),
                "data_source": "é¢„å¤„ç†æ–‡ä»¶ï¼ˆæé€Ÿæ¨¡å¼ï¼‰"
            }

        except Exception as file_error:
            print(f"è¯»å–é¢„å¤„ç†æ–‡ä»¶å¤±è´¥: {file_error}")

            # é™çº§åˆ°é™æ€ç¤ºä¾‹è½¦è¾†åˆ—è¡¨
            return get_static_sample_vehicles(start_time, end_time, limit)

    except Exception as e:
        print(f"é¢„å¤„ç†æ–‡ä»¶æŸ¥è¯¢å¤±è´¥: {e}")
        return get_static_sample_vehicles(start_time, end_time, limit)


def get_static_sample_vehicles(start_time: float, end_time: float, limit: int):
    """
    æä¾›é™æ€çš„ç¤ºä¾‹è½¦è¾†åˆ—è¡¨ä½œä¸ºåå¤‡æ–¹æ¡ˆ
    ä½¿ç”¨çœŸå®æ•°æ®æ ¼å¼çš„æ•°å­—è½¦è¾†ID
    """
    # åŸºäºçœŸå®æ•°æ®çš„å¸¸è§è½¦è¾†IDï¼ˆæ•°å­—æ ¼å¼ï¼‰
    static_vehicles = [
        {"vehicle_id": "15053114280", "data_points": 150, "description": "è½¦è¾† 15053114280 (150ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114281", "data_points": 120, "description": "è½¦è¾† 15053114281 (120ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114282", "data_points": 200, "description": "è½¦è¾† 15053114282 (200ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114283", "data_points": 180, "description": "è½¦è¾† 15053114283 (180ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114284", "data_points": 160, "description": "è½¦è¾† 15053114284 (160ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114285", "data_points": 140, "description": "è½¦è¾† 15053114285 (140ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114286", "data_points": 190, "description": "è½¦è¾† 15053114286 (190ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114287", "data_points": 170, "description": "è½¦è¾† 15053114287 (170ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114288", "data_points": 130, "description": "è½¦è¾† 15053114288 (130ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114289", "data_points": 210, "description": "è½¦è¾† 15053114289 (210ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114290", "data_points": 155, "description": "è½¦è¾† 15053114290 (155ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114291", "data_points": 175, "description": "è½¦è¾† 15053114291 (175ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114292", "data_points": 165, "description": "è½¦è¾† 15053114292 (165ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114293", "data_points": 145, "description": "è½¦è¾† 15053114293 (145ä¸ªæ•°æ®ç‚¹)"},
        {"vehicle_id": "15053114294", "data_points": 185, "description": "è½¦è¾† 15053114294 (185ä¸ªæ•°æ®ç‚¹)"}
    ]

    # è¿”å›æŒ‡å®šæ•°é‡çš„è½¦è¾†
    selected_vehicles = static_vehicles[:limit]

    return {
        "success": True,
        "message": f"æä¾› {len(selected_vehicles)} ä¸ªç¤ºä¾‹è½¦è¾†ï¼ˆé™æ€åˆ—è¡¨ï¼Œæ•°å­—æ ¼å¼ï¼‰",
        "vehicles": selected_vehicles,
        "time_range": f"{start_time} - {end_time}",
        "total_vehicles": len(static_vehicles),
        "data_source": "é™æ€ç¤ºä¾‹æ•°æ®ï¼ˆåå¤‡æ¨¡å¼ï¼Œå·²ä¿®æ­£ä¸ºæ•°å­—æ ¼å¼ï¼‰"
    }


@router.get("/anomaly/detection", response_model=dict)
async def detect_anomalies(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        detection_types: str = Query("all",
                                     description="æ£€æµ‹ç±»å‹ï¼šall, long_stop, abnormal_route, speed_anomaly, cluster_anomaly"),
        threshold_params: Optional[str] = Query(None, description="é˜ˆå€¼å‚æ•°JSONå­—ç¬¦ä¸²")
):
    """
    å¼‚å¸¸æ£€æµ‹API - æ£€æµ‹å„ç§ç±»å‹çš„äº¤é€šå¼‚å¸¸
    """
    try:
        # è®°å½•å¼‚å¸¸æ£€æµ‹è¯·æ±‚
        logger.anomaly("å¼‚å¸¸æ£€æµ‹", f"å¼€å§‹å¼‚å¸¸æ£€æµ‹ï¼Œæ—¶é—´èŒƒå›´: {start_time} - {end_time}",
                       source="APIè°ƒç”¨", user="ç”¨æˆ·",
                       details={"start_time": start_time, "end_time": end_time, "detection_types": detection_types})

        import json

        # è§£æé˜ˆå€¼å‚æ•°
        thresholds = {}
        if threshold_params:
            try:
                thresholds = json.loads(threshold_params)
            except json.JSONDecodeError:
                pass

        # é»˜è®¤é˜ˆå€¼
        default_thresholds = {
            "long_stop_duration": 300,  # 5åˆ†é’Ÿ
            "speed_threshold_low": 5,  # ä½é€Ÿé˜ˆå€¼ km/h
            "speed_threshold_high": 80,  # é«˜é€Ÿé˜ˆå€¼ km/h
            "detour_ratio": 1.5,  # ç»•è·¯æ¯”ä¾‹
            "cluster_density": 50  # èšé›†å¯†åº¦
        }
        thresholds = {**default_thresholds, **thresholds}

        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)

        if df.empty:
            logger.warning("å¼‚å¸¸æ£€æµ‹", "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®", source="APIè°ƒç”¨", user="ç”¨æˆ·")
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                "anomalies": [],
                "statistics": {}
            }

        # æ£€æµ‹å¼‚å¸¸
        anomalies = data_processor.detect_anomalies(df, detection_types, thresholds)

        # è®¡ç®—å¼‚å¸¸ç»Ÿè®¡
        stats = data_processor.calculate_anomaly_statistics(anomalies)

        return {
            "success": True,
            "message": f"æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(anomalies)} ä¸ªå¼‚å¸¸äº‹ä»¶",
            "anomalies": convert_numpy_types(anomalies),
            "statistics": convert_numpy_types(stats),
            "thresholds_used": thresholds
        }

    except Exception as e:
        print(f"å¼‚å¸¸æ£€æµ‹é”™è¯¯: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return {
            "success": False,
            "message": f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}",
            "anomalies": [],
            "statistics": {}
        }


@router.get("/anomaly/realtime", response_model=dict)
async def get_realtime_anomalies(
        time_window: int = Query(3600, description="æ—¶é—´çª—å£ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å°æ—¶"),
        limit: int = Query(50, description="è¿”å›å¼‚å¸¸æ•°é‡é™åˆ¶")
):
    """
    è·å–å®æ—¶å¼‚å¸¸äº‹ä»¶
    """
    try:
        import time

        # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆæœ€è¿‘time_windowç§’ï¼‰
        end_time = time.time()
        start_time = end_time - time_window

        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)

        if df.empty:
            return {
                "success": True,
                "anomalies": [],
                "total_count": 0,
                "time_range": {"start": start_time, "end": end_time}
            }

        # æ£€æµ‹å¼‚å¸¸
        anomalies = data_processor.detect_anomalies(df, "all", {})

        # æŒ‰æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
        anomalies.sort(key=lambda x: x.get('timestamp', 0), reverse=True)
        limited_anomalies = anomalies[:limit]

        return {
            "success": True,
            "anomalies": convert_numpy_types(limited_anomalies),
            "total_count": len(anomalies),
            "time_range": {"start": start_time, "end": end_time}
        }

    except Exception as e:
        print(f"è·å–å®æ—¶å¼‚å¸¸å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"è·å–å®æ—¶å¼‚å¸¸å¤±è´¥: {str(e)}",
            "anomalies": [],
            "total_count": 0
        }


@router.get("/anomaly/types", response_model=dict)
async def get_anomaly_types():
    """
    è·å–æ”¯æŒçš„å¼‚å¸¸ç±»å‹åˆ—è¡¨
    """
    anomaly_types = [
        {
            "type": "long_stop",
            "name": "é•¿æ—¶é—´åœè½¦",
            "description": "è½¦è¾†åœ¨åŒä¸€ä½ç½®åœç•™æ—¶é—´è¿‡é•¿",
            "icon": "parking",
            "color": "#ff6b6b",
            "default_threshold": {"duration": 300}
        },
        {
            "type": "abnormal_route",
            "name": "å¼‚å¸¸ç»•è·¯",
            "description": "è½¦è¾†è¡Œé©¶è·¯å¾„æ˜æ˜¾åç¦»æ­£å¸¸è·¯çº¿",
            "icon": "route",
            "color": "#ffa726",
            "default_threshold": {"detour_ratio": 1.5}
        },
        {
            "type": "speed_anomaly",
            "name": "é€Ÿåº¦å¼‚å¸¸",
            "description": "è½¦è¾†é€Ÿåº¦å¼‚å¸¸ï¼ˆè¿‡å¿«æˆ–è¿‡æ…¢ï¼‰",
            "icon": "speed",
            "color": "#42a5f5",
            "default_threshold": {"low": 5, "high": 80}
        },
        {
            "type": "cluster_anomaly",
            "name": "å¼‚å¸¸èšé›†",
            "description": "è½¦è¾†åœ¨ç‰¹å®šåŒºåŸŸå¼‚å¸¸èšé›†",
            "icon": "cluster",
            "color": "#ab47bc",
            "default_threshold": {"density": 50}
        },
        {
            "type": "trajectory_anomaly",
            "name": "è½¨è¿¹å¼‚å¸¸",
            "description": "è½¦è¾†è½¨è¿¹æ¨¡å¼å¼‚å¸¸",
            "icon": "trajectory",
            "color": "#66bb6a",
            "default_threshold": {"pattern_threshold": 0.7}
        }
    ]

    return {
        "success": True,
        "anomaly_types": anomaly_types
    }


@router.get("/anomaly/heatmap", response_model=dict)
async def get_anomaly_heatmap(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        anomaly_type: str = Query("all", description="å¼‚å¸¸ç±»å‹"),
        resolution: float = Query(0.002, description="çƒ­åŠ›å›¾åˆ†è¾¨ç‡")
):
    """
    è·å–å¼‚å¸¸äº‹ä»¶çƒ­åŠ›å›¾æ•°æ®
    """
    try:
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)

        if df.empty:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                "heatmap_points": []
            }

        # æ£€æµ‹å¼‚å¸¸
        anomalies = data_processor.detect_anomalies(df, anomaly_type, {})

        # ç”Ÿæˆå¼‚å¸¸çƒ­åŠ›å›¾
        heatmap_points = data_processor.generate_anomaly_heatmap(anomalies, resolution)

        return {
            "success": True,
            "heatmap_points": convert_numpy_types(heatmap_points),
            "total_anomalies": len(anomalies),
            "resolution": resolution
        }

    except Exception as e:
        print(f"ç”Ÿæˆå¼‚å¸¸çƒ­åŠ›å›¾å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"ç”Ÿæˆå¼‚å¸¸çƒ­åŠ›å›¾å¤±è´¥: {str(e)}",
            "heatmap_points": []
        }


@router.get("/spatiotemporal/dynamic-heatmap", response_model=DynamicHeatmapResponse)
async def get_dynamic_heatmap(
        start_time: str,
        end_time: str,
        temporal_resolution: int = 15,
        spatial_resolution: float = 0.001,
        smoothing: bool = True
):
    """
    è·å–åŠ¨æ€çƒ­åŠ›å›¾æ•°æ®

    Args:
        start_time: å¼€å§‹æ—¶é—´ (ISOæ ¼å¼)
        end_time: ç»“æŸæ—¶é—´ (ISOæ ¼å¼)
        temporal_resolution: æ—¶é—´åˆ†è¾¨ç‡ï¼ˆåˆ†é’Ÿï¼‰
        spatial_resolution: ç©ºé—´åˆ†è¾¨ç‡ï¼ˆåº¦ï¼‰
        smoothing: æ˜¯å¦å¹³æ»‘å¤„ç†
    """
    try:
        logger.info(f"è·å–åŠ¨æ€çƒ­åŠ›å›¾: {start_time} to {end_time}, æ—¶é—´åˆ†è¾¨ç‡: {temporal_resolution}åˆ†é’Ÿ")

        # æ—¶é—´æ ¼å¼è½¬æ¢
        start_timestamp = convert_time_to_timestamp(start_time)
        end_timestamp = convert_time_to_timestamp(end_time)

        # è·å–æ•°æ®
        processor = TrafficDataProcessor()
        df = processor.get_data_by_time_range(start_timestamp, end_timestamp)

        if df.empty:
            return DynamicHeatmapResponse(
                success=True,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®",
                frames=[],
                time_series_stats={},
                spatial_stats={}
            )

        # ç”ŸæˆåŠ¨æ€çƒ­åŠ›å›¾
        frames = processor.generate_dynamic_heatmap(
            df,
            temporal_resolution=temporal_resolution,
            spatial_resolution=spatial_resolution,
            smoothing=smoothing
        )

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        time_series_stats = processor._calculate_time_series_stats(frames)
        spatial_stats = processor._calculate_spatial_stats(df)

        # è½¬æ¢numpyç±»å‹
        frames = convert_numpy_types(frames)
        time_series_stats = convert_numpy_types(time_series_stats)
        spatial_stats = convert_numpy_types(spatial_stats)

        return DynamicHeatmapResponse(
            success=True,
            message=f"æˆåŠŸç”Ÿæˆ{len(frames)}ä¸ªæ—¶é—´å¸§çš„åŠ¨æ€çƒ­åŠ›å›¾",
            frames=frames,
            time_series_stats=time_series_stats,
            spatial_stats=spatial_stats
        )

    except Exception as e:
        logger.error(f"è·å–åŠ¨æ€çƒ­åŠ›å›¾æ—¶å‡ºé”™: {str(e)}")
        return DynamicHeatmapResponse(
            success=False,
            message=f"è·å–åŠ¨æ€çƒ­åŠ›å›¾å¤±è´¥: {str(e)}",
            frames=[],
            time_series_stats={},
            spatial_stats={}
        )


@router.post("/spatiotemporal/clustering", response_model=ClusteringResponse)
async def perform_clustering_analysis(
        start_time: str,
        end_time: str,
        request: ClusteringRequest
):
    """
    æ‰§è¡Œèšç±»åˆ†æ

    Args:
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        request: èšç±»è¯·æ±‚å‚æ•°
    """
    try:
        logger.info(f"æ‰§è¡Œèšç±»åˆ†æ: {request.algorithm}, æ•°æ®ç±»å‹: {request.data_type}")

        # æ—¶é—´è½¬æ¢
        start_timestamp = convert_time_to_timestamp(start_time)
        end_timestamp = convert_time_to_timestamp(end_time)

        # è·å–æ•°æ®
        processor = TrafficDataProcessor()
        df = processor.get_data_by_time_range(start_timestamp, end_timestamp)

        if df.empty:
            return ClusteringResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®",
                clusters=[],
                algorithm_used=request.algorithm,
                parameters=request.params,
                statistics={}
            )

        # æ‰§è¡Œèšç±»åˆ†æ
        clusters, metrics = processor.perform_clustering_analysis(
            df,
            data_type=request.data_type,
            algorithm=request.algorithm,
            params=request.params
        )

        # è½¬æ¢æ•°æ®ç±»å‹
        clusters = convert_numpy_types(clusters)
        metrics = convert_numpy_types(metrics)

        return ClusteringResponse(
            success=True,
            message=f"èšç±»åˆ†æå®Œæˆï¼Œå‘ç°{len(clusters)}ä¸ªèšç±»",
            clusters=clusters,
            algorithm_used=request.algorithm,
            parameters=request.params,
            statistics=metrics
        )

    except Exception as e:
        logger.error(f"èšç±»åˆ†ææ—¶å‡ºé”™: {str(e)}")
        return ClusteringResponse(
            success=False,
            message=f"èšç±»åˆ†æå¤±è´¥: {str(e)}",
            clusters=[],
            algorithm_used=request.algorithm,
            parameters=request.params,
            statistics={}
        )


@router.post("/spatiotemporal/od-analysis", response_model=ODFlowResponse)
async def perform_od_analysis(
        start_time: str,
        end_time: str,
        request: ODAnalysisRequest
):
    """
    æ‰§è¡ŒODå¯¹åˆ†æ

    Args:
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        request: ODåˆ†æè¯·æ±‚å‚æ•°
    """
    try:
        logger.info(f"æ‰§è¡ŒODå¯¹åˆ†æ: æœ€å°è¡Œç¨‹æ—¶é—´ {request.min_trip_duration}ç§’")

        # æ—¶é—´è½¬æ¢
        start_timestamp = convert_time_to_timestamp(start_time)
        end_timestamp = convert_time_to_timestamp(end_time)

        # è·å–æ•°æ®
        processor = TrafficDataProcessor()
        df = processor.get_data_by_time_range(start_timestamp, end_timestamp)

        if df.empty:
            return ODFlowResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®",
                od_pairs=[],
                top_flows=[],
                statistics={}
            )

        # æå–ODå¯¹
        od_pairs = processor.extract_od_pairs_from_data(
            df,
            min_trip_duration=request.min_trip_duration,
            max_trip_duration=request.max_trip_duration,
            min_trip_distance=request.min_trip_distance
        )

        if not od_pairs:
            return ODFlowResponse(
                success=True,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ODå¯¹",
                od_pairs=[],
                top_flows=[],
                statistics={}
            )

        # ODæµé‡åˆ†æ
        from .od_analysis_engine import ODAnalysisEngine
        od_engine = ODAnalysisEngine()
        od_engine.od_pairs = od_pairs

        # åˆ†æé¡¶çº§æµé‡
        top_flows = od_engine.analyze_top_flows(od_pairs, top_k=20)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        statistics = od_engine.calculate_od_statistics(od_pairs)

        # ç”Ÿæˆæµé‡çŸ©é˜µï¼ˆå¦‚æœè¯·æ±‚èšåˆçº§åˆ«éœ€è¦ï¼‰
        flow_matrix = None
        if request.aggregate_level == "grid":
            matrix, grid_info = od_engine.generate_flow_matrix(od_pairs)
            if matrix.size > 0:
                flow_matrix = matrix.tolist()
                statistics['grid_info'] = grid_info

        # è½¬æ¢æ•°æ®ç±»å‹
        od_pairs = convert_numpy_types(od_pairs)
        top_flows = convert_numpy_types(top_flows)
        statistics = convert_numpy_types(statistics)

        return ODFlowResponse(
            success=True,
            message=f"ODåˆ†æå®Œæˆï¼Œæ‰¾åˆ°{len(od_pairs)}ä¸ªODå¯¹",
            od_pairs=od_pairs,
            flow_matrix=flow_matrix,
            top_flows=top_flows,
            statistics=statistics
        )

    except Exception as e:
        logger.error(f"ODåˆ†ææ—¶å‡ºé”™: {str(e)}")
        return ODFlowResponse(
            success=False,
            message=f"ODåˆ†æå¤±è´¥: {str(e)}",
            od_pairs=[],
            top_flows=[],
            statistics={}
        )


@router.post("/spatiotemporal/comprehensive", response_model=SpatioTemporalResponse)
async def perform_comprehensive_analysis(
        start_time: str,
        end_time: str,
        heatmap_request: HeatmapRequest
):
    """
    æ‰§è¡Œç»¼åˆæ—¶ç©ºåˆ†æ

    Args:
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        heatmap_request: çƒ­åŠ›å›¾åˆ†æå‚æ•°
    """
    try:
        logger.info(f"æ‰§è¡Œç»¼åˆæ—¶ç©ºåˆ†æ: {start_time} to {end_time}")

        # æ—¶é—´è½¬æ¢
        start_timestamp = convert_time_to_timestamp(start_time)
        end_timestamp = convert_time_to_timestamp(end_time)

        # è·å–æ•°æ®
        processor = TrafficDataProcessor()
        df = processor.get_data_by_time_range(start_timestamp, end_timestamp)

        if df.empty:
            return SpatioTemporalResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®",
                analysis_type="comprehensive",
                data=SpatioTemporalAnalysis(
                    analysis_type="comprehensive",
                    time_range={},
                    spatial_bounds={},
                    data=[],
                    statistics={},
                    algorithm_params={}
                )
            )

        # ç”Ÿæˆæ—¶ç©ºçƒ­åŠ›å›¾åˆ†æ
        analysis_result = processor.generate_spatiotemporal_heatmap(
            df,
            analysis_type="comprehensive",
            temporal_resolution=heatmap_request.temporal_resolution,
            spatial_resolution=heatmap_request.spatial_resolution
        )

        # æ„é€ å“åº”æ•°æ®
        spatiotemporal_data = SpatioTemporalAnalysis(
            analysis_type="comprehensive",
            time_range=analysis_result['time_range'],
            spatial_bounds=analysis_result['spatial_bounds'],
            data=analysis_result['heatmap_frames'],  # æ—¶é—´å¸§æ•°æ®
            statistics={
                'time_series_stats': analysis_result['time_series_stats'],
                'spatial_stats': analysis_result['spatial_stats']
            },
            algorithm_params=analysis_result['algorithm_params']
        )

        # è½¬æ¢æ•°æ®ç±»å‹
        spatiotemporal_data_dict = convert_numpy_types(spatiotemporal_data.dict())

        return SpatioTemporalResponse(
            success=True,
            message="ç»¼åˆæ—¶ç©ºåˆ†æå®Œæˆ",
            analysis_type="comprehensive",
            data=SpatioTemporalAnalysis(**spatiotemporal_data_dict),
            processing_time=None
        )

    except Exception as e:
        logger.error(f"ç»¼åˆæ—¶ç©ºåˆ†ææ—¶å‡ºé”™: {str(e)}")
        return SpatioTemporalResponse(
            success=False,
            message=f"ç»¼åˆæ—¶ç©ºåˆ†æå¤±è´¥: {str(e)}",
            analysis_type="comprehensive",
            data=SpatioTemporalAnalysis(
                analysis_type="comprehensive",
                time_range={},
                spatial_bounds={},
                data=[],
                statistics={},
                algorithm_params={}
            )
        )


@router.get("/spatiotemporal/algorithms")
async def get_available_algorithms():
    """è·å–å¯ç”¨çš„èšç±»ç®—æ³•åˆ—è¡¨"""
    try:
        from .clustering_engine import ClusteringEngine

        clustering_engine = ClusteringEngine()
        algorithms = clustering_engine.get_available_algorithms()

        algorithm_info = {}
        for algo in algorithms:
            algorithm_info[algo] = clustering_engine.get_algorithm_params(algo)

        return {
            "success": True,
            "message": "è·å–ç®—æ³•åˆ—è¡¨æˆåŠŸ",
            "algorithms": algorithms,
            "algorithm_params": algorithm_info
        }

    except Exception as e:
        logger.error(f"è·å–ç®—æ³•åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
        return {
            "success": False,
            "message": f"è·å–ç®—æ³•åˆ—è¡¨å¤±è´¥: {str(e)}",
            "algorithms": [],
            "algorithm_params": {}
        }


# è·¯æ®µåˆ†æç›¸å…³APIæ¥å£

@router.post("/road/analysis", response_model=RoadAnalysisResponse)
async def analyze_road_segments(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        request: RoadAnalysisRequest = Body(...)
):
    """
    è·¯æ®µåˆ†æAPI
    åˆ†æé“è·¯ç½‘ç»œçš„é€šè¡ŒçŠ¶å†µã€é€Ÿåº¦åˆ†å¸ƒã€æ‹¥å µæƒ…å†µç­‰
    æ”¯æŒç¼“å­˜ç³»ç»Ÿï¼Œé¿å…é‡å¤è®¡ç®—
    """
    import math
    import pandas as pd
    import os
    try:
        start_processing = time.time()

        # è·å–ç¼“å­˜ç³»ç»Ÿ
        cache = get_traffic_cache()

        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = cache.generate_cache_key(
            start_time=start_time,
            end_time=end_time,
            analysis_type=request.analysis_type,
            min_vehicles=getattr(request, 'min_vehicles', 10)
        )

        # å°è¯•ä»ç¼“å­˜åŠ è½½ç»“æœ
        logger.info("è·¯æ®µåˆ†æ", f"æ£€æŸ¥ç¼“å­˜: {cache_key}")
        cached_result = cache.load_analysis_result(cache_key)

        if cached_result:
            logger.info("è·¯æ®µåˆ†æ", "ä½¿ç”¨ç¼“å­˜çš„åˆ†æç»“æœï¼ˆç§’çº§å“åº”ï¼‰")

            # æ„é€ åˆ†æç»“æœå¯¹è±¡
            analysis = RoadNetworkAnalysis(
                analysis_type=cached_result.get("analysis_metadata", {}).get("analysis_type", request.analysis_type),
                time_range=cached_result.get("analysis_metadata", {}).get("time_range",
                                                                          {"start": start_time, "end": end_time}),
                total_segments=cached_result.get("total_segments", 0),
                segments_data=cached_result.get("segments_data", []),
                network_summary=cached_result.get("network_summary", {}),
                bottleneck_segments=cached_result.get("bottleneck_segments", [])
            )

            return RoadAnalysisResponse(
                success=True,
                message="ä»ç¼“å­˜åŠ è½½åˆ†æç»“æœ",
                analysis=analysis,
                segments=cached_result.get("segments", []),  # æ·»åŠ segmentså­—æ®µ
                speed_distributions=cached_result.get("speed_distributions", []),
                flow_patterns=cached_result.get("flow_patterns", []),
                processing_time=time.time() - start_processing
            )

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå®æ—¶åˆ†æ
        logger.info("è·¯æ®µåˆ†æ", "ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå®æ—¶åˆ†æ...")
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_time, end_time)

        if df.empty:
            return RoadAnalysisResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰äº¤é€šæ•°æ®",
                analysis=RoadNetworkAnalysis(
                    analysis_type=request.analysis_type,
                    time_range={"start": start_time, "end": end_time},
                    total_segments=0,
                    segments_data=[],
                    network_summary={},
                    bottleneck_segments=[]
                ),
                speed_distributions=[],
                flow_patterns=[],
                processing_time=0
            )

        # æ‰§è¡Œè·¯æ®µåˆ†æ
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type=request.analysis_type,
            min_vehicles=getattr(request, 'min_vehicles', 10)
        )

        if "error" in analysis_result:
            return RoadAnalysisResponse(
                success=False,
                message=analysis_result["error"],
                analysis=RoadNetworkAnalysis(
                    analysis_type=request.analysis_type,
                    time_range={"start": start_time, "end": end_time},
                    total_segments=0,
                    segments_data=[],
                    network_summary={},
                    bottleneck_segments=[]
                ),
                speed_distributions=[],
                flow_patterns=[],
                processing_time=time.time() - start_processing
            )

        # ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜
        metadata = {
            "start_time": start_time,
            "end_time": end_time,
            "analysis_type": request.analysis_type,
            "min_vehicles": getattr(request, 'min_vehicles', 10),
            "request_time": datetime.now().isoformat(),
            "data_records": len(df)
        }

        cache_saved = cache.save_analysis_result(cache_key, analysis_result, metadata)
        if cache_saved:
            logger.info("è·¯æ®µåˆ†æ", f"åˆ†æç»“æœå·²ä¿å­˜åˆ°ç¼“å­˜: {cache_key}")
        else:
            logger.warning("è·¯æ®µåˆ†æ", "ç¼“å­˜ä¿å­˜å¤±è´¥")

        # æ„é€ åˆ†æç»“æœå¯¹è±¡
        analysis = RoadNetworkAnalysis(
            analysis_type=request.analysis_type,
            time_range={"start": start_time, "end": end_time},
            total_segments=analysis_result.get("total_segments", 0),
            segments_data=analysis_result.get("segments_data", []),
            network_summary=analysis_result.get("network_summary", {}),
            bottleneck_segments=analysis_result.get("bottleneck_segments", [])
        )

        processing_time = time.time() - start_processing
        message = f"å®æ—¶åˆ†æå®Œæˆï¼Œè€—æ—¶ {processing_time:.2f} ç§’"
        if cache_saved:
            message += "ï¼Œç»“æœå·²ç¼“å­˜"

        # æ–°å¢ï¼šè¯¦ç»†æ—¥å¿—è®°å½•
        try:
            avg_speed = None
            avg_flow = None
            if analysis_result.get("segments_data"):
                speeds = [seg.get("avg_speed", 0) for seg in analysis_result["segments_data"] if
                          seg.get("avg_speed") is not None]
                flows = [seg.get("flow_rate", 0) for seg in analysis_result["segments_data"] if
                         seg.get("flow_rate") is not None]
                if speeds:
                    avg_speed = sum(speeds) / len(speeds)
                if flows:
                    avg_flow = sum(flows) / len(flows)
            logger.traffic(
                "è·¯æ®µåˆ†æ",  # module
                f"åˆ†æå®Œæˆ: {message}",  # message
                source="APIè°ƒç”¨",
                user="ç”¨æˆ·",
                details={
                    "start_time": start_time,
                    "end_time": end_time,
                    "analysis_type": request.analysis_type,
                    "total_segments": analysis_result.get("total_segments", 0),
                    "bottleneck_count": len(analysis_result.get("bottleneck_segments", [])),
                    "avg_speed": avg_speed,
                    "avg_flow": avg_flow,
                    "processing_time": processing_time
                }
            )
        except Exception as log_err:
            logger.error("è·¯æ®µåˆ†æ", f"æ—¥å¿—è®°å½•å¤±è´¥: {log_err}")

        return RoadAnalysisResponse(
            success=True,
            message=message,
            analysis=analysis,
            segments=analysis_result.get("segments", []),  # æ·»åŠ segmentså­—æ®µ
            speed_distributions=analysis_result.get("speed_distributions", []),
            flow_patterns=analysis_result.get("flow_patterns", []),
            processing_time=processing_time
        )

    except Exception as e:
        logger.error("è·¯æ®µåˆ†æ", f"è·¯æ®µåˆ†ææ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.error("è·¯æ®µåˆ†æ", f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

        return RoadAnalysisResponse(
            success=False,
            message=f"è·¯æ®µåˆ†ææ—¶å‡ºé”™: {str(e)}",
            analysis=RoadNetworkAnalysis(
                analysis_type=request.analysis_type,
                time_range={"start": start_time, "end": end_time},
                total_segments=0,
                segments_data=[],
                network_summary={},
                bottleneck_segments=[]
            ),
            speed_distributions=[],
            flow_patterns=[],
            processing_time=time.time() - start_processing
        )


@router.get("/road/segments", response_model=RoadSegmentResponse)
async def get_road_segments():
    """
    è·å–è·¯æ®µä¿¡æ¯API
    è¿”å›å½“å‰ç³»ç»Ÿè¯†åˆ«çš„æ‰€æœ‰è·¯æ®µåŸºç¡€ä¿¡æ¯
    """
    try:
        # è·å–æœ€è¿‘çš„æ•°æ®æ¥æå–è·¯æ®µ
        current_time = time.time()
        start_timestamp = current_time - 3600  # 1å°æ—¶å‰
        end_timestamp = current_time

        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)

        if df.empty:
            return RoadSegmentResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„æ•°æ®æ¥æå–è·¯æ®µä¿¡æ¯",
                segments=[],
                total_segments=0
            )

        # æ‰§è¡Œç®€å•çš„è·¯æ®µåˆ†ææ¥è·å–è·¯æ®µä¿¡æ¯
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type="comprehensive",
            min_vehicles=1  # é™ä½é˜ˆå€¼ä»¥è·å–æ›´å¤šè·¯æ®µ
        )

        if "error" in analysis_result:
            return RoadSegmentResponse(
                success=False,
                message=analysis_result["error"],
                segments=[],
                total_segments=0
            )

        # è½¬æ¢è·¯æ®µæ•°æ®
        segments_data = analysis_result.get("segments", [])
        segments = []
        for segment_data in segments_data:
            segment = RoadSegment(**segment_data)
            segments.append(segment)

        return RoadSegmentResponse(
            success=True,
            message=f"æˆåŠŸè·å– {len(segments)} ä¸ªè·¯æ®µä¿¡æ¯",
            segments=segments,
            total_segments=len(segments)
        )

    except Exception as e:
        logger.error(f"è·å–è·¯æ®µä¿¡æ¯APIé”™è¯¯: {str(e)}")
        return RoadSegmentResponse(
            success=False,
            message=f"è·å–å¤±è´¥: {str(e)}",
            segments=[],
            total_segments=0
        )


@router.post("/road/traffic", response_model=RoadTrafficResponse)
async def get_road_traffic_data(time_range: Dict[str, float]):
    """
    è·å–è·¯æ®µäº¤é€šæ•°æ®API
    è¿”å›æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„è·¯æ®µäº¤é€šçŠ¶å†µæ•°æ®
    """
    try:
        start_timestamp = time_range.get("start", time.time() - 3600)
        end_timestamp = time_range.get("end", time.time())

        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)

        if df.empty:
            return RoadTrafficResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰äº¤é€šæ•°æ®",
                traffic_data=[],
                statistics={}
            )

        # æ‰§è¡Œè·¯æ®µåˆ†æ
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type="comprehensive",
            min_vehicles=5
        )

        if "error" in analysis_result:
            return RoadTrafficResponse(
                success=False,
                message=analysis_result["error"],
                traffic_data=[],
                statistics={}
            )

        # è½¬æ¢äº¤é€šæ•°æ®
        traffic_data_list = analysis_result.get("traffic_data", [])
        traffic_data = []
        for data in traffic_data_list:
            traffic_item = RoadTrafficData(**data)
            traffic_data.append(traffic_item)

        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        statistics = {
            "total_records": len(traffic_data),
            "time_range": {"start": start_timestamp, "end": end_timestamp},
            "avg_speed": np.mean([t.avg_speed for t in traffic_data]) if traffic_data else 0,
            "avg_flow": np.mean([t.flow_rate for t in traffic_data]) if traffic_data else 0,
            "congestion_summary": {}
        }

        # æ‹¥å µåˆ†å¸ƒç»Ÿè®¡
        congestion_counts = {}
        for t in traffic_data:
            level = t.congestion_level
            congestion_counts[level] = congestion_counts.get(level, 0) + 1
        statistics["congestion_summary"] = congestion_counts

        return RoadTrafficResponse(
            success=True,
            message=f"æˆåŠŸè·å– {len(traffic_data)} æ¡äº¤é€šæ•°æ®è®°å½•",
            traffic_data=traffic_data,
            statistics=statistics
        )

    except Exception as e:
        logger.error(f"è·å–äº¤é€šæ•°æ®APIé”™è¯¯: {str(e)}")
        return RoadTrafficResponse(
            success=False,
            message=f"è·å–å¤±è´¥: {str(e)}",
            traffic_data=[],
            statistics={}
        )


@router.post("/road/visualization", response_model=RoadVisualizationResponse)
async def get_road_visualization_data(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        request: Dict[str, Any] = Body(...)
):
    """
    è·¯æ®µå¯è§†åŒ–API
    ç”Ÿæˆè·¯æ®µå¯è§†åŒ–æ•°æ®
    """
    import math
    import pandas as pd
    import os
    try:
        start_processing = time.time()
        # åªèµ°å®æ—¶åˆ†æé€»è¾‘
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_time, end_time)
        if df.empty:
            return RoadVisualizationResponse(
                success=False,
                message="æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰äº¤é€šæ•°æ®",
                visualization_data=[],
                segment_colors={},
                legend_info={}
            )
        # æ‰§è¡Œè·¯æ®µåˆ†æï¼Œç”Ÿæˆå¯è§†åŒ–æ•°æ®
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type=getattr(request, 'analysis_type', 'comprehensive'),
            min_vehicles=getattr(request, 'min_vehicles', 10)
        )
        if "error" in analysis_result:
            return RoadVisualizationResponse(
                success=False,
                message=analysis_result["error"],
                visualization_data=[],
                segment_colors={},
                legend_info={}
            )
        visualization_data = analysis_result.get("segments", [])
        return RoadVisualizationResponse(
            success=True,
            message="å®æ—¶åˆ†æå®Œæˆ",
            visualization_data=visualization_data,
            segment_colors=analysis_result.get("segment_colors", {}),
            legend_info=analysis_result.get("legend_info", {})
        )
    except Exception as e:
        logger.error(f"ç”Ÿæˆè·¯æ®µå¯è§†åŒ–æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return RoadVisualizationResponse(
            success=False,
            message=f"ç”Ÿæˆå¯è§†åŒ–æ•°æ®æ—¶å‡ºé”™: {str(e)}",
            visualization_data=[],
            segment_colors={},
            legend_info={}
        )


@router.get("/road/metrics", response_model=Dict[str, Any])
async def get_road_network_metrics():
    """
    è·å–è·¯ç½‘æ•´ä½“æŒ‡æ ‡API
    è¿”å›é“è·¯ç½‘ç»œçš„ç»¼åˆæ€§èƒ½æŒ‡æ ‡
    """
    try:
        # ä½¿ç”¨å½“å‰æ—¶é—´èŒƒå›´ï¼ˆæ¼”ç¤ºç”¨ï¼‰
        current_time = time.time()
        start_timestamp = current_time - 24 * 3600  # 24å°æ—¶å‰
        end_timestamp = current_time

        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)

        if df.empty:
            return {
                "success": False,
                "message": "æ²¡æœ‰å¯ç”¨çš„äº¤é€šæ•°æ®",
                "metrics": {}
            }

        # æ‰§è¡Œè·¯æ®µåˆ†æè·å–åŸºç¡€æ•°æ®
        analysis_result = data_processor.analyze_road_segments(
            df,
            analysis_type="comprehensive",
            min_vehicles=3
        )

        if "error" in analysis_result:
            return {
                "success": False,
                "message": analysis_result["error"],
                "metrics": {}
            }

        # è®¡ç®—ç½‘ç»œæŒ‡æ ‡
        segments_data = analysis_result.get("segments", [])
        traffic_data = analysis_result.get("traffic_data", [])

        metrics = data_processor.calculate_road_network_metrics(segments_data, traffic_data)

        return {
            "success": True,
            "metrics": metrics,
            "timestamp": time.time()
        }

    except Exception as e:
        logger.error(f"è·å–è·¯ç½‘æŒ‡æ ‡å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"è·å–è·¯ç½‘æŒ‡æ ‡å¤±è´¥: {str(e)}",
            "metrics": {}
        }


@router.get("/weekly-passenger-flow", response_model=Dict[str, Any])
async def get_weekly_passenger_flow_analysis(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰")
):
    """
    è·å–å‘¨å®¢æµé‡åˆ†ææ•°æ®

    Args:
        start_time: å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰
        end_time: ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰

    Returns:
        å‘¨å®¢æµé‡åˆ†æç»“æœï¼ŒåŒ…æ‹¬æ—¥æµé‡å¯¹æ¯”ã€å·¥ä½œæ—¥vså‘¨æœ«ã€é«˜å³°æ—¶æ®µç­‰
    """
    try:
        logger.info(f"å¼€å§‹å‘¨å®¢æµé‡åˆ†æ: {start_time} - {end_time}")

        # éªŒè¯æ—¶é—´èŒƒå›´ï¼ˆç¡®ä¿è‡³å°‘æœ‰3å¤©æ•°æ®ï¼‰
        time_span = end_time - start_time
        if time_span < 3 * 24 * 3600:  # å°‘äº3å¤©
            return {
                "success": False,
                "message": "åˆ†æå‘¨å®¢æµé‡éœ€è¦è‡³å°‘3å¤©çš„æ•°æ®",
                "data": {}
            }

        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_time, end_time)

        if df.empty:
            return {
                "success": False,
                "message": "æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰äº¤é€šæ•°æ®",
                "data": {}
            }

        # æ‰§è¡Œå‘¨å®¢æµé‡åˆ†æ
        analysis_result = data_processor.analyze_weekly_passenger_flow(df)

        if not analysis_result.get('success', False):
            return {
                "success": False,
                "message": analysis_result.get('message', 'åˆ†æå¤±è´¥'),
                "data": {}
            }

        # æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
        analysis_result['metadata'] = {
            'query_start_time': start_time,
            'query_end_time': end_time,
            'analysis_timestamp': time.time(),
            'data_quality': {
                'total_records': len(df),
                'unique_vehicles': df['COMMADDR'].nunique() if 'COMMADDR' in df.columns else 0,
                'time_coverage': analysis_result.get('analysis_period', {})
            }
        }

        logger.info(f"å‘¨å®¢æµé‡åˆ†æå®Œæˆï¼Œæ•°æ®è´¨é‡: {analysis_result['metadata']['data_quality']}")

        return {
            "success": True,
            "message": "å‘¨å®¢æµé‡åˆ†æå®Œæˆ",
            "data": analysis_result
        }

    except Exception as e:
        logger.error(f"å‘¨å®¢æµé‡åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())

        return {
            "success": False,
            "message": f"åˆ†æå¤±è´¥: {str(e)}",
            "data": {}
        }


# æ™ºèƒ½å®¢è¿ç›‘æ§ç›¸å…³APIæ¥å£

@router.post("/smart-passenger/analysis", response_model=SmartPassengerResponse)
async def analyze_smart_passenger(request: SmartPassengerRequest):
    """
    æ™ºèƒ½å®¢è¿åˆ†æAPI
    åˆ†æå¤©æ°”å¯¹å®¢æµçš„å½±å“ï¼Œè½½å®¢å‡ºç§Ÿè½¦éœ€æ±‚ç­‰
    """
    try:
        start_time = time.time()

        # è·å–å½“å‰æ—¶é—´ä½œä¸ºæ—¶é—´èŒƒå›´ï¼ˆæ¼”ç¤ºç”¨ï¼‰
        current_time = time.time()
        start_timestamp = current_time - 24 * 3600  # 24å°æ—¶å‰
        end_timestamp = current_time

        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)

        if df.empty:
            logger.warning("æ™ºèƒ½ä¹˜å®¢åˆ†æ", "æ²¡æœ‰å¯ç”¨çš„äº¤é€šæ•°æ®", source="APIè°ƒç”¨", user="ç”¨æˆ·")
            return SmartPassengerResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„äº¤é€šæ•°æ®",
                analysis_type=request.analysis_type,
                statistics=None,
                processing_time=time.time() - start_time
            )

        # åˆå§‹åŒ–æ™ºèƒ½å®¢è¿åˆ†æå¼•æ“
        from .smart_passenger_engine import SmartPassengerEngine
        smart_engine = SmartPassengerEngine()

        # è·å–å¤©æ°”æ•°æ®
        weather_data = []
        if request.include_weather:
            weather_data = smart_engine.get_weather_data(start_timestamp, end_timestamp)

        # è¯†åˆ«è½½å®¢è½¦è¾†å’Œå®¢æµæ•°æ®
        passenger_flows = smart_engine.identify_passenger_vehicles(df)

        # åˆ†æå‡ºç§Ÿè½¦éœ€æ±‚
        taxi_demand_data = []
        if request.include_taxi_analysis:
            taxi_demand_data = smart_engine.analyze_taxi_demand(df, request.time_resolution)

        # åˆ†æå¤©æ°”å½±å“
        weather_impact = []
        if request.weather_correlation and weather_data:
            weather_impact = smart_engine.analyze_weather_impact(passenger_flows, weather_data)

        # è®¡ç®—å‡ºç§Ÿè½¦ä¾›éœ€
        taxi_supply_demand = []
        if taxi_demand_data:
            taxi_supply_demand = smart_engine.calculate_taxi_supply_demand(taxi_demand_data)

        # ç”Ÿæˆç»Ÿè®¡æ•°æ®
        statistics = smart_engine.generate_smart_passenger_statistics(
            passenger_flows, weather_data, taxi_demand_data, weather_impact,
            (start_timestamp, end_timestamp)
        )

        return SmartPassengerResponse(
            success=True,
            message=f"æ™ºèƒ½å®¢è¿ç›‘æ§åˆ†æå®Œæˆï¼Œåˆ†æäº† {len(passenger_flows)} æ¡å®¢æµæ•°æ®",
            analysis_type=request.analysis_type,
            statistics=statistics,
            weather_impact=weather_impact if weather_impact else None,
            taxi_demand=taxi_supply_demand if taxi_supply_demand else None,
            processing_time=time.time() - start_time
        )

    except Exception as e:
        logger.error(f"æ™ºèƒ½å®¢è¿ç›‘æ§åˆ†æå¤±è´¥: {str(e)}")
        return SmartPassengerResponse(
            success=False,
            message=f"åˆ†æå¤±è´¥: {str(e)}",
            analysis_type=request.analysis_type,
            statistics=None,
            processing_time=time.time() - start_time if 'start_time' in locals() else 0
        )


@router.post("/smart-passenger/weather-impact", response_model=WeatherImpactResponse)
async def analyze_weather_impact(request: WeatherImpactRequest):
    """
    å¤©æ°”å½±å“åˆ†æAPI
    ä¸“é—¨åˆ†æå¤©æ°”å˜åŒ–å¯¹å®¢æµé‡çš„å½±å“
    """
    try:
        # è·å–æ—¶é—´èŒƒå›´
        current_time = time.time()
        # ä¼˜å…ˆä½¿ç”¨è¯·æ±‚ä¸­çš„æ—¶é—´èŒƒå›´ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        if request.start_time and request.end_time:
            start_timestamp = request.start_time
            end_timestamp = request.end_time
        else:
            # é»˜è®¤ä½¿ç”¨2013å¹´9æœˆ12-18æ—¥çš„æ•°æ®èŒƒå›´
            start_timestamp = 1378944000  # 2013-09-12 00:00:00 UTC
            end_timestamp = 1379548800  # 2013-09-18 23:59:59 UTC

        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        logger.info(f"å¼€å§‹åŠ è½½äº¤é€šæ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_timestamp} - {end_timestamp}")
        df = data_processor.load_data(start_timestamp, end_timestamp)
        logger.info(f"äº¤é€šæ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")

        if df.empty:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°äº¤é€šæ•°æ®")
            return WeatherImpactResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„äº¤é€šæ•°æ®",
                weather_impact_analysis=[],
                correlation_matrix={},
                weather_stats={}
            )

        # åˆå§‹åŒ–æ™ºèƒ½å®¢è¿åˆ†æå¼•æ“
        from .smart_passenger_engine import SmartPassengerEngine
        smart_engine = SmartPassengerEngine()

        # è·å–å¤©æ°”æ•°æ®
        weather_data = smart_engine.get_weather_data(start_timestamp, end_timestamp)

        # è¯†åˆ«å®¢æµæ•°æ®
        passenger_flows = smart_engine.identify_passenger_vehicles(df)

        # åˆ†æå¤©æ°”å½±å“
        weather_impact_analysis = smart_engine.analyze_weather_impact(passenger_flows, weather_data)

        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix = {}
        for impact in weather_impact_analysis:
            correlation_matrix[impact.weather_condition] = impact.correlation_coefficient

        # ç”Ÿæˆå¤©æ°”ç»Ÿè®¡
        weather_stats = {
            "total_weather_records": len(weather_data),
            "weather_type_distribution": {},
            "avg_temperature": np.mean([w.temperature for w in weather_data]) if weather_data else 0,
            "avg_precipitation": np.mean([w.precipitation for w in weather_data]) if weather_data else 0
        }

        # ç»Ÿè®¡å¤©æ°”ç±»å‹åˆ†å¸ƒ
        for weather in weather_data:
            weather_type = weather.weather_type
            weather_stats["weather_type_distribution"][weather_type] = \
                weather_stats["weather_type_distribution"].get(weather_type, 0) + 1

        return WeatherImpactResponse(
            success=True,
            message=f"å¤©æ°”å½±å“åˆ†æå®Œæˆï¼Œåˆ†æäº† {len(weather_data)} æ¡å¤©æ°”æ•°æ®",
            weather_impact_analysis=weather_impact_analysis,
            correlation_matrix=correlation_matrix,
            weather_stats=weather_stats,
            prediction_data=None  # æš‚ä¸å®ç°é¢„æµ‹åŠŸèƒ½
        )

    except Exception as e:
        logger.error(f"å¤©æ°”å½±å“åˆ†æå¤±è´¥: {str(e)}")
        return WeatherImpactResponse(
            success=False,
            message=f"åˆ†æå¤±è´¥: {str(e)}",
            weather_impact_analysis=[],
            correlation_matrix={},
            weather_stats={}
        )


# å·²åˆ é™¤æ—§çš„å‡ºç§Ÿè½¦éœ€æ±‚åˆ†æAPIï¼Œå› ä¸ºä½¿ç”¨äº†ä¼°ç®—é€»è¾‘
# æ–°çš„è½½å®¢è½¦è¾†æ—¶é—´çº¿APIä½¿ç”¨çœŸå®çš„è½½å®¢è½¦è¾†æ•°æ®

@router.post("/smart-passenger/visualization", response_model=PassengerVisualizationResponse)
async def get_passenger_visualization_data(request: Dict[str, Any]):
    """
    å®¢è¿å¯è§†åŒ–æ•°æ®API
    ç”Ÿæˆå®¢æµçƒ­åŠ›å›¾ã€å¤©æ°”å…³è”å›¾è¡¨ã€å‡ºç§Ÿè½¦éœ€æ±‚åœ°å›¾ç­‰å¯è§†åŒ–æ•°æ®
    """
    try:
        visualization_type = request.get("visualization_type", "comprehensive")
        time_range = request.get("time_range", {})

        start_timestamp = time_range.get("start", time.time() - 3600)
        end_timestamp = time_range.get("end", time.time())

        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)

        if df.empty:
            return PassengerVisualizationResponse(
                success=False,
                message="æ²¡æœ‰å¯ç”¨çš„æ•°æ®è¿›è¡Œå¯è§†åŒ–",
                passenger_heatmap={},
                weather_correlation_chart={},
                taxi_demand_map={},
                time_series_data={}
            )

        # åˆå§‹åŒ–æ™ºèƒ½å®¢è¿åˆ†æå¼•æ“
        from .smart_passenger_engine import SmartPassengerEngine
        smart_engine = SmartPassengerEngine()

        # è·å–åŸºç¡€æ•°æ®
        weather_data = smart_engine.get_weather_data(start_timestamp, end_timestamp)
        passenger_flows = smart_engine.identify_passenger_vehicles(df)
        taxi_demand_data = smart_engine.analyze_taxi_demand(df)

        # ç”Ÿæˆå®¢æµçƒ­åŠ›å›¾æ•°æ®
        passenger_heatmap = {
            "heatmap_points": [
                {
                    "lat": pf.location["lat"],
                    "lng": pf.location["lng"],
                    "intensity": pf.passenger_count,
                    "type": "pickup" if pf.is_pickup else "dropoff"
                }
                for pf in passenger_flows
            ],
            "legend": {
                "pickup": "ä¸Šå®¢ç‚¹",
                "dropoff": "ä¸‹å®¢ç‚¹"
            }
        }

        # ç”Ÿæˆå¤©æ°”å…³è”å›¾è¡¨æ•°æ®
        weather_correlation_chart = {
            "weather_passenger_correlation": [],
            "time_series": []
        }

        if weather_data and passenger_flows:
            weather_impact = smart_engine.analyze_weather_impact(passenger_flows, weather_data)
            for impact in weather_impact:
                weather_correlation_chart["weather_passenger_correlation"].append({
                    "weather_type": impact.weather_condition,
                    "impact_percentage": impact.impact_percentage,
                    "correlation": impact.correlation_coefficient
                })

        # ç”Ÿæˆå‡ºç§Ÿè½¦éœ€æ±‚åœ°å›¾æ•°æ®
        taxi_demand_map = {
            "demand_zones": [
                {
                    "lat": td.location["lat"],
                    "lng": td.location["lng"],
                    "loaded_taxis": td.loaded_taxis,
                    "demand_index": td.demand_index,
                    "supply_ratio": td.supply_ratio
                }
                for td in taxi_demand_data
            ],
            "supply_demand_legend": {
                "high_demand": {"color": "#ff4444", "threshold": 0.7},
                "medium_demand": {"color": "#ffaa44", "threshold": 0.4},
                "low_demand": {"color": "#44ff44", "threshold": 0.0}
            }
        }

        # ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
        time_series_data = {
            "passenger_flow_trend": [],
            "weather_trend": [],
            "taxi_demand_trend": []
        }

        # æŒ‰å°æ—¶èšåˆæ—¶é—´åºåˆ—
        hourly_passenger = defaultdict(int)
        hourly_weather = defaultdict(list)
        hourly_taxi = defaultdict(list)

        for pf in passenger_flows:
            hour = datetime.fromtimestamp(pf.timestamp).strftime("%Y-%m-%d %H:00")
            hourly_passenger[hour] += pf.passenger_count

        for w in weather_data:
            hour = datetime.fromtimestamp(w.timestamp).strftime("%Y-%m-%d %H:00")
            hourly_weather[hour].append(w)

        for td in taxi_demand_data:
            hour = datetime.fromtimestamp(td.timestamp).strftime("%Y-%m-%d %H:00")
            hourly_taxi[hour].append(td)

        for hour in sorted(set(list(hourly_passenger.keys()) + list(hourly_weather.keys()))):
            time_series_data["passenger_flow_trend"].append({
                "time": hour,
                "passenger_count": hourly_passenger.get(hour, 0)
            })

            if hour in hourly_weather:
                avg_temp = np.mean([w.temperature for w in hourly_weather[hour]])
                avg_precip = np.mean([w.precipitation for w in hourly_weather[hour]])
                time_series_data["weather_trend"].append({
                    "time": hour,
                    "temperature": avg_temp,
                    "precipitation": avg_precip
                })

            if hour in hourly_taxi:
                avg_demand = np.mean([td.demand_index for td in hourly_taxi[hour]])
                total_loaded = sum(td.loaded_taxis for td in hourly_taxi[hour])
                time_series_data["taxi_demand_trend"].append({
                    "time": hour,
                    "demand_index": avg_demand,
                    "loaded_taxis": total_loaded
                })

        return PassengerVisualizationResponse(
            success=True,
            message=f"æˆåŠŸç”Ÿæˆå¯è§†åŒ–æ•°æ®ï¼ŒåŒ…å« {len(passenger_flows)} ä¸ªå®¢æµç‚¹",
            passenger_heatmap=passenger_heatmap,
            weather_correlation_chart=weather_correlation_chart,
            taxi_demand_map=taxi_demand_map,
            time_series_data=time_series_data
        )

    except Exception as e:
        logger.error(f"å®¢è¿å¯è§†åŒ–æ•°æ®ç”Ÿæˆå¤±è´¥: {str(e)}")
        return PassengerVisualizationResponse(
            success=False,
            message=f"ç”Ÿæˆå¯è§†åŒ–æ•°æ®å¤±è´¥: {str(e)}",
            passenger_heatmap={},
            weather_correlation_chart={},
            taxi_demand_map={},
            time_series_data={}
        )


@router.get("/smart-passenger/historical", response_model=Dict[str, Any])
async def get_historical_passenger_analysis():
    """
    å†å²å®¢è¿åˆ†æAPI
    è·å–å†å²æ—¶æ®µçš„å®¢æµå’Œè½½å®¢è½¦è¾†çŠ¶æ€
    """
    try:
        # ä½¿ç”¨2013å¹´9æœˆ12-18æ—¥çš„æ•°æ®èŒƒå›´
        start_timestamp = 1378944000  # 2013-09-12 00:00:00 UTC
        end_timestamp = 1379548800  # 2013-09-18 23:59:59 UTC
        current_time = time.time()  # å½“å‰æ—¶é—´ç”¨äºçŠ¶æ€è®¡ç®—

        # åŠ è½½å†å²æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)

        if df.empty:
            return {
                "success": False,
                "message": "æ²¡æœ‰å†å²æ•°æ®",
                "historical_data": {}
            }

        # åˆå§‹åŒ–æ™ºèƒ½å®¢è¿åˆ†æå¼•æ“
        from .smart_passenger_engine import SmartPassengerEngine
        smart_engine = SmartPassengerEngine()

        # è·å–å†å²æ•°æ®
        passenger_flows = smart_engine.identify_passenger_vehicles(df)
        taxi_demand_data = smart_engine.analyze_taxi_demand(df, 5)  # 5åˆ†é’Ÿåˆ†è¾¨ç‡

        # è®¡ç®—å†å²æŒ‡æ ‡
        historical_data = {
            "current_timestamp": current_time,
            "time_window": "15min",
            "passenger_stats": {
                "active_passengers": sum(pf.passenger_count for pf in passenger_flows),
                "pickup_points": len([pf for pf in passenger_flows if pf.is_pickup]),
                "dropoff_points": len([pf for pf in passenger_flows if not pf.is_pickup])
            },
            "taxi_stats": {
                "loaded_taxis": sum(td.loaded_taxis for td in taxi_demand_data),
                "empty_taxis": sum(td.empty_taxis for td in taxi_demand_data),
                "total_demand": sum(td.total_orders for td in taxi_demand_data),
                "avg_demand_index": np.mean([td.demand_index for td in taxi_demand_data]) if taxi_demand_data else 0
            },
            "status_indicators": {
                "demand_level": "normal",
                "supply_status": "adequate",
                "traffic_flow": "smooth"
            }
        }

        # åˆ¤æ–­çŠ¶æ€æŒ‡æ ‡
        if historical_data["taxi_stats"]["avg_demand_index"] > 0.7:
            historical_data["status_indicators"]["demand_level"] = "high"
        elif historical_data["taxi_stats"]["avg_demand_index"] < 0.3:
            historical_data["status_indicators"]["demand_level"] = "low"

        supply_ratio = (historical_data["taxi_stats"]["loaded_taxis"] /
                        max(historical_data["taxi_stats"]["total_demand"], 1))
        if supply_ratio < 0.5:
            historical_data["status_indicators"]["supply_status"] = "shortage"
        elif supply_ratio > 1.5:
            historical_data["status_indicators"]["supply_status"] = "surplus"

        return {
            "success": True,
            "message": "å†å²åˆ†ææ•°æ®è·å–æˆåŠŸ",
            "historical_data": historical_data
        }

    except Exception as e:
        logger.error(f"å†å²å®¢è¿åˆ†æå¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"å†å²åˆ†æå¤±è´¥: {str(e)}",
            "historical_data": {}
        }


# ===== è·¯ç¨‹åˆ†æå’Œè®¢å•é€Ÿåº¦åˆ†æAPIæ¥å£ =====

@router.post("/road/trip-analysis", response_model=TripAnalysisResponse)
async def analyze_trip_distance_classification(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        request: TripAnalysisRequest = None
):
    """
    è·¯ç¨‹åˆ†æåŠŸèƒ½
    è§„å®šä»¥å°äº4åƒç±³ä¸ºçŸ­é€”ï¼Œ4åƒç±³è‡³8åƒç±³ä¸ºä¸­é€”ï¼Œå¤§äº8åƒç±³ä¸ºé•¿é€”
    æŸ¥çœ‹æ¯å¤©ä¸‰ç§è·ç¦»è¿è¾“çš„å æ¯”ï¼Œå®Œæˆè·¯ç¨‹åˆ†æçš„å¯è§†åŒ–å±•ç¤º
    """
    try:
        start_processing = time.time()

        # é»˜è®¤è¯·æ±‚å‚æ•°
        if request is None:
            request = TripAnalysisRequest()

        # æ ¹æ®é€‰æ‹©çš„æ—¥æœŸè°ƒæ•´æ—¶é—´èŒƒå›´
        if request.selected_date and request.selected_date != 'all':
            # å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³
            from datetime import datetime
            date_obj = datetime.strptime(request.selected_date, '%Y-%m-%d')
            # è®¾ç½®ä¸ºè¯¥æ—¥æœŸçš„å¼€å§‹å’Œç»“æŸæ—¶é—´ï¼ˆUTCï¼‰
            day_start = int(date_obj.timestamp())
            day_end = day_start + 86400 - 1  # 86400ç§’ = 24å°æ—¶

            actual_start_time = day_start
            actual_end_time = day_end
        else:
            actual_start_time = start_time
            actual_end_time = end_time

        # åŠ è½½æ•°æ®
        df = data_processor.load_data(actual_start_time, actual_end_time)

        if df.empty:
            return TripAnalysisResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                analysis_result=TripAnalysisStatistics(
                    time_range={"start": actual_start_time, "end": actual_end_time},
                    daily_classifications=[],
                    overall_stats={},
                    trend_analysis={}
                ),
                visualization_data={}
            )

        # åˆ›å»ºè·¯æ®µåˆ†æå¼•æ“
        from .road_analysis_engine import RoadAnalysisEngine
        road_engine = RoadAnalysisEngine()

        # æ ‡å‡†åŒ–æ•°æ®åˆ—åå¹¶è°ƒè¯•
        if 'COMMADDR' in df.columns:
            df['vehicle_id'] = df['COMMADDR']
        if 'UTC' in df.columns:
            df['timestamp'] = df['UTC']
        if 'LAT' in df.columns and 'LON' in df.columns:
            df['latitude'] = df['LAT'] / 1e5
            df['longitude'] = df['LON'] / 1e5

        # è¿›è¡Œè·¯ç¨‹åˆ†æ
        analysis_result = road_engine.analyze_trip_distance_classification(df)

        # ç”Ÿæˆå¯è§†åŒ–æ•°æ®
        visualization_data = {
            "daily_chart": {
                "type": "stacked_bar",
                "title": "æ¯æ—¥è·¯ç¨‹åˆ†ç±»ç»Ÿè®¡",
                "data": [
                    {
                        "date": daily.date,
                        "short_trips": daily.short_trips,
                        "medium_trips": daily.medium_trips,
                        "long_trips": daily.long_trips,
                        "short_percentage": daily.short_percentage,
                        "medium_percentage": daily.medium_percentage,
                        "long_percentage": daily.long_percentage
                    }
                    for daily in analysis_result.daily_classifications
                ]
            },
            "pie_chart": {
                "type": "pie",
                "title": "æ€»ä½“è·¯ç¨‹åˆ†ç±»å æ¯”",
                "data": [
                    {
                        "name": "çŸ­é€” (<4km)",
                        "value": analysis_result.overall_stats.get("overall_short_percentage", 0),
                        "count": analysis_result.overall_stats.get("short_trips_total", 0)
                    },
                    {
                        "name": "ä¸­é€” (4-8km)",
                        "value": analysis_result.overall_stats.get("overall_medium_percentage", 0),
                        "count": analysis_result.overall_stats.get("medium_trips_total", 0)
                    },
                    {
                        "name": "é•¿é€” (>8km)",
                        "value": analysis_result.overall_stats.get("overall_long_percentage", 0),
                        "count": analysis_result.overall_stats.get("long_trips_total", 0)
                    }
                ]
            },
            "trend_chart": {
                "type": "line",
                "title": "è·¯ç¨‹åˆ†ç±»è¶‹åŠ¿åˆ†æ",
                "data": {
                    "short_trip_trend": analysis_result.trend_analysis.get("short_trip_trend", "stable"),
                    "medium_trip_trend": analysis_result.trend_analysis.get("medium_trip_trend", "stable"),
                    "long_trip_trend": analysis_result.trend_analysis.get("long_trip_trend", "stable"),
                    "dominant_category": analysis_result.trend_analysis.get("most_common_distance_category", "unknown")
                }
            },
            "statistics_summary": {
                "total_trips": analysis_result.overall_stats.get("total_trips", 0),
                "avg_daily_trips": analysis_result.overall_stats.get("avg_daily_trips", 0),
                "overall_avg_distance": analysis_result.overall_stats.get("overall_avg_distance", 0),
                "analysis_days": len(analysis_result.daily_classifications)
            }
        }

        processing_time = time.time() - start_processing

        logger.info("è·¯ç¨‹åˆ†æ", f"è·¯ç¨‹åˆ†æå®Œæˆï¼Œå¤„ç†æ—¶é—´: {processing_time:.2f}ç§’", source="APIè°ƒç”¨", user="ç”¨æˆ·")

        return TripAnalysisResponse(
            success=True,
            message=f"è·¯ç¨‹åˆ†æå®Œæˆï¼Œå…±åˆ†æäº† {analysis_result.overall_stats.get('total_trips', 0)} ä¸ªè®¢å•",
            analysis_result=analysis_result,
            visualization_data=visualization_data,
            processing_time=processing_time
        )

    except Exception as e:
        logger.error("è·¯ç¨‹åˆ†æ", f"è·¯ç¨‹åˆ†æå¤±è´¥: {str(e)}", source="APIè°ƒç”¨", user="ç”¨æˆ·")
        logger.error("è·¯ç¨‹åˆ†æ", f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}", source="APIè°ƒç”¨", user="ç”¨æˆ·")

        return TripAnalysisResponse(
            success=False,
            message=f"è·¯ç¨‹åˆ†æå¤±è´¥: {str(e)}",
            analysis_result=TripAnalysisStatistics(
                time_range={"start": actual_start_time if 'actual_start_time' in locals() else start_time,
                            "end": actual_end_time if 'actual_end_time' in locals() else end_time},
                daily_classifications=[],
                overall_stats={"error": str(e)},
                trend_analysis={}
            ),
            visualization_data={}
        )


@router.post("/road/order-speed-analysis", response_model=OrderSpeedAnalysisResponse)
async def analyze_order_based_road_speed(
        start_time: float = Query(..., description="å¼€å§‹æ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        end_time: float = Query(..., description="ç»“æŸæ—¶é—´æˆ³ï¼ˆUTCï¼‰"),
        request: OrderSpeedAnalysisRequest = None
):
    """
    åŸºäºè®¢å•çš„é“è·¯é€Ÿåº¦åˆ†æ
    åˆ©ç”¨ä¸­çŸ­é€”è®¢å•æ•°æ®ä¸­çš„é¢„ä¼°è·ç¦»ä¸èµ·æ­¢æ—¶é—´ï¼Œè®¡ç®—è®¢å•çš„å¹³å‡é€Ÿåº¦
    å¤§æ•°æ®èƒŒæ™¯ä¸‹ï¼Œå¤§é‡è®¢å•çš„å¹³å‡é€Ÿåº¦å¯ä»¥å®æ—¶åæ˜ é“è·¯çš„æ‹¥å µçŠ¶å†µ
    å®Œæˆé“è·¯é€Ÿåº¦çš„å¯è§†åŒ–å±•ç¤º
    """
    try:
        start_processing = time.time()

        # é»˜è®¤è¯·æ±‚å‚æ•°
        if request is None:
            request = OrderSpeedAnalysisRequest()

        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_time, end_time)

        if df.empty:
            return OrderSpeedAnalysisResponse(
                success=False,
                message="æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®",
                speed_analysis=RoadSpeedAnalysisResult(
                    time_range={"start": start_time, "end": end_time},
                    speed_data=[],
                    heatmap_data=[],
                    congestion_summary={},
                    road_speed_trends=[]
                ),
                visualization_data={}
            )

        # åˆ›å»ºè·¯æ®µåˆ†æå¼•æ“
        from .road_analysis_engine import RoadAnalysisEngine
        road_engine = RoadAnalysisEngine()

        # æ ‡å‡†åŒ–æ•°æ®åˆ—å
        if 'COMMADDR' in df.columns:
            df['vehicle_id'] = df['COMMADDR']
        if 'UTC' in df.columns:
            df['timestamp'] = df['UTC']
        if 'LAT' in df.columns and 'LON' in df.columns:
            df['latitude'] = df['LAT'] / 1e5
            df['longitude'] = df['LON'] / 1e5

        # è¿›è¡Œè®¢å•é€Ÿåº¦åˆ†æ
        speed_analysis = road_engine.analyze_order_based_road_speed(
            df,
            include_short_medium_only=request.include_short_medium_only,
            spatial_resolution=request.spatial_resolution,
            min_orders_per_location=request.min_orders_per_location,
            congestion_threshold=request.congestion_threshold
        )

        # ç”Ÿæˆå¯è§†åŒ–æ•°æ®
        visualization_data = {
            "speed_heatmap": {
                "type": "heatmap",
                "title": "é“è·¯é€Ÿåº¦çƒ­åŠ›å›¾",
                "data": [
                    {
                        "lat": heatmap.lat,
                        "lng": heatmap.lng,
                        "speed": heatmap.speed,
                        "intensity": heatmap.intensity,
                        "order_count": heatmap.order_count,
                        "congestion_level": heatmap.congestion_level
                    }
                    for heatmap in speed_analysis.heatmap_data
                ]
            },
            "congestion_distribution": {
                "type": "pie",
                "title": "æ‹¥å µç­‰çº§åˆ†å¸ƒ",
                "data": [
                    {
                        "name": f"{level} (æ‹¥å µç­‰çº§)",
                        "value": info["percentage"],
                        "count": info["count"]
                    }
                    for level, info in speed_analysis.congestion_summary.get("congestion_distribution", {}).items()
                ]
            },
            "speed_trends": {
                "type": "line",
                "title": "24å°æ—¶é“è·¯é€Ÿåº¦è¶‹åŠ¿",
                "data": [
                    {
                        "hour": trend["hour"],
                        "avg_speed": trend["avg_speed"],
                        "order_count": trend["order_count"],
                        "is_peak_hour": trend["is_peak_hour"],
                        "speed_category": trend["speed_category"]
                    }
                    for trend in speed_analysis.road_speed_trends
                ]
            },
            "speed_statistics": {
                "overall_avg_speed": speed_analysis.congestion_summary.get("overall_avg_speed", 0),
                "total_analysis_locations": speed_analysis.congestion_summary.get("total_analysis_locations", 0),
                "high_confidence_locations": speed_analysis.congestion_summary.get("high_confidence_locations", 0),
                "total_orders_analyzed": speed_analysis.congestion_summary.get("total_orders_analyzed", 0),
                "speed_range": {
                    "min": speed_analysis.congestion_summary.get("speed_statistics", {}).get("min_speed", 0),
                    "max": speed_analysis.congestion_summary.get("speed_statistics", {}).get("max_speed", 0),
                    "median": speed_analysis.congestion_summary.get("speed_statistics", {}).get("median_speed", 0)
                }
            },
            "congestion_zones": {
                "type": "scatter",
                "title": "æ‹¥å µåŒºåŸŸåˆ†å¸ƒ",
                "data": [
                    {
                        "lat": data.location["lat"],
                        "lng": data.location["lng"],
                        "speed": data.avg_speed,
                        "congestion_level": data.congestion_level,
                        "order_count": data.order_count,
                        "confidence": data.confidence_score
                    }
                    for data in speed_analysis.speed_data
                    if data.congestion_level in ["heavy", "jam"]  # åªæ˜¾ç¤ºæ‹¥å µåŒºåŸŸ
                ]
            }
        }

        processing_time = time.time() - start_processing

        logger.info(f"è®¢å•é€Ÿåº¦åˆ†æå®Œæˆï¼Œå¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")

        return OrderSpeedAnalysisResponse(
            success=True,
            message=f"è®¢å•é€Ÿåº¦åˆ†æå®Œæˆï¼Œåˆ†æäº† {len(speed_analysis.speed_data)} ä¸ªä½ç½®çš„é€Ÿåº¦æ•°æ®",
            speed_analysis=speed_analysis,
            visualization_data=visualization_data,
            processing_time=processing_time
        )

    except Exception as e:
        logger.error(f"è®¢å•é€Ÿåº¦åˆ†æå¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

        return OrderSpeedAnalysisResponse(
            success=False,
            message=f"è®¢å•é€Ÿåº¦åˆ†æå¤±è´¥: {str(e)}",
            speed_analysis=RoadSpeedAnalysisResult(
                time_range={"start": start_time, "end": end_time},
                speed_data=[],
                heatmap_data=[],
                congestion_summary={"error": str(e)},
                road_speed_trends=[]
            ),
            visualization_data={}
        )


class DailyWeatherImpactRequest(BaseModel):
    start_time: float
    end_time: float


class HourlyWeatherImpactResponse(BaseModel):
    hour: int
    weather_condition: str
    temperature: float
    humidity: float
    precipitation: float
    loaded_vehicles: int
    total_vehicles: int
    impact_factor: float
    impact_description: str


class DailyWeatherImpactResponseItem(BaseModel):
    date: str
    hourly_impacts: List[HourlyWeatherImpactResponse]
    daily_summary: Dict[str, Any]


class DailyWeatherImpactResponse(BaseModel):
    success: bool
    message: str
    daily_impacts: List[DailyWeatherImpactResponseItem]


@router.post("/smart-passenger/daily-weather-impact", response_model=DailyWeatherImpactResponse)
async def analyze_daily_weather_impact(request: DailyWeatherImpactRequest):
    """åˆ†ææ¯å¤©æ¯å°æ—¶çš„å¤©æ°”å½±å“"""
    try:
        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(request.start_time, request.end_time)
        if df.empty:
            return DailyWeatherImpactResponse(success=False, message="æ²¡æœ‰å¯ç”¨çš„äº¤é€šæ•°æ®", daily_impacts=[])
        # è·å–å¤©æ°”æ•°æ®
        from .smart_passenger_engine import SmartPassengerEngine
        smart_engine = SmartPassengerEngine()
        weather_data = smart_engine.get_weather_data(request.start_time, request.end_time)
        # è°ƒç”¨åˆ†ææ–¹æ³•
        daily_impacts = smart_engine.analyze_daily_weather_impact(df, weather_data)
        # è½¬æ¢ä¸ºå“åº”ç»“æ„
        response_items = []
        for d in daily_impacts:
            response_items.append(DailyWeatherImpactResponseItem(
                date=d.date,
                hourly_impacts=[HourlyWeatherImpactResponse(**h.dict()) for h in d.hourly_impacts],
                daily_summary=d.daily_summary
            ))
        return DailyWeatherImpactResponse(success=True, message="åˆ†ææˆåŠŸ", daily_impacts=response_items)
    except Exception as e:
        return DailyWeatherImpactResponse(success=False, message=f"åˆ†æå¤±è´¥: {str(e)}", daily_impacts=[])


@router.get("/road/test-visualization")
async def test_road_visualization():
    """æµ‹è¯•è·¯æ®µå¯è§†åŒ–æ•°æ®ç”Ÿæˆ"""
    try:
        logger.info("æµ‹è¯•è·¯æ®µå¯è§†åŒ–æ•°æ®...")

        # ä½¿ç”¨å›ºå®šçš„æµ‹è¯•æ•°æ®
        test_segments = [
            {
                "segment_id": "test_001",
                "start_point": {"lat": 36.651, "lng": 117.129},
                "end_point": {"lat": 36.655, "lng": 117.135},
                "road_name": "æµ‹è¯•è·¯æ®µ1",
                "road_type": "urban",
                "avg_speed": 35.5,
                "congestion_level": "moderate"
            }
        ]

        logger.info(f"è¿”å› {len(test_segments)} æ¡æµ‹è¯•è·¯æ®µæ•°æ®")
        return {
            "success": True,
            "visualization_data": test_segments,
            "message": "æµ‹è¯•æ•°æ®ç”ŸæˆæˆåŠŸ"
        }

    except Exception as e:
        logger.error(f"æµ‹è¯•è·¯æ®µå¯è§†åŒ–æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return {
            "success": False,
            "message": f"æµ‹è¯•å¤±è´¥: {str(e)}",
            "visualization_data": []
        }


# ç¼“å­˜ç®¡ç†ç›¸å…³APIæ¥å£

@router.get("/cache/statistics")
async def get_cache_statistics():
    """è·å–ç¼“å­˜ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        cache = get_traffic_cache()
        stats = cache.get_cache_statistics()

        return {
            "success": True,
            "message": "ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ",
            "statistics": stats
        }

    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜ç»Ÿè®¡æ—¶å‡ºé”™: {str(e)}")
        return {
            "success": False,
            "message": f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}",
            "statistics": {}
        }


@router.get("/cache/list")
async def get_cached_analyses():
    """è·å–æ‰€æœ‰ç¼“å­˜çš„åˆ†æç»“æœåˆ—è¡¨"""
    try:
        cache = get_traffic_cache()
        cached_list = cache.get_cached_analysis_list()

        return {
            "success": True,
            "message": f"æ‰¾åˆ° {len(cached_list)} ä¸ªç¼“å­˜çš„åˆ†æç»“æœ",
            "cached_analyses": cached_list
        }

    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
        return {
            "success": False,
            "message": f"è·å–ç¼“å­˜åˆ—è¡¨å¤±è´¥: {str(e)}",
            "cached_analyses": []
        }


@router.get("/cache/report/{cache_key}")
async def get_cache_report(cache_key: str):
    """è·å–æŒ‡å®šç¼“å­˜çš„å¯è¯»æ€§æŠ¥å‘Š"""
    try:
        cache = get_traffic_cache()
        report = cache.get_cache_report(cache_key)

        if report:
            return {
                "success": True,
                "message": "ç¼“å­˜æŠ¥å‘Šè·å–æˆåŠŸ",
                "report": report
            }
        else:
            return {
                "success": False,
                "message": "ç¼“å­˜æŠ¥å‘Šä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ",
                "report": None
            }

    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜æŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
        return {
            "success": False,
            "message": f"è·å–ç¼“å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}",
            "report": None
        }


@router.delete("/cache/{cache_key}")
async def delete_cache_by_key(cache_key: str):
    """åˆ é™¤æŒ‡å®šçš„ç¼“å­˜"""
    try:
        cache = get_traffic_cache()
        deleted = cache.delete_cache(cache_key)

        if deleted:
            return {
                "success": True,
                "message": f"ç¼“å­˜ {cache_key} åˆ é™¤æˆåŠŸ"
            }
        else:
            return {
                "success": False,
                "message": f"ç¼“å­˜ {cache_key} ä¸å­˜åœ¨æˆ–åˆ é™¤å¤±è´¥"
            }

    except Exception as e:
        logger.error(f"åˆ é™¤ç¼“å­˜æ—¶å‡ºé”™: {str(e)}")
        return {
            "success": False,
            "message": f"åˆ é™¤ç¼“å­˜å¤±è´¥: {str(e)}"
        }


@router.post("/cache/clear")
async def clear_all_cache():
    """æ¸…ç†æ‰€æœ‰è¿‡æœŸç¼“å­˜"""
    try:
        cache = get_traffic_cache()
        # è§¦å‘ç¼“å­˜æ¸…ç†
        cache._cleanup_expired_cache()

        # è·å–æ¸…ç†åçš„ç»Ÿè®¡ä¿¡æ¯
        stats = cache.get_cache_statistics()

        return {
            "success": True,
            "message": "è¿‡æœŸç¼“å­˜æ¸…ç†å®Œæˆ",
            "statistics_after_cleanup": stats
        }

    except Exception as e:
        logger.error(f"æ¸…ç†ç¼“å­˜æ—¶å‡ºé”™: {str(e)}")
        return {
            "success": False,
            "message": f"æ¸…ç†ç¼“å­˜å¤±è´¥: {str(e)}"
        }


# å¯¼å…¥æ—¥å¿—è·¯ç”±
from .log import router as log_router

# æ³¨å†Œæ—¥å¿—è·¯ç”±
router.include_router(log_router, tags=["äº¤é€šæ—¥å¿—"])


@lru_cache(maxsize=128)
def get_preaggregated_data(date: str, time_resolution: int) -> Optional[Dict[str, Any]]:
    """
    ä»é¢„èšåˆæ•°æ®æ–‡ä»¶è¯»å–æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰

    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
        time_resolution: æ—¶é—´åˆ†è¾¨ç‡ (åˆ†é’Ÿ)

    Returns:
        é¢„èšåˆæ•°æ®å­—å…¸æˆ–None
    """
    try:
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        base_dir = os.path.join(
            os.path.dirname(os.path.abspath(__file__)),
            'data', 'preaggregated'
        )
        resolution_dir = f"{time_resolution}min"
        json_filename = f"{date}_{time_resolution}min.json"
        filepath = os.path.join(base_dir, resolution_dir, json_filename)

        if not os.path.exists(filepath):
            return None

        # è¯»å–JSONæ–‡ä»¶
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)

        return data

    except Exception as e:
        print(f"è¯»å–é¢„èšåˆæ•°æ®å¤±è´¥: {e}")
        return None


@router.get("/smart-passenger/loaded-vehicles-timeline")
async def get_loaded_vehicles_timeline(
        date: str = Query(..., description="æ—¥æœŸ YYYY-MM-DD"),
        time_resolution: int = Query(15, description="æ—¶é—´åˆ†è¾¨ç‡(åˆ†é’Ÿ)")
):
    """
    è·å–æŒ‡å®šæ—¥æœŸçš„è½½å®¢è½¦è¾†æ—¶é—´çº¿æ•°æ®ï¼ˆä½¿ç”¨é¢„èšåˆæ•°æ®ï¼‰
    å±•ç¤ºçœŸå®çš„è½½å®¢è½¦è¾†æ•°é‡å˜åŒ–ï¼Œè€Œä¸æ˜¯ä¼°ç®—çš„è®¢å•æ•°é‡
    """
    try:
        logger.info("è½½å®¢è½¦è¾†åˆ†æ", f"å¼€å§‹è·å– {date} çš„è½½å®¢è½¦è¾†æ—¶é—´çº¿ï¼ˆé¢„èšåˆï¼‰", source="APIè°ƒç”¨", user="ç³»ç»Ÿ")

        # éªŒè¯æ—¶é—´åˆ†è¾¨ç‡
        if time_resolution not in [15, 30, 60]:
            return {
                "success": False,
                "message": "æ—¶é—´åˆ†è¾¨ç‡å¿…é¡»æ˜¯ 15ã€30 æˆ– 60 åˆ†é’Ÿ",
                "data": []
            }

        # å°è¯•ä»é¢„èšåˆæ•°æ®è¯»å–
        preaggregated_data = get_preaggregated_data(date, time_resolution)

        if preaggregated_data:
            print(f"âš¡ ä½¿ç”¨é¢„èšåˆæ•°æ®: {date} - {time_resolution}åˆ†é’Ÿ")
            logger.info("è½½å®¢è½¦è¾†åˆ†æ", f"ä½¿ç”¨é¢„èšåˆæ•°æ®ï¼Œå…± {len(preaggregated_data['data'])} ä¸ªæ—¶é—´æ®µ",
                        source="APIè°ƒç”¨", user="ç³»ç»Ÿ")

            return {
                "success": True,
                "message": f"æˆåŠŸè·å– {date} çš„è½½å®¢è½¦è¾†æ—¶é—´çº¿æ•°æ®ï¼ˆé¢„èšåˆï¼‰",
                "date": date,
                "time_resolution": time_resolution,
                "data": preaggregated_data['data'],
                "summary": preaggregated_data['summary'],
                "source": "preaggregated"
            }

        # å¦‚æœé¢„èšåˆæ•°æ®ä¸å­˜åœ¨ï¼Œå›é€€åˆ°å®æ—¶è®¡ç®—ï¼ˆä½†ä¼šè­¦å‘Šï¼‰
        print(f"âš ï¸ é¢„èšåˆæ•°æ®ä¸å­˜åœ¨ï¼Œå›é€€åˆ°å®æ—¶è®¡ç®—: {date} - {time_resolution}åˆ†é’Ÿ")
        logger.warning("è½½å®¢è½¦è¾†åˆ†æ", f"é¢„èšåˆæ•°æ®ä¸å­˜åœ¨ï¼Œä½¿ç”¨å®æ—¶è®¡ç®—", source="APIè°ƒç”¨", user="ç³»ç»Ÿ")

        # è§£ææ—¥æœŸï¼Œè½¬æ¢ä¸ºæ—¶é—´æˆ³
        try:
            from datetime import datetime
            date_obj = datetime.strptime(date, "%Y-%m-%d")
            start_timestamp = int(date_obj.timestamp())
            end_timestamp = start_timestamp + 24 * 3600  # 24å°æ—¶
        except ValueError:
            return {
                "success": False,
                "message": "æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼",
                "data": []
            }

        # åŠ è½½æ•°æ®
        data_processor = TrafficDataProcessor()
        df = data_processor.load_data(start_timestamp, end_timestamp)

        if df.empty:
            return {
                "success": False,
                "message": f"æ²¡æœ‰æ‰¾åˆ° {date} çš„æ•°æ®",
                "data": []
            }

        # å¤§å¹…å‡å°‘æ•°æ®é‡ä»¥æé«˜æ€§èƒ½ï¼Œç¡®ä¿APIåœ¨10ç§’å†…å“åº”
        if len(df) > 2000:
            print(f"æ•°æ®é‡è¿‡å¤§({len(df)}æ¡)ï¼Œå¤§å¹…é‡‡æ ·åˆ°2000æ¡ä»¥ç¡®ä¿æ€§èƒ½")
            df = df.sample(n=2000, random_state=42)

        # æ ‡å‡†åŒ–åˆ—å
        column_mapping = {
            'COMMADDR': 'vehicle_id',
            'UTC': 'timestamp',
            'lat': 'latitude',
            'lon': 'longitude',
            'SPEED': 'speed',
            'TFLAG': 'tflag',
            'is_occupied': 'is_occupied'
        }

        # åªé‡å‘½åå­˜åœ¨çš„åˆ—ï¼Œé¿å…é‡å¤
        existing_columns = {k: v for k, v in column_mapping.items() if k in df.columns}
        df = df.rename(columns=existing_columns)

        # æ£€æŸ¥å¹¶åˆ é™¤é‡å¤åˆ—å
        duplicate_columns = df.columns[df.columns.duplicated()].tolist()
        if duplicate_columns:
            df = df.loc[:, ~df.columns.duplicated()]

        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        required_columns = ['vehicle_id', 'timestamp', 'latitude', 'longitude']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            return {
                "success": False,
                "message": f"æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}",
                "data": []
            }

        # ç¡®ä¿timestampåˆ—æ˜¯æ•°å€¼ç±»å‹
        df['timestamp'] = pd.to_numeric(df['timestamp'], errors='coerce')
        df = df.dropna(subset=['timestamp'])

        # ç¡®ä¿vehicle_idåˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
        df['vehicle_id'] = df['vehicle_id'].astype(str)

        # éªŒè¯æ—¶é—´æˆ³èŒƒå›´
        if df.empty:
            return {
                "success": False,
                "message": f"æ—¶é—´æˆ³è½¬æ¢åæ²¡æœ‰æœ‰æ•ˆæ•°æ®",
                "data": []
            }

        # ä¼˜åŒ–ï¼šé¢„å…ˆè®¡ç®—è½½å®¢çŠ¶æ€
        if 'is_occupied' in df.columns:
            df['is_loaded'] = df['is_occupied']
        elif 'tflag' in df.columns:
            df['is_loaded'] = df['tflag'] == 268435456
        else:
            df['is_loaded'] = True  # å¦‚æœæ²¡æœ‰è½½å®¢çŠ¶æ€å­—æ®µï¼Œå‡è®¾æ‰€æœ‰è½¦è¾†éƒ½è½½å®¢

        # æŒ‰æ—¶é—´çª—å£ç»Ÿè®¡è½½å®¢è½¦è¾†ï¼ˆé«˜æ€§èƒ½ç‰ˆæœ¬ï¼‰
        timeline_data = []
        current_time = start_timestamp

        # å¤§å¹…å‡å°‘æ—¶é—´çª—å£æ•°é‡ä»¥æé«˜æ€§èƒ½
        max_windows = 48  # æœ€å¤š48ä¸ªæ—¶é—´çª—å£ï¼ˆ24å°æ—¶ * 2ä¸ªçª—å£/å°æ—¶ï¼‰
        window_count = 0

        # é¢„è®¡ç®—æ—¶é—´çª—å£è¾¹ç•Œ
        time_windows = []
        while current_time < end_timestamp and window_count < max_windows:
            window_end = current_time + (time_resolution * 60)
            time_windows.append((current_time, window_end))
            current_time = window_end
            window_count += 1

        # æ‰¹é‡å¤„ç†æ—¶é—´çª—å£
        for start_win, end_win in time_windows:
            # è¿‡æ»¤å½“å‰æ—¶é—´çª—å£çš„æ•°æ®
            mask = (df['timestamp'] >= start_win) & (df['timestamp'] < end_win)
            window_data = df[mask]

            if not window_data.empty:
                # å¿«é€Ÿç»Ÿè®¡è½½å®¢è½¦è¾†æ•°é‡
                loaded_vehicles = window_data[window_data['is_loaded'] == True]['vehicle_id'].nunique()
                total_vehicles = window_data['vehicle_id'].nunique()

                # è®¡ç®—è½½å®¢ç‡
                occupancy_rate = (loaded_vehicles / total_vehicles * 100) if total_vehicles > 0 else 0

                # å¤§å¹…é™åˆ¶ä½ç½®ç‚¹æ•°é‡ä»¥æé«˜æ€§èƒ½
                loaded_positions = window_data[window_data['is_loaded'] == True][['latitude', 'longitude']].head(
                    20).to_dict('records')

                timeline_data.append({
                    "timestamp": start_win,
                    "time_label": datetime.fromtimestamp(start_win).strftime("%H:%M"),
                    "loaded_vehicles": loaded_vehicles,
                    "total_vehicles": total_vehicles,
                    "occupancy_rate": round(occupancy_rate, 2),
                    "vehicle_positions": loaded_positions
                })

        logger.info("è½½å®¢è½¦è¾†åˆ†æ", f"å®Œæˆ {date} çš„è½½å®¢è½¦è¾†åˆ†æï¼Œå…± {len(timeline_data)} ä¸ªæ—¶é—´æ®µ", source="APIè°ƒç”¨",
                    user="ç³»ç»Ÿ")

        return {
            "success": True,
            "message": f"æˆåŠŸè·å– {date} çš„è½½å®¢è½¦è¾†æ—¶é—´çº¿æ•°æ®ï¼ˆå®æ—¶è®¡ç®—ï¼‰",
            "date": date,
            "time_resolution": time_resolution,
            "data": timeline_data,
            "summary": {
                "total_time_windows": len(timeline_data),
                "max_loaded_vehicles": max([d['loaded_vehicles'] for d in timeline_data]) if timeline_data else 0,
                "avg_occupancy_rate": round(sum([d['occupancy_rate'] for d in timeline_data]) / len(timeline_data),
                                            2) if timeline_data else 0
            },
            "source": "realtime"
        }

    except Exception as e:
        logger.error("è½½å®¢è½¦è¾†åˆ†æ", f"è½½å®¢è½¦è¾†æ—¶é—´çº¿åˆ†æå¤±è´¥: {str(e)}", source="APIè°ƒç”¨", user="ç³»ç»Ÿ")
        return {
            "success": False,
            "message": f"åˆ†æå¤±è´¥: {str(e)}",
            "data": []
        }

class DailyTrafficResponse(BaseModel):
    success: bool
    message: str
    data: List[int]  # 24å°æ—¶çš„è½¦è¾†æ•°æ•°ç»„

class WeeklyTrafficResponse(BaseModel):
    success: bool
    message: str
    data: List[Dict[str, Union[str, int]]]  # æŒ‰å¤©çš„æ—¥æœŸå’Œè½¦è¾†æ€»æ•°

class KeyMetric(BaseModel):
    title: str
    value: str
    trend: float

class MetricsResponse(BaseModel):
    success: bool
    message: str
    data: List[KeyMetric]

class AreaStat(BaseModel):
    id: int
    name: str
    totalVehicles: int
    avgSpeed: float
    congestionRate: float
    trafficLevel: str

class AreaStatsResponse(BaseModel):
    success: bool
    message: str
    data: List[AreaStat]

class PeriodStat(BaseModel):
    name: str
    timeRange: str
    avgVehicles: int
    avgSpeed: float
    status: str
    statusClass: str

class PeriodStatsResponse(BaseModel):
    success: bool
    message: str
    data: List[PeriodStat]

# --- è¾…åŠ©å‡½æ•° ---
def load_spatial_grid():
    grid_file = Path(os.path.join(os.path.dirname(__file__), 'data', 'indexes', 'spatial_grid_0.001.json'))
    if grid_file.exists():
        with open(grid_file, 'r') as f:
            return json.load(f)
    # é»˜è®¤ç½‘æ ¼ï¼ˆç¤ºä¾‹æ•°æ®ï¼Œéœ€æ›¿æ¢ä¸ºå®é™…ç½‘æ ¼ï¼‰
    return {
        "1": {"name": "å¸‚ä¸­å¿ƒæ ¸å¿ƒåŒº", "bounds": [36.65, 36.67, 117.00, 117.02]},
        "2": {"name": "å•†ä¸šè´­ç‰©åŒº", "bounds": [36.67, 36.69, 117.02, 117.04]},
        "3": {"name": "ä½å®…å±…æ°‘åŒº", "bounds": [36.69, 36.71, 117.04, 117.06]},
        "4": {"name": "å·¥ä¸šå¼€å‘åŒº", "bounds": [36.71, 36.73, 117.06, 117.08]},
        "5": {"name": "æ–‡æ•™ç§‘ç ”åŒº", "bounds": [36.73, 36.75, 117.08, 117.10]},
        "6": {"name": "äº¤é€šæ¢çº½åŒº", "bounds": [36.75, 36.77, 117.10, 117.12]},
        "7": {"name": "ä¼‘é—²å¨±ä¹åŒº", "bounds": [36.77, 36.79, 117.12, 117.14]}
    }

def calculate_congestion_rate(vehicle_count: int, area_size: float = 1.2321) -> Tuple[float, str]:
    """è®¡ç®—æ‹¥å µç‡å’Œæµé‡ç­‰çº§ï¼Œé¢ç§¯å•ä½ä¸ºå¹³æ–¹å…¬é‡Œ"""
    density = vehicle_count / area_size
    congestion_rate = min(density * 100, 100)  # ç®€å•å¯†åº¦å…¬å¼ï¼Œéœ€è°ƒæ•´
    if congestion_rate >= 80:
        return congestion_rate, "ä¸¥é‡æ‹¥å µ"
    elif congestion_rate >= 60:
        return congestion_rate, "é‡åº¦æ‹¥å µ"
    elif congestion_rate >= 40:
        return congestion_rate, "ä¸­åº¦æ‹¥å µ"
    elif congestion_rate >= 20:
        return congestion_rate, "è½»åº¦æ‹¥å µ"
    else:
        return congestion_rate, "åŸºæœ¬ç•…é€š"

# --- æ–°å¢ API ç«¯ç‚¹ ---
@router.get("/daily", response_model=DailyTrafficResponse)
async def get_daily_traffic(
        date: str = Query(None, description="æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œé»˜è®¤ä¸ºä»Šå¤©")
):
    """è·å–æ¯æ—¥æµé‡è¶‹åŠ¿ï¼ˆ24å°æ—¶è½¦è¾†æ•°ï¼‰- åªè¯»ç¼“å­˜æ–‡ä»¶"""
    if not date:
        date = datetime.now().strftime("%Y-%m-%d")
    cache_file = os.path.join(CACHE_DIR, f'daily_{date}.json')
    if os.path.exists(cache_file):
        with open(cache_file, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        logger.info("æµé‡åˆ†æ", f"ç›´æ¥è¿”å›ç¼“å­˜: {cache_file}")
        return DailyTrafficResponse(**cache_data)
    else:
        logger.error("æµé‡åˆ†æ", f"æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶: {cache_file}")
        return DailyTrafficResponse(
            success=False,
            message=f"æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶: {cache_file}ï¼Œè¯·å…ˆè¿è¡Œç¼“å­˜ç”Ÿæˆè„šæœ¬ã€‚",
            data=[0] * 24
        )

@router.get("/weekly", response_model=WeeklyTrafficResponse)
async def get_weekly_traffic(
        start_date: str = Query(None, description="å‘¨èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œé»˜è®¤ä¸ºæ•°æ®é›†çš„èµ·å§‹å‘¨")
):
    if not start_date:
        start_date = "2013-09-12"
    # åªå…è®¸2013-09-12
    if start_date != "2013-09-12":
        return WeeklyTrafficResponse(
            success=False,
            message="åªæ”¯æŒ2013-09-12ä½œä¸ºèµ·å§‹æ—¥æœŸ",
            data=[]
        )
    cache_file = os.path.join(CACHE_DIR, f'weekly_{start_date}.json')
    if os.path.exists(cache_file):
        with open(cache_file, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        logger.info("æµé‡åˆ†æ", f"ç›´æ¥è¿”å›ç¼“å­˜: {cache_file}")
        return WeeklyTrafficResponse(**cache_data)
    else:
        logger.error("æµé‡åˆ†æ", f"æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶: {cache_file}")
        return WeeklyTrafficResponse(
            success=False,
            message=f"æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶: {cache_file}ï¼Œè¯·å…ˆè¿è¡Œç¼“å­˜ç”Ÿæˆè„šæœ¬ã€‚",
            data=[]
        )

@router.get("/metrics", response_model=MetricsResponse)
async def get_metrics(
        date: str = Query(None, description="æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œé»˜è®¤ä¸ºæ•°æ®é›†èµ·å§‹æ—¥æœŸ"),
        period: str = Query("today", description="æ—¶é—´èŒƒå›´ï¼štoday, weekï¼ˆå…¼å®¹æ€§å‚æ•°ï¼‰")
):
    """è·å–å…³é”®æŒ‡æ ‡ï¼ˆæ€»æµé‡ã€å¹³å‡é€Ÿåº¦ã€é«˜å³°æ—¶é•¿ã€æ´»è·ƒç”¨æˆ·ï¼‰- ä½¿ç”¨å®Œæ•´æ•°æ®ç»Ÿè®¡"""
    # ç¼“å­˜æ–‡ä»¶åé€»è¾‘
    if date:
        cache_file = os.path.join(CACHE_DIR, f'metrics_{date}.json')
    else:
        # week æ¨¡å¼ä¸‹ç”¨ metrics_week_2013-09-12.json
        cache_file = os.path.join(CACHE_DIR, f'metrics_week_2013-09-12.json')
    if os.path.exists(cache_file):
        with open(cache_file, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        logger.info("æµé‡åˆ†æ", f"ç›´æ¥è¿”å›æŒ‡æ ‡ç¼“å­˜: {cache_file}")
        return MetricsResponse(**cache_data)
    try:
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ•°æ®é›†çš„å®é™…æ—¥æœŸèŒƒå›´
        if date:
            # éªŒè¯æ—¥æœŸæ˜¯å¦åœ¨æ•°æ®é›†èŒƒå›´å†…
            dataset_start = "2013-09-12"
            dataset_end = "2013-09-18"

            requested_date = datetime.strptime(date, "%Y-%m-%d")
            dataset_start_date = datetime.strptime(dataset_start, "%Y-%m-%d")
            dataset_end_date = datetime.strptime(dataset_end, "%Y-%m-%d")

            if requested_date < dataset_start_date or requested_date > dataset_end_date:
                return MetricsResponse(
                    success=False,
                    message=f"è¯·æ±‚çš„æ—¥æœŸè¶…å‡ºæ•°æ®é›†èŒƒå›´ï¼Œå¯ç”¨æ—¥æœŸï¼š{dataset_start} åˆ° {dataset_end}",
                    data=[
                        KeyMetric(title="æ€»æµé‡", value="0", trend=0),
                        KeyMetric(title="å¹³å‡é€Ÿåº¦", value="0.0km/h", trend=0),
                        KeyMetric(title="é«˜å³°æ—¶é•¿", value="0.0h", trend=0),
                        KeyMetric(title="æ´»è·ƒç”¨æˆ·", value="0", trend=0)
                    ]
                )

            # ä½¿ç”¨ä¼ å…¥çš„å…·ä½“æ—¥æœŸ
            start_time = datetime.strptime(date, "%Y-%m-%d")
            end_time = start_time + timedelta(days=1) - timedelta(seconds=1)
            hours = 24
            period_desc = f"æ—¥æœŸ {date}"
        else:
            # ä½¿ç”¨æ•°æ®é›†çš„é»˜è®¤æ—¥æœŸè€Œä¸æ˜¯å½“å‰æ—¶é—´
            if period == "today":
                start_time = datetime.strptime("2013-09-12", "%Y-%m-%d")  # æ•°æ®é›†èµ·å§‹æ—¥æœŸ
                end_time = start_time + timedelta(days=1) - timedelta(seconds=1)
                hours = 24
                period_desc = "æ•°æ®é›†èµ·å§‹æ—¥æœŸ(2013-09-12)"
            else:  # week
                start_time = datetime.strptime("2013-09-12", "%Y-%m-%d")  # æ•°æ®é›†èµ·å§‹æ—¥æœŸ
                end_time = start_time + timedelta(days=7) - timedelta(seconds=1)
                hours = 7 * 24
                period_desc = "æ•°æ®é›†çš„å®Œæ•´å‘¨(2013-09-12 åˆ° 2013-09-18)"

        start_timestamp = int(start_time.timestamp())
        end_timestamp = int(end_time.timestamp())

        logger.info("æµé‡åˆ†æ", f"æŸ¥è¯¢å…³é”®æŒ‡æ ‡: {period_desc} ({start_timestamp} - {end_timestamp})")

        # ğŸš€ ä½¿ç”¨æ–°çš„æµé‡ç»Ÿè®¡åŠ è½½å™¨è®¡ç®—æŒ‡æ ‡
        metrics_data = traffic_stats_loader.calculate_key_metrics(
            start_timestamp,
            end_timestamp,
            hours
        )

        if not metrics_data["data_available"]:
            logger.warning("æµé‡åˆ†æ", f"æœªæ‰¾åˆ° {period_desc} çš„æ•°æ®")
            return MetricsResponse(
                success=False,
                message=f"æœªæ‰¾åˆ°{period_desc}çš„æµé‡æ•°æ®",
                data=[
                    KeyMetric(title="æ€»æµé‡", value="0", trend=0),
                    KeyMetric(title="å¹³å‡é€Ÿåº¦", value="0.0km/h", trend=0),
                    KeyMetric(title="é«˜å³°æ—¶é•¿", value="0.0h", trend=0),
                    KeyMetric(title="æ´»è·ƒç”¨æˆ·", value="0", trend=0)
                ]
            )

        # ğŸ“Š æ„å»ºæŒ‡æ ‡ç»“æœ
        metrics = [
            KeyMetric(
                title="æ€»æµé‡",
                value=f"{metrics_data['total_vehicles']:,}",
                trend=0
            ),
            KeyMetric(
                title="å¹³å‡é€Ÿåº¦",
                value=f"{metrics_data['avg_speed']:.1f}km/h",
                trend=0
            ),
            KeyMetric(
                title="é«˜å³°æ—¶é•¿",
                value=f"{metrics_data['peak_hours']:.1f}h",
                trend=0
            ),
            KeyMetric(
                title="æ´»è·ƒç”¨æˆ·",
                value=f"{metrics_data['unique_vehicles']:,}",
                trend=0
            )
        ]

        # ğŸ“ˆ æ·»åŠ è¯¦ç»†çš„æˆåŠŸæ¶ˆæ¯
        success_message = (
            f"æˆåŠŸè·å–{period_desc}çš„å…³é”®æŒ‡æ ‡ - "
            f"æ€»æµé‡: {metrics_data['total_vehicles']:,}, "
            f"æ´»è·ƒè½¦è¾†: {metrics_data['unique_vehicles']:,}, "
            f"å¹³å‡é€Ÿåº¦: {metrics_data['avg_speed']:.1f}km/h"
        )

        result = MetricsResponse(
            success=True,
            message=success_message,
            data=convert_numpy_types(metrics)
        )
        # å†™å…¥ç¼“å­˜
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(result.dict(), f, ensure_ascii=False)
        return result

    except ValueError as ve:
        # å¤„ç†æ—¥æœŸæ ¼å¼é”™è¯¯
        logger.error("æµé‡åˆ†æ", f"æ—¥æœŸæ ¼å¼é”™è¯¯: {str(ve)}")
        result = MetricsResponse(
            success=False,
            message=f"æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨YYYY-MM-DDæ ¼å¼: {str(ve)}",
            data=[
                KeyMetric(title="æ€»æµé‡", value="0", trend=0),
                KeyMetric(title="å¹³å‡é€Ÿåº¦", value="0.0km/h", trend=0),
                KeyMetric(title="é«˜å³°æ—¶é•¿", value="0.0h", trend=0),
                KeyMetric(title="æ´»è·ƒç”¨æˆ·", value="0", trend=0)
            ]
        )
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(result.dict(), f, ensure_ascii=False)
        return result
    except Exception as e:
        logger.error("æµé‡åˆ†æ", f"å…³é”®æŒ‡æ ‡æŸ¥è¯¢å¤±è´¥: {str(e)}")
        logger.error("æµé‡åˆ†æ", f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        result = MetricsResponse(
            success=False,
            message=f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
            data=[
                KeyMetric(title="æ€»æµé‡", value="0", trend=0),
                KeyMetric(title="å¹³å‡é€Ÿåº¦", value="0.0km/h", trend=0),
                KeyMetric(title="é«˜å³°æ—¶é•¿", value="0.0h", trend=0),
                KeyMetric(title="æ´»è·ƒç”¨æˆ·", value="0", trend=0)
            ]
        )
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(result.dict(), f, ensure_ascii=False)
        return result

@router.get("/areas", response_model=AreaStatsResponse)
async def get_area_stats(
        date: str = Query(None, description="æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD"),
        period: str = Query("today", description="æ—¶é—´èŒƒå›´ï¼štoday, weekï¼ˆå…¼å®¹æ€§å‚æ•°ï¼‰")
):
    """è·å–åŒºåŸŸç»Ÿè®¡ï¼ˆè½¦è¾†æ•°ã€å¹³å‡é€Ÿåº¦ã€æ‹¥å µç‡ã€æµé‡ç­‰çº§ï¼‰"""
    try:
        spatial_grid = load_spatial_grid()

        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¼ å…¥çš„æ—¥æœŸ
        if date:
            start_time = datetime.strptime(date, "%Y-%m-%d")
            end_time = start_time + timedelta(days=1) - timedelta(seconds=1)
            period_desc = f"æ—¥æœŸ {date}"
        else:
            start_time = datetime.now()
            if period == "today":
                start_time = start_time.replace(hour=0, minute=0, second=0, microsecond=0)
                end_time = start_time + timedelta(days=1) - timedelta(seconds=1)
                period_desc = "ä»Šæ—¥"
            else:  # week
                start_time = start_time - timedelta(days=start_time.weekday())
                end_time = start_time + timedelta(days=7) - timedelta(seconds=1)
                period_desc = "æœ¬å‘¨"

        start_timestamp = int(start_time.timestamp())
        end_timestamp = int(end_time.timestamp())

        logger.info("æµé‡åˆ†æ", f"æŸ¥è¯¢åŒºåŸŸç»Ÿè®¡: {period_desc} ({start_timestamp} - {end_timestamp})")

        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_timestamp, end_timestamp)

        if df.empty:
            return AreaStatsResponse(
                success=False,
                message="æœªæ‰¾åˆ°æŒ‡å®šæ—¶é—´èŒƒå›´çš„æµé‡æ•°æ®",
                data=[]
            )

        # æŒ‰åŒºåŸŸèšåˆ
        area_data = {grid_id: {"total_vehicles": 0, "total_speed": 0, "count": 0} for grid_id in spatial_grid}

        for _, row in df.iterrows():
            # æ­£ç¡®å¤„ç†åæ ‡
            lat = row['LAT'] / 1e5 if 'LAT' in row else 0
            lon = row['LON'] / 1e5 if 'LON' in row else 0

            for grid_id, grid in spatial_grid.items():
                min_lat, max_lat, min_lon, max_lon = grid['bounds']
                if min_lat <= lat <= max_lat and min_lon <= lon <= max_lon:
                    area_data[grid_id]['total_vehicles'] += 1
                    # æ­£ç¡®å¤„ç†é€Ÿåº¦
                    speed = 0
                    if 'SPEED' in row and pd.notna(row['SPEED']):
                        speed = row['SPEED']
                    elif 'speed' in row and pd.notna(row['speed']):
                        speed = row['speed']
                    area_data[grid_id]['total_speed'] += speed
                    area_data[grid_id]['count'] += 1
                    break

        # æ„é€ ç»“æœ
        result = []
        for grid_id, data in area_data.items():
            if data['count'] > 0:
                avg_speed = data['total_speed'] / data['count']
                congestion_rate, traffic_level = calculate_congestion_rate(data['total_vehicles'])
                result.append(AreaStat(
                    id=int(grid_id),
                    name=spatial_grid[grid_id]['name'],
                    totalVehicles=data['total_vehicles'],
                    avgSpeed=round(avg_speed, 1),
                    congestionRate=round(congestion_rate, 1),
                    trafficLevel=traffic_level
                ))

        return AreaStatsResponse(
            success=True,
            message=f"æˆåŠŸè·å– {period} çš„åŒºåŸŸç»Ÿè®¡æ•°æ®ï¼Œå…± {len(result)} ä¸ªåŒºåŸŸ",
            data=convert_numpy_types(result)
        )
    except Exception as e:
        logger.error("æµé‡åˆ†æ", f"åŒºåŸŸç»Ÿè®¡æŸ¥è¯¢å¤±è´¥: {str(e)}")
        return AreaStatsResponse(
            success=False,
            message=f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
            data=[]
        )

@router.get("/periods", response_model=PeriodStatsResponse)
async def get_period_stats(
        date: str = Query(None, description="æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD"),
        period: str = Query("today", description="æ—¶é—´èŒƒå›´ï¼štoday, weekï¼ˆå…¼å®¹æ€§å‚æ•°ï¼‰")
):
    """è·å–æ—¶é—´æ®µç»Ÿè®¡ï¼ˆæ—©é«˜å³°ã€æ™šé«˜å³°ã€å¹³å³°ï¼‰"""
    try:
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¼ å…¥çš„æ—¥æœŸ
        if date:
            start_time = datetime.strptime(date, "%Y-%m-%d")
            days = 1
            period_desc = f"æ—¥æœŸ {date}"
        else:
            start_time = datetime.now()
            if period == "today":
                start_time = start_time.replace(hour=0, minute=0, second=0, microsecond=0)
                days = 1
                period_desc = "ä»Šæ—¥"
            else:  # week
                start_time = start_time - timedelta(days=start_time.weekday())
                days = 7
                period_desc = "æœ¬å‘¨"

        periods = [
            {"name": "æ—©é«˜å³°", "start_hour": 7, "end_hour": 9, "timeRange": "07:00-09:00"},
            {"name": "æ™šé«˜å³°", "start_hour": 17, "end_hour": 19, "timeRange": "17:00-19:00"},
            {"name": "å¹³å³°æ—¶æ®µ", "start_hour": 10, "end_hour": 16, "timeRange": "10:00-16:00"}
        ]

        start_timestamp = int(start_time.timestamp())
        end_timestamp = int((start_time + timedelta(days=days)).timestamp() - 1)

        logger.info("æµé‡åˆ†æ", f"æŸ¥è¯¢æ—¶é—´æ®µç»Ÿè®¡: {period_desc} ({start_timestamp} - {end_timestamp})")
        # åŠ è½½æ•°æ®
        df = data_processor.load_data(start_timestamp, end_timestamp)

        if df.empty:
            return PeriodStatsResponse(
                success=False,
                message="æœªæ‰¾åˆ°æŒ‡å®šæ—¶é—´èŒƒå›´çš„æµé‡æ•°æ®",
                data=[PeriodStat(
                    name=p['name'],
                    timeRange=p['timeRange'],
                    avgVehicles=0,
                    avgSpeed=0.0,
                    status="ç•…é€š",
                    statusClass="bg-green-500/20 text-green-400"
                ) for p in periods]
            )

        # æŒ‰æ—¶é—´æ®µèšåˆ
        result = []
        time_col = 'UTC' if 'UTC' in df.columns else 'timestamp'
        df['hour'] = pd.to_datetime(df[time_col], unit='s').dt.hour
        df['date'] = pd.to_datetime(df[time_col], unit='s').dt.date

        for p in periods:
            total_vehicles = 0
            total_speed = 0
            count = 0

            for day in range(days):
                day_start = start_time + timedelta(days=day)
                day_df = df[df['date'] == day_start.date()]
                period_df = day_df[day_df['hour'].between(p['start_hour'], p['end_hour'] - 1)]

                total_vehicles += len(period_df)
                # æ­£ç¡®è®¡ç®—é€Ÿåº¦
                speed_sum = 0
                if 'SPEED' in period_df.columns:
                    speed_sum = period_df['SPEED'].fillna(0).sum()
                elif 'speed' in period_df.columns:
                    speed_sum = period_df['speed'].fillna(0).sum()
                total_speed += speed_sum
                count += len(period_df) if not period_df.empty else 0

            avg_vehicles = total_vehicles / (days * (p['end_hour'] - p['start_hour'])) if count > 0 else 0
            avg_speed = total_speed / total_vehicles if total_vehicles > 0 else 0
            status = "ç•…é€š" if avg_speed > 50 else "æ‹¥å µ" if avg_speed < 35 else "ä¸­åº¦æ‹¥å µ"
            status_class = (
                "bg-green-500/20 text-green-400" if status == "ç•…é€š" else
                "bg-red-500/20 text-red-400" if status == "æ‹¥å µ" else
                "bg-orange-500/20 text-orange-400"
            )

            result.append(PeriodStat(
                name=p['name'],
                timeRange=p['timeRange'],
                avgVehicles=round(avg_vehicles),
                avgSpeed=round(avg_speed, 1),
                status=status,
                statusClass=status_class
            ))

        return PeriodStatsResponse(
            success=True,
            message=f"æˆåŠŸè·å– {period} çš„æ—¶é—´æ®µç»Ÿè®¡æ•°æ®",
            data=convert_numpy_types(result)
        )
    except Exception as e:
        logger.error("æµé‡åˆ†æ", f"æ—¶é—´æ®µç»Ÿè®¡æŸ¥è¯¢å¤±è´¥: {str(e)}")
        return PeriodStatsResponse(
            success=False,
            message=f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
            data=[PeriodStat(
                name=p['name'],
                timeRange=p['timeRange'],
                avgVehicles=0,
                avgSpeed=0.0,
                status="ç•…é€š",
                statusClass="bg-green-500/20 text-green-400"
            ) for p in periods]
        )


