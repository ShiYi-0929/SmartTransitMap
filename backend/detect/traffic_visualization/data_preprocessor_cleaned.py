import pandas as pd
import numpy as np
import os
import json
import pickle
from typing import Dict, List, Tuple
from collections import defaultdict
import math

def convert_numpy_types(obj):
    """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œç¡®ä¿JSONåºåˆ—åŒ–å…¼å®¹"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    else:
        return obj

class CleanedTrafficDataPreprocessor:
    """
    æ¸…æ´—åäº¤é€šæ•°æ®é¢„å¤„ç†å™¨
    å°†æ¸…æ´—åçš„CSVæ•°æ®é¢„å¤„ç†ä¸ºå¯å¿«é€ŸæŸ¥è¯¢çš„æ ¼å¼
    
    æ¸…æ´—åæ•°æ®æ ¼å¼ï¼š
    COMMADDR,UTC,LAT,LON,HEAD,SPEED,TFLAG,lat,lon,speed_kmh,is_occupied,timestamp
    """
    
    def __init__(self, data_dir: str = None):
        if data_dir is None:
            # æŒ‡å‘æ¸…æ´—åæ•°æ®ç›®å½•
            self.data_dir = os.path.join(os.path.dirname(__file__), 'data', 'cleaned')
        else:
            self.data_dir = data_dir
        
        # ç¡®ä¿ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®ç›®å½•
        if not self.data_dir.endswith('cleaned'):
            self.data_dir = os.path.join(self.data_dir, 'cleaned')
        
        # é¢„å¤„ç†è¾“å‡ºç›®å½•ï¼ˆä¸åŸæ¥çš„ä¿æŒä¸€è‡´ï¼Œä½†åŸºäºæ¸…æ´—åæ•°æ®ï¼‰
        self.processed_dir = os.path.join(os.path.dirname(self.data_dir), 'processed')
        self.index_dir = os.path.join(os.path.dirname(self.data_dir), 'indexes')
        
        print(f"æ¸…æ´—åæ•°æ®ç›®å½•: {self.data_dir}")
        print(f"é¢„å¤„ç†è¾“å‡ºç›®å½•: {self.processed_dir}")
        print(f"ç´¢å¼•è¾“å‡ºç›®å½•: {self.index_dir}")
        
        # åˆ›å»ºå¤„ç†åçš„æ•°æ®ç›®å½•
        os.makedirs(self.processed_dir, exist_ok=True)
        os.makedirs(self.index_dir, exist_ok=True)
    
    def preprocess_all_data(self):
        """é¢„å¤„ç†æ‰€æœ‰æ¸…æ´—åçš„æ•°æ®"""
        print("å¼€å§‹é¢„å¤„ç†æ¸…æ´—åçš„æ•°æ®...")
        
        # æ£€æŸ¥æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶
        cleaned_files = self._get_cleaned_csv_files()
        if not cleaned_files:
            print("âŒ æœªæ‰¾åˆ°æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶ï¼")
            print(f"è¯·ç¡®ä¿ {self.data_dir} ç›®å½•ä¸‹æœ‰æ¸…æ´—åçš„CSVæ–‡ä»¶")
            return
        
        print(f"æ‰¾åˆ° {len(cleaned_files)} ä¸ªæ¸…æ´—åçš„æ•°æ®æ–‡ä»¶")
        for file_path in cleaned_files:
            file_size = os.path.getsize(file_path) / (1024 * 1024)
            print(f"  - {os.path.basename(file_path)}: {file_size:.1f} MB")
        
        # 1. æ—¶é—´åˆ†ç‰‡é¢„èšåˆ
        self._create_time_based_aggregations()
        
        # 2. ç©ºé—´ç½‘æ ¼åŒ–
        self._create_spatial_grids()
        
        # 3. è½¦è¾†è½¨è¿¹ç´¢å¼•
        self._create_vehicle_indexes()
        
        # 4. çƒ­åŠ›å›¾é¢„è®¡ç®—
        self._create_heatmap_precomputed()
        
        # 5. ç”Ÿæˆæ•°æ®ç»Ÿè®¡ä¿¡æ¯
        self._create_data_summary()
        
        print("âœ… æ¸…æ´—åæ•°æ®é¢„å¤„ç†å®Œæˆï¼")
    
    def _create_time_based_aggregations(self):
        """æŒ‰æ—¶é—´åˆ†ç‰‡èšåˆæ¸…æ´—åçš„æ•°æ®"""
        print("åˆ›å»ºæ—¶é—´åˆ†ç‰‡èšåˆ...")
        
        cleaned_files = self._get_cleaned_csv_files()
        hourly_data = defaultdict(list)
        total_records = 0
        
        for file_path in cleaned_files:
            print(f"å¤„ç†æ¸…æ´—åæ–‡ä»¶: {os.path.basename(file_path)}")
            
            try:
                # åˆ†å—è¯»å–å¤§æ–‡ä»¶ï¼ˆæ¸…æ´—åçš„æ•°æ®åº”è¯¥å·²ç»æœ‰äº†æ­£ç¡®çš„åˆ—åï¼‰
                chunk_count = 0
                for chunk in pd.read_csv(file_path, chunksize=50000):
                    chunk_count += 1
                    if chunk_count % 20 == 0:
                        print(f"  å¤„ç†ç¬¬ {chunk_count} ä¸ªæ•°æ®å—...")
                    
                    # éªŒè¯å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
                    if 'UTC' not in chunk.columns:
                        print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {os.path.basename(file_path)} - ç¼ºå°‘UTCåˆ—")
                        continue
                    
                    # æ¸…æ´—åçš„æ•°æ®å·²ç»æœ‰äº†è½¬æ¢åçš„åæ ‡ï¼Œç›´æ¥ä½¿ç”¨
                    if 'lat' not in chunk.columns or 'lon' not in chunk.columns:
                        print(f"âš ï¸ æ–‡ä»¶ {os.path.basename(file_path)} ç¼ºå°‘è½¬æ¢åçš„åæ ‡åˆ—")
                        continue
                    
                    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                    chunk['UTC'] = pd.to_numeric(chunk['UTC'], errors='coerce')
                    chunk = chunk.dropna(subset=['UTC'])
                    
                    if len(chunk) == 0:
                        continue
                    
                    # è½¬æ¢ä¸ºå°æ—¶æ—¶é—´æˆ³
                    chunk['hour'] = (chunk['UTC'] // 3600) * 3600
                    
                    # æŒ‰å°æ—¶åˆ†ç»„
                    for hour, group in chunk.groupby('hour'):
                        # æ¯å°æ—¶æœ€å¤šä¿ç•™2ä¸‡ä¸ªç‚¹ï¼ˆæ¯”åŸç‰ˆæ›´å¤šï¼Œå› ä¸ºæ¸…æ´—åæ•°æ®è´¨é‡æ›´å¥½ï¼‰
                        if len(group) > 20000:
                            # åˆ†å±‚é‡‡æ ·ï¼šä¿æŒè½¦è¾†åˆ†å¸ƒ
                            if 'COMMADDR' in group.columns:
                                vehicle_groups = group.groupby('COMMADDR')
                                sample_per_vehicle = max(1, 20000 // len(vehicle_groups))
                                sampled_groups = []
                                for _, vg in vehicle_groups:
                                    if len(vg) <= sample_per_vehicle:
                                        sampled_groups.append(vg)
                                    else:
                                        sampled_groups.append(vg.sample(sample_per_vehicle, random_state=42))
                                group = pd.concat(sampled_groups, ignore_index=True)
                            else:
                                group = group.sample(20000, random_state=42)
                        
                        hourly_data[hour].append(group)
                    
                    total_records += len(chunk)
                
                print(f"  æ–‡ä»¶å¤„ç†å®Œæˆï¼Œæ€»è®°å½•æ•°: {total_records}")
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶ {os.path.basename(file_path)} æ—¶å‡ºé”™: {e}")
                continue
        
        # ä¿å­˜æ¯å°æ—¶çš„æ•°æ®
        saved_hours = 0
        for hour, data_list in hourly_data.items():
            if data_list:
                try:
                    hour_df = pd.concat(data_list, ignore_index=True)
                    filename = f"hour_{int(hour)}.parquet"
                    filepath = os.path.join(self.processed_dir, filename)
                    hour_df.to_parquet(filepath, compression='snappy')
                    
                    # è½¬æ¢æ—¶é—´æˆ³ä¸ºå¯è¯»æ ¼å¼ç”¨äºæ—¥å¿—
                    from datetime import datetime
                    readable_time = datetime.fromtimestamp(hour).strftime('%Y-%m-%d %H:%M')
                    print(f"âœ… ä¿å­˜å°æ—¶æ•°æ®: {filename} ({readable_time}), è®°å½•æ•°: {len(hour_df)}")
                    saved_hours += 1
                except Exception as e:
                    print(f"âŒ ä¿å­˜å°æ—¶æ•°æ®å¤±è´¥ (hour={hour}): {e}")
        
        print(f"âœ… æ—¶é—´åˆ†ç‰‡èšåˆå®Œæˆï¼Œä¿å­˜äº† {saved_hours} ä¸ªå°æ—¶çš„æ•°æ®")
    
    def _create_spatial_grids(self):
        """åˆ›å»ºåŸºäºæ¸…æ´—åæ•°æ®çš„ç©ºé—´ç½‘æ ¼ç´¢å¼•"""
        print("åˆ›å»ºç©ºé—´ç½‘æ ¼ç´¢å¼•...")
        
        # ä¸åŒåˆ†è¾¨ç‡çš„ç½‘æ ¼
        resolutions = [0.001, 0.002, 0.005, 0.01]  # å¢åŠ 0.002ç²¾åº¦ç”¨äºçƒ­åŠ›å›¾
        
        for resolution in resolutions:
            print(f"åˆ›å»º {resolution} åº¦åˆ†è¾¨ç‡ç½‘æ ¼...")
            grid_data = defaultdict(int)
            
            # è¯»å–æ‰€æœ‰å°æ—¶æ•°æ®
            processed_files = [f for f in os.listdir(self.processed_dir) if f.endswith('.parquet')]
            if not processed_files:
                print("âš ï¸ æœªæ‰¾åˆ°é¢„å¤„ç†çš„å°æ—¶æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ—¶é—´åˆ†ç‰‡èšåˆ")
                continue
            
            for filename in processed_files:
                filepath = os.path.join(self.processed_dir, filename)
                try:
                    df = pd.read_parquet(filepath)
                    
                    # ä½¿ç”¨æ¸…æ´—åæ•°æ®ä¸­å·²ç»è½¬æ¢å¥½çš„åæ ‡
                    if 'lat' in df.columns and 'lon' in df.columns:
                        # ç›´æ¥ä½¿ç”¨è½¬æ¢åçš„åæ ‡
                        df['lat_grid'] = (df['lat'] / resolution).round() * resolution
                        df['lng_grid'] = (df['lon'] / resolution).round() * resolution
                        
                        # ç»Ÿè®¡æ¯ä¸ªç½‘æ ¼çš„ç‚¹æ•°
                        grid_counts = df.groupby(['lat_grid', 'lng_grid']).size()
                        
                        for (lat, lng), count in grid_counts.items():
                            grid_key = f"{lat:.6f},{lng:.6f}"
                            grid_data[grid_key] += count
                    else:
                        print(f"âš ï¸ æ–‡ä»¶ {filename} ç¼ºå°‘åæ ‡åˆ—")
                        
                except Exception as e:
                    print(f"âŒ å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
            
            # ä¿å­˜ç½‘æ ¼æ•°æ®
            if grid_data:
                grid_filename = f"spatial_grid_{resolution}.json"
                grid_filepath = os.path.join(self.index_dir, grid_filename)
                with open(grid_filepath, 'w') as f:
                    # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
                    grid_data_serializable = convert_numpy_types(dict(grid_data))
                    json.dump(grid_data_serializable, f)
                
                print(f"âœ… ä¿å­˜ç©ºé—´ç½‘æ ¼: {grid_filename}, ç½‘æ ¼æ•°: {len(grid_data)}")
            else:
                print(f"âš ï¸ åˆ†è¾¨ç‡ {resolution} æœªç”Ÿæˆæœ‰æ•ˆç½‘æ ¼æ•°æ®")
    
    def _create_vehicle_indexes(self):
        """åˆ›å»ºåŸºäºæ¸…æ´—åæ•°æ®çš„è½¦è¾†ç´¢å¼•ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        print("åˆ›å»ºè½¦è¾†ç´¢å¼•...")
        
        vehicle_index = defaultdict(list)  # vehicle_id -> [æ—¶é—´æ®µåˆ—è¡¨]
        vehicle_stats = defaultdict(lambda: {'records': 0, 'time_span': [float('inf'), 0]})
        
        processed_files = [f for f in os.listdir(self.processed_dir) if f.endswith('.parquet')]
        total_files = len(processed_files)
        
        for i, filename in enumerate(processed_files):
            print(f"å¤„ç†è½¦è¾†ç´¢å¼• ({i+1}/{total_files}): {filename}")
            
            try:
                hour = int(filename.split('_')[1].split('.')[0])
                filepath = os.path.join(self.processed_dir, filename)
                df = pd.read_parquet(filepath)
                
                if 'COMMADDR' in df.columns:
                    # ä¼˜åŒ–ï¼šä½¿ç”¨groupbyä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰è½¦è¾†ï¼Œé¿å…åå¤ç­›é€‰
                    vehicle_groups = df.groupby('COMMADDR')
                    
                    for vehicle_id, vehicle_data in vehicle_groups:
                        vehicle_str = str(vehicle_id)
                        vehicle_index[vehicle_str].append(hour)
                        
                        # ç»Ÿè®¡è½¦è¾†ä¿¡æ¯
                        record_count = len(vehicle_data)
                        vehicle_stats[vehicle_str]['records'] += record_count
                        
                        if 'UTC' in vehicle_data.columns:
                            min_time = vehicle_data['UTC'].min()
                            max_time = vehicle_data['UTC'].max()
                            vehicle_stats[vehicle_str]['time_span'][0] = min(
                                vehicle_stats[vehicle_str]['time_span'][0], min_time
                            )
                            vehicle_stats[vehicle_str]['time_span'][1] = max(
                                vehicle_stats[vehicle_str]['time_span'][1], max_time
                            )
                    
                    print(f"  å¤„ç†äº† {len(vehicle_groups)} ä¸ªè½¦è¾†çš„æ•°æ®")
                            
            except Exception as e:
                print(f"âŒ å¤„ç†è½¦è¾†ç´¢å¼•æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
        
        # ä¿å­˜è½¦è¾†ç´¢å¼•
        if vehicle_index:
            print("ä¿å­˜è½¦è¾†ç´¢å¼•æ–‡ä»¶...")
            index_filepath = os.path.join(self.index_dir, 'vehicle_index.json')
            with open(index_filepath, 'w') as f:
                # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
                vehicle_index_serializable = convert_numpy_types(dict(vehicle_index))
                json.dump(vehicle_index_serializable, f)
            
            # ä¿å­˜è½¦è¾†ç»Ÿè®¡ä¿¡æ¯
            print("ä¿å­˜è½¦è¾†ç»Ÿè®¡ä¿¡æ¯...")
            stats_filepath = os.path.join(self.index_dir, 'vehicle_stats.json')
            with open(stats_filepath, 'w') as f:
                # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
                vehicle_stats_serializable = convert_numpy_types(dict(vehicle_stats))
                json.dump(vehicle_stats_serializable, f)
            
            print(f"âœ… è½¦è¾†ç´¢å¼•åˆ›å»ºå®Œæˆï¼Œç´¢å¼•è½¦è¾†æ•°: {len(vehicle_index)}")
            
            # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
            if vehicle_stats:
                record_counts = [stats['records'] for stats in vehicle_stats.values()]
                print(f"   è½¦è¾†è®°å½•æ•°ç»Ÿè®¡: å¹³å‡ {np.mean(record_counts):.0f}, "
                      f"æœ€å¤š {max(record_counts)}, æœ€å°‘ {min(record_counts)}")
        else:
            print("âš ï¸ æœªç”Ÿæˆè½¦è¾†ç´¢å¼•æ•°æ®")
    
    def _create_heatmap_precomputed(self):
        """åŸºäºæ¸…æ´—åæ•°æ®é¢„è®¡ç®—çƒ­åŠ›å›¾"""
        print("é¢„è®¡ç®—çƒ­åŠ›å›¾æ•°æ®...")
        
        # æŒ‰å¤©å’ŒæŒ‰å°æ—¶é¢„è®¡ç®—çƒ­åŠ›å›¾
        daily_heatmaps = defaultdict(lambda: defaultdict(int))
        hourly_heatmaps = defaultdict(lambda: defaultdict(int))
        
        processed_files = [f for f in os.listdir(self.processed_dir) if f.endswith('.parquet')]
        
        for filename in processed_files:
            try:
                hour = int(filename.split('_')[1].split('.')[0])
                day = hour // (24 * 3600) * (24 * 3600)  # æ‰€å±å¤©
                
                filepath = os.path.join(self.processed_dir, filename)
                df = pd.read_parquet(filepath)
                
                # ä½¿ç”¨æ¸…æ´—åæ•°æ®çš„è½¬æ¢åæ ‡
                if 'lat' in df.columns and 'lon' in df.columns:
                    resolution = 0.002  # çƒ­åŠ›å›¾ä½¿ç”¨ä¸­ç­‰åˆ†è¾¨ç‡
                    df['lat_grid'] = (df['lat'] / resolution).round() * resolution
                    df['lng_grid'] = (df['lon'] / resolution).round() * resolution
                    
                    # ç»Ÿè®¡ç½‘æ ¼å¯†åº¦
                    grid_counts = df.groupby(['lat_grid', 'lng_grid']).size()
                    
                    for (lat, lng), count in grid_counts.items():
                        grid_key = f"{lat:.6f},{lng:.6f}"
                        daily_heatmaps[day][grid_key] += count
                        hourly_heatmaps[hour][grid_key] += count
                        
            except Exception as e:
                print(f"âŒ å¤„ç†çƒ­åŠ›å›¾æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
        
        # ä¿å­˜æ¯æ—¥çƒ­åŠ›å›¾
        saved_daily = 0
        for day, heatmap_data in daily_heatmaps.items():
            if heatmap_data:
                try:
                    filename = f"heatmap_day_{int(day)}.json"
                    filepath = os.path.join(self.index_dir, filename)
                    with open(filepath, 'w') as f:
                        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
                        heatmap_data_serializable = convert_numpy_types(dict(heatmap_data))
                        json.dump(heatmap_data_serializable, f)
                    
                    from datetime import datetime
                    readable_date = datetime.fromtimestamp(day).strftime('%Y-%m-%d')
                    print(f"âœ… ä¿å­˜æ—¥çƒ­åŠ›å›¾: {filename} ({readable_date}), ç½‘æ ¼æ•°: {len(heatmap_data)}")
                    saved_daily += 1
                except Exception as e:
                    print(f"âŒ ä¿å­˜æ—¥çƒ­åŠ›å›¾å¤±è´¥ (day={day}): {e}")
        
        # ä¿å­˜æ¯å°æ—¶çƒ­åŠ›å›¾ï¼ˆç”¨äºæ›´ç²¾ç»†çš„æŸ¥è¯¢ï¼‰
        saved_hourly = 0
        for hour, heatmap_data in hourly_heatmaps.items():
            if heatmap_data:
                try:
                    filename = f"heatmap_hour_{int(hour)}.json"
                    filepath = os.path.join(self.index_dir, filename)
                    with open(filepath, 'w') as f:
                        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
                        heatmap_data_serializable = convert_numpy_types(dict(heatmap_data))
                        json.dump(heatmap_data_serializable, f)
                    saved_hourly += 1
                except Exception as e:
                    print(f"âŒ ä¿å­˜å°æ—¶çƒ­åŠ›å›¾å¤±è´¥ (hour={hour}): {e}")
        
        print(f"âœ… çƒ­åŠ›å›¾é¢„è®¡ç®—å®Œæˆ: {saved_daily} ä¸ªæ—¥çƒ­åŠ›å›¾, {saved_hourly} ä¸ªå°æ—¶çƒ­åŠ›å›¾")
    
    def _create_data_summary(self):
        """ç”Ÿæˆæ•°æ®æ¦‚è¦ç»Ÿè®¡"""
        print("ç”Ÿæˆæ•°æ®æ¦‚è¦ç»Ÿè®¡...")
        
        summary = {
            'preprocessing_time': pd.Timestamp.now().isoformat(),
            'source_directory': self.data_dir,
            'total_hours': 0,
            'total_records': 0,
            'total_vehicles': 0,
            'time_range': {'start': None, 'end': None},
            'coordinate_range': {'lat_min': float('inf'), 'lat_max': float('-inf'),
                               'lng_min': float('inf'), 'lng_max': float('-inf')},
            'files_processed': []
        }
        
        # ç»Ÿè®¡é¢„å¤„ç†ç»“æœ
        processed_files = [f for f in os.listdir(self.processed_dir) if f.endswith('.parquet')]
        
        all_times = []
        all_lats = []
        all_lngs = []
        
        for filename in processed_files:
            try:
                filepath = os.path.join(self.processed_dir, filename)
                df = pd.read_parquet(filepath)
                
                summary['total_records'] += len(df)
                
                if 'UTC' in df.columns:
                    times = df['UTC'].dropna()
                    all_times.extend(times.tolist())
                
                if 'lat' in df.columns and 'lon' in df.columns:
                    lats = df['lat'].dropna()
                    lngs = df['lon'].dropna()
                    all_lats.extend(lats.tolist())
                    all_lngs.extend(lngs.tolist())
                
                if 'COMMADDR' in df.columns:
                    unique_vehicles = df['COMMADDR'].nunique()
                    summary['total_vehicles'] = max(summary['total_vehicles'], unique_vehicles)
                    
            except Exception as e:
                print(f"âŒ ç»Ÿè®¡æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
        
        # æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
        summary['total_hours'] = len(processed_files)
        
        if all_times:
            summary['time_range']['start'] = min(all_times)
            summary['time_range']['end'] = max(all_times)
        
        if all_lats and all_lngs:
            summary['coordinate_range'] = {
                'lat_min': min(all_lats),
                'lat_max': max(all_lats),
                'lng_min': min(all_lngs),
                'lng_max': max(all_lngs)
            }
        
        # è®°å½•åŸå§‹æ–‡ä»¶ä¿¡æ¯
        for file_path in self._get_cleaned_csv_files():
            file_size = os.path.getsize(file_path) / (1024 * 1024)
            summary['files_processed'].append({
                'filename': os.path.basename(file_path),
                'size_mb': round(file_size, 2)
            })
        
        # ä¿å­˜æ¦‚è¦ç»Ÿè®¡
        summary_filepath = os.path.join(self.index_dir, 'data_summary.json')
        with open(summary_filepath, 'w') as f:
            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            summary_serializable = convert_numpy_types(summary)
            json.dump(summary_serializable, f, indent=2)
        
        print(f"âœ… æ•°æ®æ¦‚è¦ç»Ÿè®¡å®Œæˆ:")
        print(f"   æ€»è®°å½•æ•°: {summary['total_records']:,}")
        print(f"   æ€»è½¦è¾†æ•°: {summary['total_vehicles']:,}")
        print(f"   æ—¶é—´è·¨åº¦: {summary['total_hours']} å°æ—¶")
        if summary['time_range']['start']:
            from datetime import datetime
            start_time = datetime.fromtimestamp(summary['time_range']['start'])
            end_time = datetime.fromtimestamp(summary['time_range']['end'])
            print(f"   æ—¶é—´èŒƒå›´: {start_time} ~ {end_time}")
    
    def _get_cleaned_csv_files(self) -> List[str]:
        """è·å–æ¸…æ´—åæ•°æ®ç›®å½•ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶"""
        if not os.path.exists(self.data_dir):
            print(f"âŒ æ¸…æ´—åæ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            return []
        
        csv_files = []
        for filename in os.listdir(self.data_dir):
            if filename.endswith('.csv'):
                file_path = os.path.join(self.data_dir, filename)
                csv_files.append(file_path)
        
        return sorted(csv_files)
    
    def clean_existing_preprocessed_data(self):
        """æ¸…ç†ç°æœ‰çš„é¢„å¤„ç†æ•°æ®"""
        print("æ¸…ç†ç°æœ‰çš„é¢„å¤„ç†æ•°æ®...")
        
        # æ¸…ç†processedç›®å½•
        if os.path.exists(self.processed_dir):
            for filename in os.listdir(self.processed_dir):
                if filename.endswith('.parquet'):
                    file_path = os.path.join(self.processed_dir, filename)
                    os.remove(file_path)
                    print(f"åˆ é™¤: {filename}")
        
        # æ¸…ç†indexesç›®å½•
        if os.path.exists(self.index_dir):
            for filename in os.listdir(self.index_dir):
                if filename.endswith('.json'):
                    file_path = os.path.join(self.index_dir, filename)
                    os.remove(file_path)
                    print(f"åˆ é™¤: {filename}")
        
        print("âœ… æ¸…ç†å®Œæˆ")

# ä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œåˆ›å»ºä¸€ä¸ªå¢å¼ºç‰ˆçš„å¿«é€ŸåŠ è½½å™¨
class EnhancedFastTrafficDataLoader:
    """
    å¢å¼ºç‰ˆå¿«é€Ÿäº¤é€šæ•°æ®åŠ è½½å™¨
    æ”¯æŒæ¸…æ´—åæ•°æ®çš„é¢„å¤„ç†ç»“æœ
    """
    
    def __init__(self, data_dir: str = None):
        if data_dir is None:
            self.data_dir = os.path.join(os.path.dirname(__file__), 'data')
        else:
            self.data_dir = data_dir
        
        self.processed_dir = os.path.join(self.data_dir, 'processed')
        self.index_dir = os.path.join(self.data_dir, 'indexes')
        
        # åŠ è½½ç´¢å¼•
        self.vehicle_index = self._load_vehicle_index()
        self.vehicle_stats = self._load_vehicle_stats()
        self.spatial_grids = self._load_spatial_grids()
        self.data_summary = self._load_data_summary()
        
        print(f"ğŸ“Š æ•°æ®æ¦‚è¦: {self.data_summary.get('total_records', 0):,} æ¡è®°å½•, "
              f"{self.data_summary.get('total_vehicles', 0):,} è¾†è½¦, "
              f"{self.data_summary.get('total_hours', 0)} å°æ—¶")
    
    def _load_vehicle_index(self) -> Dict:
        """åŠ è½½è½¦è¾†ç´¢å¼•"""
        index_filepath = os.path.join(self.index_dir, 'vehicle_index.json')
        if os.path.exists(index_filepath):
            with open(index_filepath, 'r') as f:
                return json.load(f)
        return {}
    
    def _load_vehicle_stats(self) -> Dict:
        """åŠ è½½è½¦è¾†ç»Ÿè®¡ä¿¡æ¯"""
        stats_filepath = os.path.join(self.index_dir, 'vehicle_stats.json')
        if os.path.exists(stats_filepath):
            with open(stats_filepath, 'r') as f:
                return json.load(f)
        return {}
    
    def _load_spatial_grids(self) -> Dict:
        """åŠ è½½ç©ºé—´ç½‘æ ¼"""
        grids = {}
        if not os.path.exists(self.index_dir):
            return grids
            
        for filename in os.listdir(self.index_dir):
            if filename.startswith('spatial_grid_') and filename.endswith('.json'):
                try:
                    resolution = float(filename.split('_')[2].split('.')[0])
                    filepath = os.path.join(self.index_dir, filename)
                    with open(filepath, 'r') as f:
                        grids[resolution] = json.load(f)
                except:
                    continue
        return grids
    
    def _load_data_summary(self) -> Dict:
        """åŠ è½½æ•°æ®æ¦‚è¦"""
        summary_filepath = os.path.join(self.index_dir, 'data_summary.json')
        if os.path.exists(summary_filepath):
            with open(summary_filepath, 'r') as f:
                return json.load(f)
        return {}
    
    def get_data_summary(self) -> Dict:
        """è·å–æ•°æ®æ¦‚è¦ç»Ÿè®¡"""
        return self.data_summary
    
    def get_vehicle_stats(self, vehicle_id: str = None) -> Dict:
        """è·å–è½¦è¾†ç»Ÿè®¡ä¿¡æ¯"""
        if vehicle_id:
            return self.vehicle_stats.get(str(vehicle_id), {})
        else:
            return self.vehicle_stats
    
    def fast_load_data(self, start_time: float, end_time: float, vehicle_id: str = None) -> pd.DataFrame:
        """å¿«é€ŸåŠ è½½æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print(f"âš¡ å¿«é€ŸåŠ è½½æ•°æ®: {start_time} - {end_time}")
        
        # è®¡ç®—éœ€è¦çš„æ—¶é—´æ®µ
        start_hour = (int(start_time) // 3600) * 3600
        end_hour = (int(end_time) // 3600) * 3600
        
        # å¦‚æœæŒ‡å®šäº†è½¦è¾†IDï¼Œå…ˆæ£€æŸ¥è½¦è¾†ç´¢å¼•
        if vehicle_id and vehicle_id in self.vehicle_index:
            available_hours = set(self.vehicle_index[vehicle_id])
            print(f"ğŸš— è½¦è¾† {vehicle_id} åœ¨ {len(available_hours)} ä¸ªå°æ—¶æ®µæœ‰æ•°æ®")
        else:
            available_hours = None
        
        # åŠ è½½ç›¸å…³æ—¶é—´æ®µçš„æ•°æ®
        data_frames = []
        current_hour = start_hour
        loaded_hours = 0
        
        while current_hour <= end_hour:
            if available_hours is None or current_hour in available_hours:
                filename = f"hour_{int(current_hour)}.parquet"
                filepath = os.path.join(self.processed_dir, filename)
                
                if os.path.exists(filepath):
                    try:
                        df = pd.read_parquet(filepath)
                        
                        # ç²¾ç¡®æ—¶é—´è¿‡æ»¤
                        df = df[(df['UTC'] >= start_time) & (df['UTC'] <= end_time)]
                        
                        # è½¦è¾†IDè¿‡æ»¤
                        if vehicle_id:
                            df['COMMADDR'] = df['COMMADDR'].astype(str)
                            df = df[df['COMMADDR'] == str(vehicle_id)]
                        
                        if not df.empty:
                            data_frames.append(df)
                            loaded_hours += 1
                            
                    except Exception as e:
                        print(f"âš ï¸ åŠ è½½æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
            
            current_hour += 3600  # ä¸‹ä¸€å°æ—¶
        
        # åˆå¹¶æ•°æ®
        if data_frames:
            result = pd.concat(data_frames, ignore_index=True)
            print(f"âœ… å¿«é€ŸåŠ è½½å®Œæˆ: {len(result):,} æ¡è®°å½• (æ¥è‡ª {loaded_hours} ä¸ªå°æ—¶æ®µ)")
            return result
        else:
            print("ğŸ“­ æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®")
            return pd.DataFrame()
    
    def fast_get_heatmap(self, start_time: float, end_time: float, resolution: str = "daily") -> List[Dict]:
        """å¿«é€Ÿè·å–çƒ­åŠ›å›¾æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print(f"ğŸ—ºï¸ å¿«é€Ÿç”Ÿæˆçƒ­åŠ›å›¾: {resolution} æ¨¡å¼")
        
        combined_heatmap = defaultdict(int)
        
        if resolution == "daily":
            # ä½¿ç”¨é¢„è®¡ç®—çš„æ—¥çƒ­åŠ›å›¾
            start_day = (int(start_time) // (24 * 3600)) * (24 * 3600)
            end_day = (int(end_time) // (24 * 3600)) * (24 * 3600)
            
            current_day = start_day
            loaded_days = 0
            
            while current_day <= end_day:
                filename = f"heatmap_day_{int(current_day)}.json"
                filepath = os.path.join(self.index_dir, filename)
                
                if os.path.exists(filepath):
                    try:
                        with open(filepath, 'r') as f:
                            day_heatmap = json.load(f)
                        
                        for grid_key, count in day_heatmap.items():
                            combined_heatmap[grid_key] += count
                        
                        loaded_days += 1
                    except Exception as e:
                        print(f"âš ï¸ åŠ è½½æ—¥çƒ­åŠ›å›¾ {filename} æ—¶å‡ºé”™: {e}")
                
                current_day += 24 * 3600  # ä¸‹ä¸€å¤©
            
            print(f"ğŸ“Š åŠ è½½äº† {loaded_days} å¤©çš„çƒ­åŠ›å›¾æ•°æ®")
            
        else:  # hourly
            # ä½¿ç”¨é¢„è®¡ç®—çš„å°æ—¶çƒ­åŠ›å›¾
            start_hour = (int(start_time) // 3600) * 3600
            end_hour = (int(end_time) // 3600) * 3600
            
            current_hour = start_hour
            loaded_hours = 0
            
            while current_hour <= end_hour:
                filename = f"heatmap_hour_{int(current_hour)}.json"
                filepath = os.path.join(self.index_dir, filename)
                
                if os.path.exists(filepath):
                    try:
                        with open(filepath, 'r') as f:
                            hour_heatmap = json.load(f)
                        
                        for grid_key, count in hour_heatmap.items():
                            combined_heatmap[grid_key] += count
                        
                        loaded_hours += 1
                    except Exception as e:
                        print(f"âš ï¸ åŠ è½½å°æ—¶çƒ­åŠ›å›¾ {filename} æ—¶å‡ºé”™: {e}")
                
                current_hour += 3600  # ä¸‹ä¸€å°æ—¶
            
            print(f"ğŸ“Š åŠ è½½äº† {loaded_hours} å°æ—¶çš„çƒ­åŠ›å›¾æ•°æ®")
        
        # è½¬æ¢ä¸ºçƒ­åŠ›å›¾ç‚¹æ ¼å¼
        heatmap_points = []
        for grid_key, count in combined_heatmap.items():
            try:
                lat, lng = map(float, grid_key.split(','))
                heatmap_points.append({
                    'lat': lat,
                    'lng': lng,
                    'count': count
                })
            except:
                continue
        
        # æŒ‰å¯†åº¦æ’åºï¼Œå–å‰10000ä¸ªç‚¹
        heatmap_points.sort(key=lambda x: x['count'], reverse=True)
        result = heatmap_points[:10000]
        
        print(f"ğŸ—ºï¸ çƒ­åŠ›å›¾ç”Ÿæˆå®Œæˆ: {len(result)} ä¸ªç‚¹")
        return result

if __name__ == "__main__":
    print("ğŸš€ åŸºäºæ¸…æ´—åæ•°æ®çš„é¢„å¤„ç†å™¨")
    print("=" * 60)
    
    # åˆ›å»ºé¢„å¤„ç†å™¨å®ä¾‹
    preprocessor = CleanedTrafficDataPreprocessor()
    
    # å¯é€‰ï¼šæ¸…ç†ç°æœ‰çš„é¢„å¤„ç†æ•°æ®
    response = input("æ˜¯å¦æ¸…ç†ç°æœ‰çš„é¢„å¤„ç†æ•°æ®ï¼Ÿ(y/N): ")
    if response.lower() == 'y':
        preprocessor.clean_existing_preprocessed_data()
    
    # è¿è¡Œé¢„å¤„ç†
    print("\nå¼€å§‹é¢„å¤„ç†æ¸…æ´—åçš„æ•°æ®...")
    preprocessor.preprocess_all_data()
    
    # æµ‹è¯•å¢å¼ºç‰ˆå¿«é€ŸåŠ è½½å™¨
    print("\n" + "=" * 60)
    print("æµ‹è¯•å¢å¼ºç‰ˆå¿«é€ŸåŠ è½½å™¨")
    print("=" * 60)
    
    loader = EnhancedFastTrafficDataLoader()
    
    # æ˜¾ç¤ºæ•°æ®æ¦‚è¦
    summary = loader.get_data_summary()
    if summary:
        print("ğŸ“Š æ•°æ®æ¦‚è¦:")
        print(f"   å¤„ç†æ—¶é—´: {summary.get('preprocessing_time', 'Unknown')}")
        print(f"   æ•°æ®æ–‡ä»¶: {len(summary.get('files_processed', []))} ä¸ª")
        print(f"   æ€»è®°å½•æ•°: {summary.get('total_records', 0):,}")
        print(f"   æ€»è½¦è¾†æ•°: {summary.get('total_vehicles', 0):,}")
        
        time_range = summary.get('time_range', {})
        if time_range.get('start'):
            from datetime import datetime
            start = datetime.fromtimestamp(time_range['start'])
            end = datetime.fromtimestamp(time_range['end'])
            print(f"   æ—¶é—´èŒƒå›´: {start} ~ {end}")
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    if summary and summary.get('time_range', {}).get('start'):
        test_start = summary['time_range']['start']
        test_end = test_start + 3600  # 1å°æ—¶èŒƒå›´
        
        print(f"\næµ‹è¯•æ•°æ®åŠ è½½ (1å°æ—¶èŒƒå›´)...")
        test_data = loader.fast_load_data(test_start, test_end)
        print(f"æµ‹è¯•ç»“æœ: {len(test_data)} æ¡è®°å½•")
        
        # æµ‹è¯•çƒ­åŠ›å›¾
        print(f"\næµ‹è¯•çƒ­åŠ›å›¾ç”Ÿæˆ...")
        test_heatmap = loader.fast_get_heatmap(test_start, test_end, "daily")
        print(f"çƒ­åŠ›å›¾ç‚¹æ•°: {len(test_heatmap)}")
    
    print("\nâœ… é¢„å¤„ç†å’Œæµ‹è¯•å®Œæˆï¼")