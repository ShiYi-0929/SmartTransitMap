import pandas as pd
import numpy as np
import os
import json
from typing import List, Dict, Tuple, Optional, Union, Any
from datetime import datetime
import math
from collections import defaultdict
from .models import HeatmapPoint, TrackPoint, VehicleTrack
import logging

class TrafficDataProcessor:
    """äº¤é€šæ•°æ®å¤„ç†ç±»ï¼Œè´Ÿè´£åŠ è½½ã€å¤„ç†å’Œè½¬æ¢äº¤é€šæ•°æ®"""
    
    def __init__(self, data_dir: str = None):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        if data_dir is None:
            # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
            current_dir = os.path.dirname(os.path.abspath(__file__))
            # å°è¯•å¤šç§å¯èƒ½çš„æ•°æ®è·¯å¾„
            possible_paths = [
                os.path.join(current_dir, 'data'),
                os.path.join(current_dir, '..', '..', 'data'),
                os.path.join(current_dir, '..', '..', '..', 'data')
            ]
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå­˜åœ¨çš„è·¯å¾„
            for path in possible_paths:
                if os.path.exists(path) and os.path.isdir(path):
                    self.data_dir = path
                    break
            else:
                # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
                self.data_dir = os.path.join(current_dir, 'data')
        else:
            self.data_dir = data_dir
            
        print(f"æ•°æ®ç›®å½•: {self.data_dir}")
        
        # é¢„å¤„ç†æ•°æ®ç›®å½•
        self.processed_dir = os.path.join(self.data_dir, 'processed')
        self.index_dir = os.path.join(self.data_dir, 'indexes')
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é¢„å¤„ç†æ•°æ®
        self.use_preprocessed = (
            os.path.exists(self.processed_dir) and 
            os.path.exists(self.index_dir) and
            len(os.listdir(self.processed_dir)) > 0
        )
        
        if self.use_preprocessed:
            print("âœ“ å‘ç°é¢„å¤„ç†æ•°æ®ï¼Œå°†ä½¿ç”¨é«˜æ•ˆæŸ¥è¯¢æ¨¡å¼")
            self._load_indexes()
        else:
            print("âœ— æœªå‘ç°é¢„å¤„ç†æ•°æ®ï¼Œå°†ä½¿ç”¨åŸå§‹CSVæ–‡ä»¶ï¼ˆè¾ƒæ…¢ï¼‰")
        
        # å¢å¼ºçš„ç¼“å­˜ç³»ç»Ÿ
        self._cached_data = {}
        self._cache_maxsize = 10  # å¢åŠ ç¼“å­˜å¤§å°
        self._sample_cache = {}   # é‡‡æ ·æ•°æ®ç¼“å­˜
        self._heatmap_cache = {}  # çƒ­åŠ›å›¾ç¼“å­˜
        self._csv_files = None
        
        # æ€§èƒ½ä¼˜åŒ–å‚æ•°
        self.max_data_points = 50000  # å•æ¬¡æŸ¥è¯¢æœ€å¤§æ•°æ®ç‚¹æ•°
        self.sample_ratio = 0.2       # æ•°æ®é‡‡æ ·æ¯”ä¾‹
        self.enable_sampling = True   # å¯ç”¨æ™ºèƒ½é‡‡æ ·
    
    def _load_indexes(self):
        """åŠ è½½é¢„å¤„ç†çš„ç´¢å¼•æ•°æ®"""
        try:
            # åŠ è½½è½¦è¾†ç´¢å¼•
            vehicle_index_path = os.path.join(self.index_dir, 'vehicle_index.json')
            if os.path.exists(vehicle_index_path):
                with open(vehicle_index_path, 'r') as f:
                    self.vehicle_index = json.load(f)
                print(f"âœ“ åŠ è½½è½¦è¾†ç´¢å¼•: {len(self.vehicle_index)} ä¸ªè½¦è¾†")
            else:
                self.vehicle_index = {}
            
            # åŠ è½½ç©ºé—´ç½‘æ ¼
            self.spatial_grids = {}
            for filename in os.listdir(self.index_dir):
                if filename.startswith('spatial_grid_') and filename.endswith('.json'):
                    resolution = float(filename.split('_')[2].split('.')[0])
                    filepath = os.path.join(self.index_dir, filename)
                    with open(filepath, 'r') as f:
                        self.spatial_grids[resolution] = json.load(f)
                    print(f"âœ“ åŠ è½½ç©ºé—´ç½‘æ ¼ ({resolution}): {len(self.spatial_grids[resolution])} ä¸ªç½‘æ ¼")
            
        except Exception as e:
            print(f"è­¦å‘Šï¼šåŠ è½½ç´¢å¼•æ—¶å‡ºé”™: {e}")
            self.use_preprocessed = False

    def get_csv_files(self) -> List[str]:
        """è·å–æ•°æ®ç›®å½•ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶"""
        if self._csv_files is None:
            # è·å–æ‰€æœ‰CSVæ–‡ä»¶
            self._csv_files = [
                os.path.join(self.data_dir, f) 
                for f in os.listdir(self.data_dir) 
                if f.endswith('.csv')
            ]
        return self._csv_files
    
    def load_data(self, start_time: float, end_time: float, vehicle_id: str = None) -> pd.DataFrame:
        """
        åŠ è½½æŒ‡å®šæ—¶é—´èŒƒå›´å’Œè½¦è¾†IDçš„æ•°æ®ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            start_time: å¼€å§‹æ—¶é—´æˆ³
            end_time: ç»“æŸæ—¶é—´æˆ³
            vehicle_id: è½¦è¾†IDï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½æ‰€æœ‰è½¦è¾†æ•°æ®
            
        Returns:
            ç¬¦åˆæ¡ä»¶çš„æ•°æ®DataFrame
        """
        print(f"âš¡ å¿«é€ŸåŠ è½½æ•°æ®: {start_time} åˆ° {end_time}")
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"data_{start_time}_{end_time}_{vehicle_id}_{self.sample_ratio}"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self._cached_data:
            print("ğŸ“¦ ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼ˆç§’çº§å“åº”ï¼‰")
            return self._cached_data[cache_key]
        
        # ä½¿ç”¨é¢„å¤„ç†æ•°æ®è¿›è¡Œå¿«é€ŸæŸ¥è¯¢
        if self.use_preprocessed:
            result = self._load_data_fast(start_time, end_time, vehicle_id)
        else:
            result = self._load_data_legacy(start_time, end_time, vehicle_id)
        
        # æ™ºèƒ½é‡‡æ ·ä»¥æé«˜æ€§èƒ½
        if self.enable_sampling and len(result) > self.max_data_points:
            print(f"ğŸ”„ æ•°æ®é‡è¿‡å¤§({len(result)}æ¡)ï¼Œè¿›è¡Œæ™ºèƒ½é‡‡æ ·...")
            result = self._smart_sample_data(result, vehicle_id)
        
        # æ›´æ–°ç¼“å­˜ï¼ˆLRUç­–ç•¥ï¼‰
        if len(self._cached_data) >= self._cache_maxsize:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜
            oldest_key = next(iter(self._cached_data))
            del self._cached_data[oldest_key]
        
        self._cached_data[cache_key] = result
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(result)} æ¡è®°å½•")
        return result
    
    def _smart_sample_data(self, df: pd.DataFrame, vehicle_id: str = None) -> pd.DataFrame:
        """æ™ºèƒ½æ•°æ®é‡‡æ ·ï¼Œä¿æŒæ•°æ®åˆ†å¸ƒç‰¹æ€§"""
        if df.empty:
            return df
        
        # å¦‚æœæŒ‡å®šäº†è½¦è¾†IDï¼Œä¿ç•™æ‰€æœ‰è¯¥è½¦è¾†çš„æ•°æ®
        if vehicle_id:
            vehicle_data = df[df['COMMADDR'].astype(str) == str(vehicle_id)]
            if len(vehicle_data) <= self.max_data_points:
                return vehicle_data
            else:
                # å¯¹å•è½¦æ•°æ®è¿›è¡Œæ—¶é—´å‡åŒ€é‡‡æ ·
                n_samples = min(self.max_data_points, len(vehicle_data))
                indices = np.linspace(0, len(vehicle_data)-1, n_samples, dtype=int)
                return vehicle_data.iloc[indices].copy()
        
        # å¯¹æ‰€æœ‰è½¦è¾†è¿›è¡Œåˆ†å±‚é‡‡æ ·
        target_size = int(self.max_data_points * self.sample_ratio)
        
        # æŒ‰è½¦è¾†IDåˆ†ç»„é‡‡æ ·
        sampled_groups = []
        vehicle_groups = df.groupby('COMMADDR')
        
        # è®¡ç®—æ¯ä¸ªè½¦è¾†åº”è¯¥é‡‡æ ·çš„æ•°é‡
        n_vehicles = len(vehicle_groups)
        samples_per_vehicle = max(1, target_size // n_vehicles)
        
        for vehicle_id, group in vehicle_groups:
            if len(group) <= samples_per_vehicle:
                sampled_groups.append(group)
            else:
                # æ—¶é—´å‡åŒ€é‡‡æ ·
                indices = np.linspace(0, len(group)-1, samples_per_vehicle, dtype=int)
                sampled_groups.append(group.iloc[indices])
        
        result = pd.concat(sampled_groups, ignore_index=True)
        print(f"   é‡‡æ ·ç»“æœ: {len(result)} æ¡è®°å½• ({len(df)} -> {len(result)})")
        return result

    def _load_data_fast(self, start_time: float, end_time: float, vehicle_id: str = None) -> pd.DataFrame:
        """ä½¿ç”¨é¢„å¤„ç†æ•°æ®è¿›è¡Œå¿«é€ŸåŠ è½½ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        print("ğŸš€ ä½¿ç”¨é¢„å¤„ç†æ•°æ®è¿›è¡Œæé€ŸæŸ¥è¯¢...")
        
        # è®¡ç®—éœ€è¦çš„æ—¶é—´æ®µï¼ˆå°æ—¶çº§åˆ«ï¼‰
        start_hour = (int(start_time) // 3600) * 3600
        end_hour = (int(end_time) // 3600) * 3600
        
        # å¦‚æœæŒ‡å®šäº†è½¦è¾†IDï¼Œå…ˆæ£€æŸ¥è½¦è¾†ç´¢å¼•
        if vehicle_id and vehicle_id in self.vehicle_index:
            available_hours = set(self.vehicle_index[vehicle_id])
            print(f"ğŸš— è½¦è¾† {vehicle_id} åœ¨ {len(available_hours)} ä¸ªæ—¶é—´æ®µå‡ºç°")
        else:
            available_hours = None
        
        # é™åˆ¶æœ€å¤§æ—¶é—´èŒƒå›´ï¼ˆé˜²æ­¢åŠ è½½è¿‡å¤šæ•°æ®ï¼‰
        max_hours = 24  # æœ€å¤šåŠ è½½24å°æ—¶çš„æ•°æ®
        time_span_hours = (end_hour - start_hour) // 3600
        
        if time_span_hours > max_hours:
            print(f"âš ï¸  æ—¶é—´èŒƒå›´è¿‡å¤§({time_span_hours}å°æ—¶)ï¼Œé™åˆ¶ä¸º{max_hours}å°æ—¶")
            end_hour = start_hour + max_hours * 3600
        
        # åŠ è½½ç›¸å…³æ—¶é—´æ®µçš„æ•°æ®
        data_frames = []
        current_hour = start_hour
        processed_count = 0
        max_files = 6  # é™åˆ¶æœ€å¤§æ–‡ä»¶æ•°
        file_count = 0
        
        while current_hour <= end_hour and file_count < max_files:
            if available_hours is None or current_hour in available_hours:
                filename = f"hour_{int(current_hour)}.parquet"
                filepath = os.path.join(self.processed_dir, filename)
                
                if os.path.exists(filepath):
                    try:
                        # ä½¿ç”¨åˆ—é€‰æ‹©ä¼˜åŒ–
                        columns_to_load = ['UTC', 'COMMADDR', 'LAT', 'LON', 'SPEED']
                        df = pd.read_parquet(filepath, columns=columns_to_load)
                        
                        # ç²¾ç¡®æ—¶é—´è¿‡æ»¤
                        df = df[(df['UTC'] >= start_time) & (df['UTC'] <= end_time)]
                        
                        # è½¦è¾†IDè¿‡æ»¤
                        if vehicle_id:
                            df['COMMADDR'] = df['COMMADDR'].astype(str)
                            df = df[df['COMMADDR'] == str(vehicle_id)]
                        
                        if not df.empty:
                            data_frames.append(df)
                            processed_count += len(df)
                            print(f"   ğŸ“ {filename}: {len(df)} æ¡è®°å½•")
                            file_count += 1
                        
                    except Exception as e:
                        print(f"âŒ è¯»å– {filename} æ—¶å‡ºé”™: {e}")
            
            current_hour += 3600  # ä¸‹ä¸€å°æ—¶
        
        # åˆå¹¶æ•°æ®
        if data_frames:
            result = pd.concat(data_frames, ignore_index=True)
            
            # æŒ‰æ—¶é—´æ’åº
            result = result.sort_values('UTC')
            
            print(f"âš¡ å¿«é€ŸåŠ è½½å®Œæˆï¼Œå…± {len(result)} æ¡è®°å½•")
            return result
        else:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®")
            return pd.DataFrame()

    def _load_data_legacy(self, start_time: float, end_time: float, vehicle_id: str = None) -> pd.DataFrame:
        """ä½¿ç”¨åŸå§‹CSVæ–‡ä»¶è¿›è¡ŒåŠ è½½ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        print("ä½¿ç”¨åŸå§‹CSVæ–‡ä»¶è¿›è¡ŒæŸ¥è¯¢ï¼ˆè¾ƒæ…¢ï¼‰...")
        
        # æ·»åŠ æ•°æ®é›†æ—¶é—´èŒƒå›´éªŒè¯
        min_valid_time = 1378944000  # 2013-09-12 00:00:00 UTC
        max_valid_time = 1379548799  # 2013-09-18 23:59:59 UTC
        
        # æ£€æŸ¥è¯·æ±‚çš„æ—¶é—´èŒƒå›´æ˜¯å¦ä¸æ•°æ®é›†æ—¶é—´èŒƒå›´æœ‰äº¤é›†
        if end_time < min_valid_time or start_time > max_valid_time:
            print(f"è­¦å‘Šï¼šè¯·æ±‚çš„æ—¶é—´èŒƒå›´ ({start_time}-{end_time}) è¶…å‡ºæ•°æ®é›†èŒƒå›´ ({min_valid_time}-{max_valid_time})")
            return pd.DataFrame()
        
        # é™åˆ¶æŸ¥è¯¢æ—¶é—´èŒƒå›´ï¼Œé¿å…å¤„ç†è¿‡å¤šæ•°æ®
        time_span_hours = (end_time - start_time) / 3600
        if time_span_hours > 24:
            print(f"è­¦å‘Šï¼šæŸ¥è¯¢æ—¶é—´è·¨åº¦è¿‡å¤§ ({time_span_hours:.1f} å°æ—¶)ï¼Œå»ºè®®ç¼©çŸ­åˆ°24å°æ—¶ä»¥å†…")
            end_time = start_time + 24 * 3600
            print(f"è‡ªåŠ¨æˆªæ–­åˆ°24å°æ—¶: {start_time} åˆ° {end_time}")
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"{start_time}_{end_time}_{vehicle_id}"
        
        # å¦‚æœå·²ç¼“å­˜ï¼Œç›´æ¥è¿”å›
        if cache_key in self._cached_data:
            print("ä½¿ç”¨ç¼“å­˜æ•°æ®")
            return self._cached_data[cache_key]
        
        # è·å–æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = self.get_csv_files()
        
        if not csv_files:
            print("æœªæ‰¾åˆ°CSVæ–‡ä»¶")
            return pd.DataFrame()
        
        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
        
        # å­˜å‚¨æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®
        all_data = []
        total_rows_processed = 0
        max_rows_limit = 200000  # å‡å°‘å†…å­˜å ç”¨
        
        for i, file_path in enumerate(csv_files):
            print(f"å¤„ç†æ–‡ä»¶ {i+1}/{len(csv_files)}: {os.path.basename(file_path)}")
            
            try:
                # ä½¿ç”¨åˆ†å—è¯»å–å¤§æ–‡ä»¶
                chunk_size = 50000
                chunks = pd.read_csv(file_path, chunksize=chunk_size)
                
                for chunk_num, chunk in enumerate(chunks):
                    if total_rows_processed >= max_rows_limit:
                        print(f"è¾¾åˆ°æœ€å¤§è¡Œæ•°é™åˆ¶ ({max_rows_limit})ï¼Œåœæ­¢å¤„ç†æ›´å¤šæ•°æ®")
                        break
                        
                    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
                    required_cols = ['UTC', 'LAT', 'LON', 'COMMADDR']
                    if not all(col in chunk.columns for col in required_cols):
                        continue
                    
                    # æ—¶é—´è¿‡æ»¤
                    filtered_chunk = chunk[(chunk['UTC'] >= start_time) & (chunk['UTC'] <= end_time)]
                    
                    # è½¦è¾†IDè¿‡æ»¤
                    if vehicle_id:
                        filtered_chunk['COMMADDR'] = filtered_chunk['COMMADDR'].astype(str)
                        filtered_chunk = filtered_chunk[filtered_chunk['COMMADDR'] == str(vehicle_id)]
                    
                    if not filtered_chunk.empty:
                        all_data.append(filtered_chunk)
                        total_rows_processed += len(filtered_chunk)
                
                if total_rows_processed >= max_rows_limit:
                    break
            
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {os.path.basename(file_path)} æ—¶å‡ºé”™: {e}")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if all_data:
            result_df = pd.concat(all_data, ignore_index=True)
            
            # æ™ºèƒ½é‡‡æ ·
            if len(result_df) > 100000:
                result_df = result_df.sample(n=100000, random_state=42)
            
            # ç¼“å­˜ç»“æœ
            if len(self._cached_data) < 3:
                self._cached_data[cache_key] = result_df
            
            return result_df
        else:
            return pd.DataFrame()

    def generate_heatmap_data(self, df: pd.DataFrame, resolution: float = 0.001) -> List[HeatmapPoint]:
        """
        ç”Ÿæˆçƒ­åŠ›å›¾æ•°æ®ï¼ˆä¼˜åŒ–ç¼“å­˜ç‰ˆæœ¬ï¼‰
        """
        # ç”Ÿæˆçƒ­åŠ›å›¾ç¼“å­˜é”®
        if df.empty:
            cache_key = f"heatmap_empty_{resolution}"
        else:
            # åŸºäºæ•°æ®ç‰¹å¾ç”Ÿæˆç¼“å­˜é”®
            data_hash = hash(f"{len(df)}_{df['UTC'].min()}_{df['UTC'].max()}_{resolution}")
            cache_key = f"heatmap_{data_hash}"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self._heatmap_cache:
            print("ğŸ“¦ ä½¿ç”¨çƒ­åŠ›å›¾ç¼“å­˜ï¼ˆç§’çº§å“åº”ï¼‰")
            return self._heatmap_cache[cache_key]
        
        # ç”Ÿæˆçƒ­åŠ›å›¾
        if self.use_preprocessed and hasattr(self, 'spatial_grids'):
            result = self._generate_heatmap_fast(df, resolution)
        else:
            result = self._generate_heatmap_legacy(df, resolution)
        
        # ç¼“å­˜ç»“æœï¼ˆé™åˆ¶ç¼“å­˜å¤§å°ï¼‰
        if len(self._heatmap_cache) >= 5:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜
            oldest_key = next(iter(self._heatmap_cache))
            del self._heatmap_cache[oldest_key]
        
        self._heatmap_cache[cache_key] = result
        return result
    
    def _generate_heatmap_fast(self, df: pd.DataFrame, resolution: float = 0.001) -> List[HeatmapPoint]:
        """ä½¿ç”¨é¢„è®¡ç®—æ•°æ®å¿«é€Ÿç”Ÿæˆçƒ­åŠ›å›¾"""
        # å¯»æ‰¾æœ€æ¥è¿‘çš„åˆ†è¾¨ç‡
        available_resolutions = list(self.spatial_grids.keys())
        if not available_resolutions:
            return self._generate_heatmap_legacy(df, resolution)
        
        # é€‰æ‹©æœ€æ¥è¿‘çš„åˆ†è¾¨ç‡
        closest_resolution = min(available_resolutions, key=lambda x: abs(x - resolution))
        print(f"ä½¿ç”¨é¢„è®¡ç®—ç½‘æ ¼ (åˆ†è¾¨ç‡: {closest_resolution})")
        
        grid_data = self.spatial_grids[closest_resolution]
        
        # å¦‚æœdfä¸ºç©ºæˆ–æ²¡æœ‰æ—¶é—´è¿‡æ»¤éœ€æ±‚ï¼Œç›´æ¥è¿”å›é¢„è®¡ç®—ç»“æœ
        if df.empty:
            heatmap_points = []
            for grid_key, count in grid_data.items():
                try:
                    lat, lng = map(float, grid_key.split(','))
                    heatmap_points.append(HeatmapPoint(lat=lat, lng=lng, count=count))
                except:
                    continue
            print(f"å¿«é€Ÿçƒ­åŠ›å›¾ç”Ÿæˆå®Œæˆï¼Œå…± {len(heatmap_points)} ä¸ªç‚¹")
            return heatmap_points
        
        # å¦‚æœæœ‰ç‰¹å®šæ•°æ®è¿‡æ»¤ï¼Œç»“åˆå®æ—¶è®¡ç®—
        return self._generate_heatmap_legacy(df, resolution)
    
    def _generate_heatmap_legacy(self, df: pd.DataFrame, resolution: float = 0.001) -> List[HeatmapPoint]:
        """ä¼ ç»Ÿæ–¹å¼ç”Ÿæˆçƒ­åŠ›å›¾"""
        if df.empty:
            return []
        
        # ç¡®ä¿ç»çº¬åº¦åˆ—å­˜åœ¨
        if 'LAT' not in df.columns or 'LON' not in df.columns:
            print("æ•°æ®ä¸­ç¼ºå°‘ç»çº¬åº¦åˆ—")
            return []
        
        # è½¬æ¢åæ ‡ï¼ˆå‡è®¾åŸå§‹æ•°æ®éœ€è¦é™¤ä»¥1e5ï¼‰
        df['lat'] = df['LAT'] / 1e5
        df['lng'] = df['LON'] / 1e5
        
        # æŒ‰åˆ†è¾¨ç‡ç½‘æ ¼åŒ–
        df['lat_grid'] = (df['lat'] / resolution).round() * resolution
        df['lng_grid'] = (df['lng'] / resolution).round() * resolution
        
        # ç»Ÿè®¡æ¯ä¸ªç½‘æ ¼çš„ç‚¹æ•°
        grid_counts = df.groupby(['lat_grid', 'lng_grid']).size().reset_index(name='count')
        
        # è½¬æ¢ä¸ºçƒ­åŠ›å›¾ç‚¹åˆ—è¡¨
        heatmap_points = [
            HeatmapPoint(lat=row['lat_grid'], lng=row['lng_grid'], count=row['count'])
            for _, row in grid_counts.iterrows()
        ]
        
        return heatmap_points
    
    def generate_track_data(self, df: pd.DataFrame, vehicle_id: str = None) -> List[VehicleTrack]:
        """
        ç”Ÿæˆè½¦è¾†è½¨è¿¹æ•°æ®ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            df: åŒ…å«è½¨è¿¹æ•°æ®çš„DataFrame
            vehicle_id: è½¦è¾†IDï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰è½¦è¾†
            
        Returns:
            è½¦è¾†è½¨è¿¹åˆ—è¡¨
        """
        if df.empty:
            return []
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        required_cols = ['UTC', 'LAT', 'LON', 'COMMADDR']
        if not all(col in df.columns for col in required_cols):
            print("âŒ æ•°æ®ä¸­ç¼ºå°‘å¿…è¦çš„åˆ—")
            return []
        
        # å¦‚æœæŒ‡å®šäº†è½¦è¾†IDï¼Œåˆ™åªå¤„ç†è¯¥è½¦è¾†
        if vehicle_id:
            # ç¡®ä¿ç±»å‹åŒ¹é…ï¼Œé¿å…å› ç±»å‹ä¸ä¸€è‡´å¯¼è‡´è¿‡æ»¤å¤±è´¥
            df['COMMADDR'] = df['COMMADDR'].astype(str)
            df = df[df['COMMADDR'] == str(vehicle_id)]
            if df.empty:
                return []
        
        # é™åˆ¶å¤„ç†çš„è½¦è¾†æ•°é‡ä»¥æé«˜æ€§èƒ½
        unique_vehicles = df['COMMADDR'].unique()
        max_vehicles = 50  # æœ€å¤šå¤„ç†50ä¸ªè½¦è¾†
        
        if len(unique_vehicles) > max_vehicles and vehicle_id is None:
            print(f"âš ï¸  è½¦è¾†æ•°é‡è¿‡å¤š({len(unique_vehicles)})ï¼Œéšæœºé€‰æ‹©{max_vehicles}ä¸ª")
            selected_vehicles = np.random.choice(unique_vehicles, max_vehicles, replace=False)
            df = df[df['COMMADDR'].isin(selected_vehicles)]
        
        # å­˜å‚¨æ‰€æœ‰è½¦è¾†çš„è½¨è¿¹
        all_tracks = []
        
        print(f"ğŸš— å¼€å§‹å¤„ç† {len(df['COMMADDR'].unique())} ä¸ªè½¦è¾†çš„è½¨è¿¹...")
        
        # æŒ‰è½¦è¾†IDåˆ†ç»„å¤„ç†
        for i, (veh_id, group) in enumerate(df.groupby('COMMADDR')):
            # æŒ‰æ—¶é—´æ’åº
            group = group.sort_values('UTC')
            
            # æå–è½¨è¿¹ç‚¹
            track_points = []
            for _, row in group.iterrows():
                point = TrackPoint(
                    lat=row['LAT'] / 1e5,
                    lng=row['LON'] / 1e5,
                    timestamp=row['UTC'],
                    speed=row.get('SPEED'),
                    direction=row.get('DIRECTION'),
                    status=row.get('STATUS')
                )
                track_points.append(point)
            
            # è®¡ç®—è½¨è¿¹è·ç¦»ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”ä½¿ç”¨Haversineå…¬å¼ï¼‰
            distance = None
            if len(track_points) > 1:
                distance = 0
                for i in range(1, len(track_points)):
                    p1, p2 = track_points[i-1], track_points[i]
                    # ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»ä½œä¸ºç®€åŒ–ï¼ˆå®é™…åº”ä½¿ç”¨Haversineå…¬å¼ï¼‰
                    dist = math.sqrt((p2.lng - p1.lng)**2 + (p2.lat - p1.lat)**2)
                    # è½¬æ¢ä¸ºå…¬é‡Œï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
                    dist_km = dist * 111  # 1åº¦çº¦ç­‰äº111å…¬é‡Œ
                    distance += dist_km
            
            # åˆ›å»ºè½¦è¾†è½¨è¿¹
            track = VehicleTrack(
                vehicle_id=veh_id,
                points=track_points,
                start_time=track_points[0].timestamp if track_points else None,
                end_time=track_points[-1].timestamp if track_points else None,
                distance=round(distance, 2) if distance is not None else None
            )
            
            all_tracks.append(track)
        
        return all_tracks
    
    def calculate_statistics(self, df: pd.DataFrame, group_by: str = 'hour') -> Dict[str, Any]:
        """
        è®¡ç®—äº¤é€šæ•°æ®ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            df: äº¤é€šæ•°æ®DataFrame
            group_by: æ—¶é—´åˆ†ç»„æ–¹å¼ï¼ˆhour, day, week, monthï¼‰
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if df.empty:
            return {
                'total_vehicles': 0,
                'total_points': 0,
                'active_vehicles': 0,
                'time_span': '0å°æ—¶',
                'coverage_area': 'æœªçŸ¥',
                'average_speed': 0,
                'time_distribution': []
            }
        
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        total_points = len(df)
        unique_vehicles = df['COMMADDR'].nunique()
        
        # è®¡ç®—æ—¶é—´è·¨åº¦
        min_time = df['UTC'].min()
        max_time = df['UTC'].max()
        time_span_hours = (max_time - min_time) / 3600
        time_span = f"{time_span_hours:.1f}å°æ—¶"
        
        # è®¡ç®—å¹³å‡é€Ÿåº¦ï¼ˆå¦‚æœæœ‰é€Ÿåº¦åˆ—ï¼‰
        avg_speed = 0
        if 'SPEED' in df.columns:
            avg_speed = df['SPEED'].mean()
        
        # è®¡ç®—è¦†ç›–åŒºåŸŸï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è®¡ç®—å‡¸åŒ…é¢ç§¯ï¼‰
        lat_range = df['LAT'].max() - df['LAT'].min()
        lon_range = df['LON'].max() - df['LON'].min()
        coverage_area = 'æµå—å¸‚åŒº'  # ç®€åŒ–å¤„ç†ï¼Œå®é™…åº”æ ¹æ®ç»çº¬åº¦èŒƒå›´ç¡®å®š
        
        # æ—¶é—´åˆ†å¸ƒç»Ÿè®¡
        df['datetime'] = pd.to_datetime(df['UTC'], unit='s')
        
        if group_by == 'hour':
            df['time_group'] = df['datetime'].dt.hour
            group_format = '{:02d}æ—¶'
        elif group_by == 'day':
            df['time_group'] = df['datetime'].dt.day
            group_format = '{:02d}æ—¥'
        elif group_by == 'week':
            df['time_group'] = df['datetime'].dt.isocalendar().week
            group_format = 'ç¬¬{:02d}å‘¨'
        elif group_by == 'month':
            df['time_group'] = df['datetime'].dt.month
            group_format = '{:02d}æœˆ'
        else:
            df['time_group'] = df['datetime'].dt.hour
            group_format = '{:02d}æ—¶'
        
        # è®¡ç®—æ¯ä¸ªæ—¶é—´æ®µçš„æ•°æ®ç‚¹æ•°é‡
        time_counts = df.groupby('time_group').size()
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        time_distribution = [
            {'time_key': group_format.format(key), 'count': count}
            for key, count in time_counts.items()
        ]
        
        # è¿”å›ç»Ÿè®¡ç»“æœ
        return {
            'total_vehicles': unique_vehicles,
            'total_points': total_points,
            'active_vehicles': unique_vehicles,
            'time_span': time_span,
            'coverage_area': coverage_area,
            'average_speed': round(avg_speed, 1),
            'time_distribution': time_distribution
        }
    
    def clear_cache(self):
        """æ¸…é™¤æ•°æ®ç¼“å­˜"""
        self._cached_data = {}
        print("æ•°æ®ç¼“å­˜å·²æ¸…é™¤")
    
    def detect_anomalies(self, df: pd.DataFrame, detection_types: str = "all", thresholds: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """
        æ£€æµ‹äº¤é€šå¼‚å¸¸äº‹ä»¶
        
        Args:
            df: äº¤é€šæ•°æ®DataFrame
            detection_types: æ£€æµ‹ç±»å‹ï¼ˆall, long_stop, abnormal_route, speed_anomaly, cluster_anomalyï¼‰
            thresholds: æ£€æµ‹é˜ˆå€¼å‚æ•°
            
        Returns:
            å¼‚å¸¸äº‹ä»¶åˆ—è¡¨
        """
        if df.empty:
            return []
        
        if thresholds is None:
            thresholds = {}
        
        # é»˜è®¤é˜ˆå€¼
        default_thresholds = {
            "long_stop_duration": 300,  # 5åˆ†é’Ÿ
            "speed_threshold_low": 5,   # ä½é€Ÿé˜ˆå€¼ km/h
            "speed_threshold_high": 80, # é«˜é€Ÿé˜ˆå€¼ km/h
            "detour_ratio": 1.5,       # ç»•è·¯æ¯”ä¾‹
            "cluster_density": 50,      # èšé›†å¯†åº¦
            "stop_distance_threshold": 0.0001  # åœè½¦è·ç¦»é˜ˆå€¼ï¼ˆåº¦ï¼‰
        }
        thresholds = {**default_thresholds, **thresholds}
        
        anomalies = []
        
        # æ ¹æ®æ£€æµ‹ç±»å‹è°ƒç”¨ç›¸åº”çš„æ£€æµ‹æ–¹æ³•
        if detection_types == "all" or "long_stop" in detection_types:
            anomalies.extend(self._detect_long_stops(df, thresholds))
        
        if detection_types == "all" or "speed_anomaly" in detection_types:
            anomalies.extend(self._detect_speed_anomalies(df, thresholds))
        
        if detection_types == "all" or "cluster_anomaly" in detection_types:
            anomalies.extend(self._detect_cluster_anomalies(df, thresholds))
        
        if detection_types == "all" or "abnormal_route" in detection_types:
            anomalies.extend(self._detect_abnormal_routes(df, thresholds))
        
        # å»é‡å’Œæ’åº
        anomalies = self._deduplicate_anomalies(anomalies)
        anomalies.sort(key=lambda x: x.get('timestamp', 0), reverse=True)
        
        return anomalies
    
    def _detect_long_stops(self, df: pd.DataFrame, thresholds: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ£€æµ‹é•¿æ—¶é—´åœè½¦å¼‚å¸¸"""
        anomalies = []
        stop_duration_threshold = thresholds.get("long_stop_duration", 300)
        distance_threshold = thresholds.get("stop_distance_threshold", 0.0001)
        
        # æŒ‰è½¦è¾†åˆ†ç»„
        for vehicle_id, group in df.groupby('COMMADDR'):
            group = group.sort_values('UTC').reset_index(drop=True)
            
            if len(group) < 2:
                continue
            
            # è®¡ç®—è¿ç»­ä½ç½®çš„è·ç¦»å’Œæ—¶é—´å·®
            for i in range(1, len(group)):
                lat1, lon1 = group.iloc[i-1]['LAT'] / 1e5, group.iloc[i-1]['LON'] / 1e5
                lat2, lon2 = group.iloc[i]['LAT'] / 1e5, group.iloc[i]['LON'] / 1e5
                
                # è®¡ç®—è·ç¦»ï¼ˆç®€åŒ–ç‰ˆï¼‰
                distance = math.sqrt((lat2 - lat1)**2 + (lon2 - lon1)**2)
                time_diff = group.iloc[i]['UTC'] - group.iloc[i-1]['UTC']
                
                # å¦‚æœè·ç¦»å¾ˆå°ä¸”æ—¶é—´å·®å¾ˆå¤§ï¼Œè®¤ä¸ºæ˜¯åœè½¦
                if distance < distance_threshold and time_diff > stop_duration_threshold:
                    anomaly = {
                        "id": f"long_stop_{vehicle_id}_{group.iloc[i-1]['UTC']}",
                        "type": "long_stop",
                        "name": "é•¿æ—¶é—´åœè½¦",
                        "vehicle_id": str(vehicle_id),
                        "timestamp": group.iloc[i-1]['UTC'],
                        "end_timestamp": group.iloc[i]['UTC'],
                        "duration": time_diff,
                        "latitude": lat1,
                        "longitude": lon1,
                        "severity": self._calculate_severity("long_stop", {"duration": time_diff}),
                        "description": f"è½¦è¾† {vehicle_id} åœ¨ä½ç½® ({lat1:.4f}, {lon1:.4f}) åœè½¦ {time_diff//60:.0f} åˆ†é’Ÿ",
                        "details": {
                            "stop_duration": time_diff,
                            "location": f"{lat1:.4f}, {lon1:.4f}",
                            "threshold_used": stop_duration_threshold
                        }
                    }
                    anomalies.append(anomaly)
        
        return anomalies
    
    def _detect_speed_anomalies(self, df: pd.DataFrame, thresholds: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ£€æµ‹é€Ÿåº¦å¼‚å¸¸"""
        anomalies = []
        low_threshold = thresholds.get("speed_threshold_low", 5)
        high_threshold = thresholds.get("speed_threshold_high", 80)
        
        # å¦‚æœæ²¡æœ‰é€Ÿåº¦åˆ—ï¼Œè®¡ç®—é€Ÿåº¦
        if 'SPEED' not in df.columns:
            df = self._calculate_speed(df)
        
        # æ£€æµ‹å¼‚å¸¸é€Ÿåº¦
        for _, row in df.iterrows():
            speed = row.get('SPEED', 0)
            
            if pd.isna(speed) or speed == 0:
                continue
            
            anomaly_type = None
            severity = "low"
            
            if speed < low_threshold:
                anomaly_type = "low_speed"
                severity = "medium" if speed < 2 else "low"
            elif speed > high_threshold:
                anomaly_type = "high_speed"
                severity = "high" if speed > 100 else "medium"
            
            if anomaly_type:
                anomaly = {
                    "id": f"speed_{anomaly_type}_{row['COMMADDR']}_{row['UTC']}",
                    "type": "speed_anomaly",
                    "name": "å¼‚å¸¸é€Ÿåº¦" if anomaly_type == "low_speed" else "è¶…é€Ÿè¡Œé©¶",
                    "vehicle_id": str(row['COMMADDR']),
                    "timestamp": row['UTC'],
                    "latitude": row['LAT'] / 1e5,
                    "longitude": row['LON'] / 1e5,
                    "severity": severity,
                    "description": f"è½¦è¾† {row['COMMADDR']} é€Ÿåº¦å¼‚å¸¸: {speed:.1f} km/h",
                    "details": {
                        "speed": speed,
                        "speed_type": anomaly_type,
                        "thresholds": {"low": low_threshold, "high": high_threshold}
                    }
                }
                anomalies.append(anomaly)
        
        return anomalies
    
    def _detect_cluster_anomalies(self, df: pd.DataFrame, thresholds: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ£€æµ‹è½¦è¾†å¼‚å¸¸èšé›†"""
        anomalies = []
        density_threshold = thresholds.get("cluster_density", 50)
        
        # ä½¿ç”¨ç©ºé—´ç½‘æ ¼æ¥æ£€æµ‹èšé›†
        grid_size = 0.005  # çº¦500ç±³
        df['lat_grid'] = (df['LAT'] / 1e5 / grid_size).round() * grid_size
        df['lng_grid'] = (df['LON'] / 1e5 / grid_size).round() * grid_size
        
        # æŒ‰æ—¶é—´çª—å£å’Œç©ºé—´ç½‘æ ¼åˆ†ç»„
        time_window = 900  # 15åˆ†é’Ÿæ—¶é—´çª—å£
        df['time_window'] = (df['UTC'] / time_window).round() * time_window
        
        cluster_counts = df.groupby(['time_window', 'lat_grid', 'lng_grid']).agg({
            'COMMADDR': 'nunique',  # å”¯ä¸€è½¦è¾†æ•°
            'UTC': 'mean'  # å¹³å‡æ—¶é—´
        }).reset_index()
        
        cluster_counts.columns = ['time_window', 'lat_grid', 'lng_grid', 'vehicle_count', 'avg_time']
        
        # æ‰¾å‡ºå¼‚å¸¸èšé›†
        abnormal_clusters = cluster_counts[cluster_counts['vehicle_count'] > density_threshold]
        
        for _, cluster in abnormal_clusters.iterrows():
            anomaly = {
                "id": f"cluster_{cluster['time_window']}_{cluster['lat_grid']}_{cluster['lng_grid']}",
                "type": "cluster_anomaly",
                "name": "è½¦è¾†å¼‚å¸¸èšé›†",
                "timestamp": cluster['avg_time'],
                "latitude": cluster['lat_grid'],
                "longitude": cluster['lng_grid'],
                "severity": self._calculate_severity("cluster", {"vehicle_count": cluster['vehicle_count']}),
                "description": f"åœ¨ä½ç½® ({cluster['lat_grid']:.4f}, {cluster['lng_grid']:.4f}) å‘ç° {cluster['vehicle_count']} è¾†è½¦å¼‚å¸¸èšé›†",
                "details": {
                    "vehicle_count": cluster['vehicle_count'],
                    "threshold_used": density_threshold,
                    "time_window": time_window
                }
            }
            anomalies.append(anomaly)
        
        return anomalies
    
    def _detect_abnormal_routes(self, df: pd.DataFrame, thresholds: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¼‚å¸¸ç»•è·¯è¡Œä¸º"""
        anomalies = []
        detour_ratio = thresholds.get("detour_ratio", 1.5)
        
        # æŒ‰è½¦è¾†åˆ†ç»„æ£€æµ‹ç»•è·¯
        for vehicle_id, group in df.groupby('COMMADDR'):
            group = group.sort_values('UTC').reset_index(drop=True)
            
            if len(group) < 3:
                continue
            
            # è®¡ç®—è½¨è¿¹æ€»é•¿åº¦å’Œç›´çº¿è·ç¦»
            total_distance = 0
            for i in range(1, len(group)):
                lat1, lon1 = group.iloc[i-1]['LAT'] / 1e5, group.iloc[i-1]['LON'] / 1e5
                lat2, lon2 = group.iloc[i]['LAT'] / 1e5, group.iloc[i]['LON'] / 1e5
                
                # è®¡ç®—ä¸¤ç‚¹é—´è·ç¦»
                segment_distance = math.sqrt((lat2 - lat1)**2 + (lon2 - lon1)**2) * 111  # è½¬æ¢ä¸ºå…¬é‡Œ
                total_distance += segment_distance
            
            # è®¡ç®—èµ·ç»ˆç‚¹ç›´çº¿è·ç¦»
            start_lat, start_lon = group.iloc[0]['LAT'] / 1e5, group.iloc[0]['LON'] / 1e5
            end_lat, end_lon = group.iloc[-1]['LAT'] / 1e5, group.iloc[-1]['LON'] / 1e5
            straight_distance = math.sqrt((end_lat - start_lat)**2 + (end_lon - start_lon)**2) * 111
            
            # è®¡ç®—ç»•è·¯æ¯”ä¾‹
            if straight_distance > 0.1:  # ç›´çº¿è·ç¦»å¤§äº100ç±³æ‰æ£€æµ‹
                actual_ratio = total_distance / straight_distance
                
                if actual_ratio > detour_ratio:
                    anomaly = {
                        "id": f"detour_{vehicle_id}_{group.iloc[0]['UTC']}",
                        "type": "abnormal_route",
                        "name": "å¼‚å¸¸ç»•è·¯",
                        "vehicle_id": str(vehicle_id),
                        "timestamp": group.iloc[0]['UTC'],
                        "end_timestamp": group.iloc[-1]['UTC'],
                        "latitude": start_lat,
                        "longitude": start_lon,
                        "severity": self._calculate_severity("detour", {"ratio": actual_ratio}),
                        "description": f"è½¦è¾† {vehicle_id} ç»•è·¯è¡Œé©¶ï¼Œå®é™…è·¯å¾„æ˜¯ç›´çº¿è·ç¦»çš„ {actual_ratio:.1f} å€",
                        "details": {
                            "total_distance": total_distance,
                            "straight_distance": straight_distance,
                            "detour_ratio": actual_ratio,
                            "threshold_used": detour_ratio,
                            "start_location": f"{start_lat:.4f}, {start_lon:.4f}",
                            "end_location": f"{end_lat:.4f}, {end_lon:.4f}"
                        }
                    }
                    anomalies.append(anomaly)
        
        return anomalies
    
    def _calculate_speed(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—è½¦è¾†é€Ÿåº¦"""
        df = df.copy()
        df['SPEED'] = 0.0
        
        for vehicle_id, group in df.groupby('COMMADDR'):
            group = group.sort_values('UTC').reset_index()
            
            for i in range(1, len(group)):
                lat1, lon1 = group.iloc[i-1]['LAT'] / 1e5, group.iloc[i-1]['LON'] / 1e5
                lat2, lon2 = group.iloc[i]['LAT'] / 1e5, group.iloc[i]['LON'] / 1e5
                time_diff = group.iloc[i]['UTC'] - group.iloc[i-1]['UTC']
                
                if time_diff > 0:
                    # è®¡ç®—è·ç¦»ï¼ˆå…¬é‡Œï¼‰
                    distance = math.sqrt((lat2 - lat1)**2 + (lon2 - lon1)**2) * 111
                    # è®¡ç®—é€Ÿåº¦ï¼ˆkm/hï¼‰
                    speed = distance / (time_diff / 3600)
                    df.loc[group.iloc[i]['index'], 'SPEED'] = speed
        
        return df
    
    def _calculate_severity(self, anomaly_type: str, params: Dict[str, Any]) -> str:
        """è®¡ç®—å¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""
        if anomaly_type == "long_stop":
            duration = params.get("duration", 0)
            if duration > 1800:  # 30åˆ†é’Ÿ
                return "high"
            elif duration > 600:  # 10åˆ†é’Ÿ
                return "medium"
            else:
                return "low"
        
        elif anomaly_type == "cluster":
            count = params.get("vehicle_count", 0)
            if count > 100:
                return "high"
            elif count > 70:
                return "medium"
            else:
                return "low"
        
        elif anomaly_type == "detour":
            ratio = params.get("ratio", 1.0)
            if ratio > 3.0:
                return "high"
            elif ratio > 2.0:
                return "medium"
            else:
                return "low"
        
        return "low"
    
    def _deduplicate_anomalies(self, anomalies: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é™¤é‡å¤çš„å¼‚å¸¸äº‹ä»¶"""
        seen_ids = set()
        unique_anomalies = []
        
        for anomaly in anomalies:
            anomaly_id = anomaly.get("id")
            if anomaly_id not in seen_ids:
                seen_ids.add(anomaly_id)
                unique_anomalies.append(anomaly)
        
        return unique_anomalies
    
    def calculate_anomaly_statistics(self, anomalies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—å¼‚å¸¸ç»Ÿè®¡ä¿¡æ¯"""
        if not anomalies:
            return {
                "total_count": 0,
                "by_type": {},
                "by_severity": {"high": 0, "medium": 0, "low": 0},
                "time_distribution": [],
                "top_locations": []
            }
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        type_counts = {}
        for anomaly in anomalies:
            anomaly_type = anomaly.get("type", "unknown")
            type_counts[anomaly_type] = type_counts.get(anomaly_type, 0) + 1
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        severity_counts = {"high": 0, "medium": 0, "low": 0}
        for anomaly in anomalies:
            severity = anomaly.get("severity", "low")
            if severity in severity_counts:
                severity_counts[severity] += 1
        
        # æ—¶é—´åˆ†å¸ƒç»Ÿè®¡ï¼ˆæŒ‰å°æ—¶ï¼‰
        import pandas as pd
        timestamps = [anomaly.get("timestamp", 0) for anomaly in anomalies]
        df_time = pd.DataFrame({"timestamp": timestamps})
        df_time['datetime'] = pd.to_datetime(df_time['timestamp'], unit='s')
        df_time['hour'] = df_time['datetime'].dt.hour
        
        time_distribution = []
        for hour in range(24):
            count = len(df_time[df_time['hour'] == hour])
            time_distribution.append({"hour": hour, "count": count})
        
        # çƒ­ç‚¹ä½ç½®ç»Ÿè®¡
        location_counts = {}
        for anomaly in anomalies:
            lat = anomaly.get("latitude", 0)
            lng = anomaly.get("longitude", 0)
            # å°†åæ ‡å››èˆäº”å…¥åˆ°å°æ•°ç‚¹å3ä½æ¥èšåˆ
            location_key = f"{lat:.3f},{lng:.3f}"
            if location_key not in location_counts:
                location_counts[location_key] = {"lat": lat, "lng": lng, "count": 0}
            location_counts[location_key]["count"] += 1
        
        # è·å–å‰5ä¸ªçƒ­ç‚¹ä½ç½®
        top_locations = sorted(
            location_counts.values(),
            key=lambda x: x["count"],
            reverse=True
        )[:5]
        
        return {
            "total_count": len(anomalies),
            "by_type": type_counts,
            "by_severity": severity_counts,
            "time_distribution": time_distribution,
            "top_locations": top_locations
        }
    
    def generate_anomaly_heatmap(self, anomalies: List[Dict[str, Any]], resolution: float = 0.002) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå¼‚å¸¸äº‹ä»¶çƒ­åŠ›å›¾"""
        if not anomalies:
            return []
        
        # æŒ‰ç½‘æ ¼èšåˆå¼‚å¸¸äº‹ä»¶
        grid_counts = {}
        
        for anomaly in anomalies:
            lat = anomaly.get("latitude", 0)
            lng = anomaly.get("longitude", 0)
            
            # ç½‘æ ¼åŒ–åæ ‡
            lat_grid = round(lat / resolution) * resolution
            lng_grid = round(lng / resolution) * resolution
            
            grid_key = f"{lat_grid},{lng_grid}"
            if grid_key not in grid_counts:
                grid_counts[grid_key] = {
                    "lat": lat_grid,
                    "lng": lng_grid,
                    "count": 0,
                    "severity_score": 0
                }
            
            grid_counts[grid_key]["count"] += 1
            
            # è®¡ç®—ä¸¥é‡æ€§åˆ†æ•°
            severity = anomaly.get("severity", "low")
            severity_score = {"high": 3, "medium": 2, "low": 1}.get(severity, 1)
            grid_counts[grid_key]["severity_score"] += severity_score
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        heatmap_points = []
        for grid_data in grid_counts.values():
            # è®¡ç®—å¹³å‡ä¸¥é‡æ€§
            avg_severity = grid_data["severity_score"] / grid_data["count"]
            
            heatmap_points.append({
                "lat": grid_data["lat"],
                "lng": grid_data["lng"],
                "count": grid_data["count"],
                "intensity": min(grid_data["count"] * avg_severity, 100)  # é™åˆ¶æœ€å¤§å¼ºåº¦
            })
        
        return heatmap_points
    
    def generate_dynamic_heatmap(
        self, 
        df: pd.DataFrame, 
        temporal_resolution: int = 15,
        spatial_resolution: float = 0.001,
        smoothing: bool = True
    ) -> List[Dict[str, Any]]:
        """
        ç”ŸæˆåŠ¨æ€çƒ­åŠ›å›¾æ•°æ®ï¼ˆæ—¶é—´åºåˆ—çƒ­åŠ›å›¾å¸§ï¼‰
        
        Args:
            df: äº¤é€šæ•°æ®DataFrame
            temporal_resolution: æ—¶é—´åˆ†è¾¨ç‡ï¼ˆåˆ†é’Ÿï¼‰
            spatial_resolution: ç©ºé—´åˆ†è¾¨ç‡ï¼ˆåº¦ï¼‰
            smoothing: æ˜¯å¦å¹³æ»‘å¤„ç†
            
        Returns:
            æ—¶é—´å¸§åˆ—è¡¨ï¼Œæ¯å¸§åŒ…å«è¯¥æ—¶é—´æ®µçš„çƒ­åŠ›å›¾æ•°æ®
        """
        if df.empty:
            return []
        
        # ç¡®ä¿æ—¶é—´åˆ—å­˜åœ¨
        if 'UTC' not in df.columns:
            return []
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        min_time = df['UTC'].min()
        max_time = df['UTC'].max()
        
        # åˆ›å»ºæ—¶é—´çª—å£
        time_window_seconds = temporal_resolution * 60
        time_frames = []
        
        current_time = min_time
        while current_time < max_time:
            end_time = current_time + time_window_seconds
            
            # è¿‡æ»¤å½“å‰æ—¶é—´çª—å£çš„æ•°æ®
            window_data = df[(df['UTC'] >= current_time) & (df['UTC'] < end_time)]
            
            if not window_data.empty:
                # ç”Ÿæˆè¯¥æ—¶é—´çª—å£çš„çƒ­åŠ›å›¾
                heatmap_points = self.generate_heatmap_data(window_data, spatial_resolution)
                
                # è®¡ç®—æ—¶é—´æ ‡ç­¾
                time_label = f"{datetime.fromtimestamp(current_time).strftime('%H:%M')}-{datetime.fromtimestamp(end_time).strftime('%H:%M')}"
                
                # è®¡ç®—æ€»å¼ºåº¦
                total_intensity = sum(point.count for point in heatmap_points)
                
                frame_data = {
                    "timestamp": current_time,
                    "time_label": time_label,
                    "heatmap_points": [
                        {"lat": point.lat, "lng": point.lng, "intensity": point.count}
                        for point in heatmap_points
                    ],
                    "total_intensity": total_intensity,
                    "point_count": len(heatmap_points)
                }
                
                time_frames.append(frame_data)
            
            current_time = end_time
        
        # å¦‚æœéœ€è¦å¹³æ»‘å¤„ç†
        if smoothing and len(time_frames) > 2:
            time_frames = self._smooth_temporal_data(time_frames)
        
        return time_frames
    
    def _smooth_temporal_data(self, time_frames: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¯¹æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œå¹³æ»‘å¤„ç†"""
        # ç®€å•çš„ç§»åŠ¨å¹³å‡å¹³æ»‘
        smoothed_frames = []
        window_size = 3
        
        for i, frame in enumerate(time_frames):
            if i < window_size // 2 or i >= len(time_frames) - window_size // 2:
                # è¾¹ç•Œå¸§ä¸å¹³æ»‘
                smoothed_frames.append(frame)
                continue
            
            # è®¡ç®—çª—å£å†…çš„å¹³å‡å€¼
            window_frames = time_frames[i - window_size//2:i + window_size//2 + 1]
            
            # å¯¹çƒ­åŠ›å›¾ç‚¹è¿›è¡Œå¹³æ»‘
            point_dict = {}
            for w_frame in window_frames:
                for point in w_frame['heatmap_points']:
                    key = f"{point['lat']:.4f},{point['lng']:.4f}"
                    if key not in point_dict:
                        point_dict[key] = {
                            'lat': point['lat'],
                            'lng': point['lng'],
                            'intensities': []
                        }
                    point_dict[key]['intensities'].append(point['intensity'])
            
            # è®¡ç®—å¹³å‡å¼ºåº¦
            smoothed_points = []
            for point_data in point_dict.values():
                avg_intensity = sum(point_data['intensities']) / len(point_data['intensities'])
                smoothed_points.append({
                    'lat': point_data['lat'],
                    'lng': point_data['lng'],
                    'intensity': avg_intensity
                })
            
            smoothed_frame = frame.copy()
            smoothed_frame['heatmap_points'] = smoothed_points
            smoothed_frame['total_intensity'] = sum(p['intensity'] for p in smoothed_points)
            smoothed_frames.append(smoothed_frame)
        
        return smoothed_frames
    
    def extract_od_pairs_from_data(
        self, 
        df: pd.DataFrame,
        min_trip_duration: int = 60,
        max_trip_duration: int = 7200,
        min_trip_distance: float = 0.1
    ) -> List[Dict[str, Any]]:
        """
        ä»äº¤é€šæ•°æ®ä¸­æå–ODå¯¹
        
        Args:
            df: äº¤é€šæ•°æ®DataFrame
            min_trip_duration: æœ€å°è¡Œç¨‹æ—¶é—´ï¼ˆç§’ï¼‰
            max_trip_duration: æœ€å¤§è¡Œç¨‹æ—¶é—´ï¼ˆç§’ï¼‰
            min_trip_distance: æœ€å°è¡Œç¨‹è·ç¦»ï¼ˆå…¬é‡Œï¼‰
            
        Returns:
            ODå¯¹åˆ—è¡¨
        """
        from .od_analysis_engine import ODAnalysisEngine
        
        od_engine = ODAnalysisEngine()
        od_pairs = od_engine.extract_od_pairs(
            df, 
            min_trip_duration=min_trip_duration,
            max_trip_duration=max_trip_duration,
            min_trip_distance=min_trip_distance
        )
        
        return od_pairs
    
    def perform_clustering_analysis(
        self, 
        df: pd.DataFrame,
        data_type: str = "pickup",
        algorithm: str = "dbscan",
        params: Dict[str, Any] = None
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        æ‰§è¡Œèšç±»åˆ†æ
        
        Args:
            df: äº¤é€šæ•°æ®DataFrame
            data_type: æ•°æ®ç±»å‹ ("pickup", "dropoff", "all_points")
            algorithm: èšç±»ç®—æ³•
            params: ç®—æ³•å‚æ•°
            
        Returns:
            èšç±»ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        from .clustering_engine import ClusteringEngine
        
        if df.empty:
            return [], {}
        
        # å‡†å¤‡èšç±»æ•°æ®
        if data_type == "pickup":
            # æå–èµ·ç‚¹æ•°æ®ï¼ˆå‡è®¾æ¯ä¸ªè½¦è¾†çš„ç¬¬ä¸€ä¸ªç‚¹æ˜¯èµ·ç‚¹ï¼‰
            pickup_data = []
            for vehicle_id, group in df.groupby('COMMADDR'):
                first_point = group.sort_values('UTC').iloc[0]
                pickup_data.append({
                    'lat': first_point['LAT'] / 1e5,
                    'lng': first_point['LON'] / 1e5,
                    'weight': 1.0
                })
            cluster_data = pickup_data
            
        elif data_type == "dropoff":
            # æå–ç»ˆç‚¹æ•°æ®ï¼ˆå‡è®¾æ¯ä¸ªè½¦è¾†çš„æœ€åä¸€ä¸ªç‚¹æ˜¯ç»ˆç‚¹ï¼‰
            dropoff_data = []
            for vehicle_id, group in df.groupby('COMMADDR'):
                last_point = group.sort_values('UTC').iloc[-1]
                dropoff_data.append({
                    'lat': last_point['LAT'] / 1e5,
                    'lng': last_point['LON'] / 1e5,
                    'weight': 1.0
                })
            cluster_data = dropoff_data
            
        else:  # all_points
            # ä½¿ç”¨æ‰€æœ‰æ•°æ®ç‚¹ï¼Œä½†è¿›è¡Œé‡‡æ ·ä»¥æé«˜æ€§èƒ½
            sample_size = min(10000, len(df))
            sampled_df = df.sample(sample_size) if len(df) > sample_size else df
            
            cluster_data = []
            for _, row in sampled_df.iterrows():
                cluster_data.append({
                    'lat': row['LAT'] / 1e5,
                    'lng': row['LON'] / 1e5,
                    'weight': 1.0
                })
        
        if not cluster_data:
            return [], {}
        
        # æ‰§è¡Œèšç±»
        clustering_engine = ClusteringEngine()
        labels, metrics = clustering_engine.cluster_data(
            cluster_data,
            algorithm=algorithm,
            params=params or {}
        )
        
        # åˆ†æèšç±»ç»“æœ
        clusters = clustering_engine.analyze_clusters(
            cluster_data,
            labels,
            cluster_type=data_type
        )
        
        return clusters, metrics
    
    def generate_spatiotemporal_heatmap(
        self, 
        df: pd.DataFrame,
        analysis_type: str = "density",
        temporal_resolution: int = 15,
        spatial_resolution: float = 0.001
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ—¶ç©ºçƒ­åŠ›å›¾åˆ†æ
        
        Args:
            df: äº¤é€šæ•°æ®DataFrame
            analysis_type: åˆ†æç±»å‹ ("density", "speed", "flow")
            temporal_resolution: æ—¶é—´åˆ†è¾¨ç‡ï¼ˆåˆ†é’Ÿï¼‰
            spatial_resolution: ç©ºé—´åˆ†è¾¨ç‡ï¼ˆåº¦ï¼‰
            
        Returns:
            æ—¶ç©ºåˆ†æç»“æœ
        """
        if df.empty:
            return {}
        
        # ç”ŸæˆåŠ¨æ€çƒ­åŠ›å›¾å¸§
        heatmap_frames = self.generate_dynamic_heatmap(
            df, 
            temporal_resolution=temporal_resolution,
            spatial_resolution=spatial_resolution
        )
        
        # è®¡ç®—æ—¶é—´åºåˆ—ç»Ÿè®¡
        time_series_stats = self._calculate_time_series_stats(heatmap_frames)
        
        # è®¡ç®—ç©ºé—´ç»Ÿè®¡
        spatial_stats = self._calculate_spatial_stats(df)
        
        # è®¡ç®—ç©ºé—´è¾¹ç•Œ
        spatial_bounds = {
            'min_lat': float(df['LAT'].min() / 1e5),
            'max_lat': float(df['LAT'].max() / 1e5),
            'min_lng': float(df['LON'].min() / 1e5),
            'max_lng': float(df['LON'].max() / 1e5)
        }
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        time_range = {
            'start': float(df['UTC'].min()),
            'end': float(df['UTC'].max())
        }
        
        return {
            'analysis_type': analysis_type,
            'time_range': time_range,
            'spatial_bounds': spatial_bounds,
            'heatmap_frames': heatmap_frames,
            'time_series_stats': time_series_stats,
            'spatial_stats': spatial_stats,
            'algorithm_params': {
                'temporal_resolution': temporal_resolution,
                'spatial_resolution': spatial_resolution
            }
        }
    
    def _calculate_time_series_stats(self, frames: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—æ—¶é—´åºåˆ—ç»Ÿè®¡ä¿¡æ¯"""
        if not frames:
            return {}
        
        # æå–æ—¶é—´åºåˆ—æ•°æ®
        timestamps = [frame['timestamp'] for frame in frames]
        intensities = [frame['total_intensity'] for frame in frames]
        point_counts = [frame['point_count'] for frame in frames]
        
        return {
            'total_frames': len(frames),
            'time_span_hours': (max(timestamps) - min(timestamps)) / 3600,
            'avg_intensity_per_frame': np.mean(intensities),
            'max_intensity': max(intensities),
            'min_intensity': min(intensities),
            'avg_points_per_frame': np.mean(point_counts),
            'intensity_trend': self._calculate_trend(intensities),
            'peak_time': timestamps[intensities.index(max(intensities))] if intensities else None
        }
    
    def _calculate_spatial_stats(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—ç©ºé—´ç»Ÿè®¡ä¿¡æ¯"""
        if df.empty:
            return {}
        
        # è®¡ç®—ç©ºé—´åˆ†å¸ƒ
        lats = df['LAT'] / 1e5
        lngs = df['LON'] / 1e5
        
        return {
            'spatial_extent_km2': self._calculate_spatial_extent(lats, lngs),
            'centroid_lat': float(lats.mean()),
            'centroid_lng': float(lngs.mean()),
            'lat_std': float(lats.std()),
            'lng_std': float(lngs.std()),
            'total_data_points': len(df),
            'unique_vehicles': df['COMMADDR'].nunique()
        }
    
    def _calculate_trend(self, values: List[float]) -> str:
        """è®¡ç®—è¶‹åŠ¿æ–¹å‘"""
        if len(values) < 2:
            return "stable"
        
        # ç®€å•çš„çº¿æ€§å›å½’è®¡ç®—è¶‹åŠ¿
        x = np.arange(len(values))
        y = np.array(values)
        
        slope = np.polyfit(x, y, 1)[0]
        
        if slope > 0.1:
            return "increasing"
        elif slope < -0.1:
            return "decreasing"
        else:
            return "stable"
    
    def _calculate_spatial_extent(self, lats: pd.Series, lngs: pd.Series) -> float:
        """è®¡ç®—ç©ºé—´èŒƒå›´ï¼ˆç®€åŒ–ä¸ºå¯¹è§’çº¿è·ç¦»ï¼‰"""
        from geopy.distance import geodesic
        
        try:
            lat_range = (lats.min(), lats.max())
            lng_range = (lngs.min(), lngs.max())
            
            # è®¡ç®—å¯¹è§’çº¿è·ç¦»ä½œä¸ºç©ºé—´èŒƒå›´æŒ‡æ ‡
            distance = geodesic(
                (lat_range[0], lng_range[0]),
                (lat_range[1], lng_range[1])
            ).kilometers
            
            return round(distance, 2)
        except:
            return 0.0

    # è·¯æ®µåˆ†æç›¸å…³æ–¹æ³•
    
    def analyze_road_segments(
        self, 
        df: pd.DataFrame,
        analysis_type: str = "comprehensive",
        segment_types: List[str] = ["all"],
        aggregation_level: str = "segment",
        min_vehicles: int = 10
    ) -> Dict[str, Any]:
        """
        åˆ†æè·¯æ®µæ•°æ®
        
        Args:
            df: è½¨è¿¹æ•°æ®DataFrame
            analysis_type: åˆ†æç±»å‹ (comprehensive, speed, flow, congestion)
            segment_types: è·¯æ®µç±»å‹è¿‡æ»¤ (highway, urban, arterial, local, all)
            aggregation_level: èšåˆçº§åˆ« (segment, road, network)
            min_vehicles: æœ€å°è½¦è¾†æ•°é˜ˆå€¼
            
        Returns:
            è·¯æ®µåˆ†æç»“æœå­—å…¸
        """
        try:
            from .road_analysis_engine import RoadAnalysisEngine
            
            if df.empty:
                return {"error": "æ•°æ®ä¸ºç©º"}
            
            # æ ‡å‡†åŒ–æ•°æ®åˆ—å
            df = self._standardize_dataframe(df)
            
            # åˆå§‹åŒ–è·¯æ®µåˆ†æå¼•æ“
            road_engine = RoadAnalysisEngine()
            
            # æå–è·¯æ®µä¿¡æ¯
            print("æå–è·¯æ®µä¿¡æ¯...")
            road_segments = road_engine.extract_road_segments(df)
            
            if not road_segments:
                return {"error": "æ— æ³•æå–è·¯æ®µä¿¡æ¯"}
            
            # åˆ†æè·¯æ®µäº¤é€šæ•°æ®
            print("åˆ†æè·¯æ®µäº¤é€šæ•°æ®...")
            traffic_data = road_engine.analyze_road_traffic(df, road_segments)
            
            if not traffic_data:
                return {"error": "æ— æ³•åˆ†æäº¤é€šæ•°æ®"}
            
            # åº”ç”¨è½¦è¾†æ•°é‡è¿‡æ»¤
            filtered_traffic_data = [
                data for data in traffic_data 
                if data.vehicle_count >= min_vehicles
            ]
            
            if not filtered_traffic_data:
                return {"error": f"æ²¡æœ‰æ»¡è¶³æœ€å°è½¦è¾†æ•°({min_vehicles})çš„è·¯æ®µæ•°æ®"}
            
            # è®¡ç®—æ—¶é—´èŒƒå›´
            timestamps = [data.timestamp for data in filtered_traffic_data]
            time_range = (min(timestamps), max(timestamps))
            
            # è®¡ç®—è·¯æ®µç»Ÿè®¡
            print("è®¡ç®—è·¯æ®µç»Ÿè®¡...")
            segment_stats = road_engine.calculate_segment_statistics(filtered_traffic_data, time_range)
            
            # åˆ†æç»“æœ
            result = {
                "success": True,
                "segments": [segment.dict() for segment in road_segments],
                "traffic_data": [data.dict() for data in filtered_traffic_data],
                "segment_statistics": [stats.dict() for stats in segment_stats],
                "analysis_metadata": {
                    "analysis_type": analysis_type,
                    "segment_types": segment_types,
                    "aggregation_level": aggregation_level,
                    "min_vehicles": min_vehicles,
                    "time_range": time_range,
                    "total_segments": len(road_segments),
                    "active_segments": len(filtered_traffic_data),
                    "analysis_timestamp": datetime.now().isoformat()
                }
            }
            
            # æ ¹æ®åˆ†æç±»å‹æ·»åŠ ç‰¹å®šåˆ†æ
            if analysis_type in ["comprehensive", "speed"]:
                speed_distributions = road_engine.analyze_speed_distribution(filtered_traffic_data)
                result["speed_distributions"] = [dist.dict() for dist in speed_distributions]
            
            if analysis_type in ["comprehensive", "flow"]:
                flow_patterns = road_engine.analyze_traffic_patterns(filtered_traffic_data)
                result["flow_patterns"] = [pattern.dict() for pattern in flow_patterns]
            
            if analysis_type in ["comprehensive", "congestion"]:
                bottlenecks = road_engine.identify_bottlenecks(segment_stats)
                result["bottlenecks"] = bottlenecks
            
            # ç”Ÿæˆç½‘ç»œæ‘˜è¦
            network_summary = road_engine.generate_network_summary(segment_stats, filtered_traffic_data)
            result["network_summary"] = network_summary
            
            print(f"è·¯æ®µåˆ†æå®Œæˆ: {len(road_segments)} ä¸ªè·¯æ®µ, {len(filtered_traffic_data)} æ¡äº¤é€šæ•°æ®")
            return result
            
        except Exception as e:
            print(f"è·¯æ®µåˆ†ææ—¶å‡ºé”™: {str(e)}")
            return {"error": f"åˆ†æå¤±è´¥: {str(e)}"}
    
    def generate_road_visualization_data(
        self, 
        segments_data: List[Dict],
        traffic_data: List[Dict],
        visualization_type: str = "speed"
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆè·¯æ®µå¯è§†åŒ–æ•°æ®
        
        Args:
            segments_data: è·¯æ®µæ•°æ®åˆ—è¡¨
            traffic_data: äº¤é€šæ•°æ®åˆ—è¡¨
            visualization_type: å¯è§†åŒ–ç±»å‹ (speed, flow, congestion, efficiency)
            
        Returns:
            å¯è§†åŒ–æ•°æ®å­—å…¸
        """
        try:
            # åˆ›å»ºè·¯æ®µåˆ°äº¤é€šæ•°æ®çš„æ˜ å°„
            segment_traffic_map = {}
            for traffic in traffic_data:
                segment_id = traffic.get('segment_id')
                if segment_id not in segment_traffic_map:
                    segment_traffic_map[segment_id] = []
                segment_traffic_map[segment_id].append(traffic)
            
            visualization_data = {
                "type": visualization_type,
                "segments": [],
                "legend": self._get_visualization_legend(visualization_type),
                "statistics": {},
                "color_mapping": {}
            }
            
            # è®¡ç®—å¯è§†åŒ–å€¼å’Œé¢œè‰²
            for segment in segments_data:
                segment_id = segment.get('segment_id')
                traffic_list = segment_traffic_map.get(segment_id, [])
                
                if not traffic_list:
                    continue
                
                # è®¡ç®—å¯è§†åŒ–å€¼
                if visualization_type == "speed":
                    value = np.mean([t.get('avg_speed', 0) for t in traffic_list])
                    color = self._get_speed_color(value)
                elif visualization_type == "flow":
                    value = np.mean([t.get('flow_rate', 0) for t in traffic_list])
                    color = self._get_flow_color(value)
                elif visualization_type == "congestion":
                    congestion_levels = [t.get('congestion_level', 'free') for t in traffic_list]
                    value = self._calculate_congestion_score(congestion_levels)
                    color = self._get_congestion_color(value)
                else:  # efficiency
                    value = 75  # é»˜è®¤æ•ˆç‡å€¼
                    color = self._get_efficiency_color(value)
                
                segment_vis_data = {
                    "segment_id": segment_id,
                    "start_point": segment.get('start_point'),
                    "end_point": segment.get('end_point'),
                    "value": round(value, 2),
                    "color": color,
                    "road_type": segment.get('road_type'),
                    "road_name": segment.get('road_name'),
                    "segment_length": segment.get('segment_length', 0)
                }
                
                visualization_data["segments"].append(segment_vis_data)
                visualization_data["color_mapping"][segment_id] = color
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            if visualization_data["segments"]:
                values = [s["value"] for s in visualization_data["segments"]]
                visualization_data["statistics"] = {
                    "min_value": min(values),
                    "max_value": max(values),
                    "avg_value": np.mean(values),
                    "total_segments": len(visualization_data["segments"])
                }
            
            return visualization_data
            
        except Exception as e:
            print(f"ç”Ÿæˆå¯è§†åŒ–æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return {"error": f"ç”Ÿæˆå¯è§†åŒ–æ•°æ®å¤±è´¥: {str(e)}"}
    
    def _get_visualization_legend(self, viz_type: str) -> Dict[str, Any]:
        """è·å–å¯è§†åŒ–å›¾ä¾‹ä¿¡æ¯"""
        legends = {
            "speed": {
                "title": "å¹³å‡é€Ÿåº¦ (km/h)",
                "ranges": [
                    {"color": "#d73027", "label": "< 20", "range": [0, 20]},
                    {"color": "#fc8d59", "label": "20-40", "range": [20, 40]},
                    {"color": "#fee08b", "label": "40-60", "range": [40, 60]},
                    {"color": "#d9ef8b", "label": "60-80", "range": [60, 80]},
                    {"color": "#91d1c2", "label": "> 80", "range": [80, 999]}
                ]
            },
            "flow": {
                "title": "äº¤é€šæµé‡ (vehicles/h)",
                "ranges": [
                    {"color": "#2c7fb8", "label": "< 100", "range": [0, 100]},
                    {"color": "#7fcdbb", "label": "100-300", "range": [100, 300]},
                    {"color": "#c7e9b4", "label": "300-500", "range": [300, 500]},
                    {"color": "#fec44f", "label": "500-800", "range": [500, 800]},
                    {"color": "#d95f0e", "label": "> 800", "range": [800, 9999]}
                ]
            },
            "congestion": {
                "title": "æ‹¥å µç¨‹åº¦",
                "ranges": [
                    {"color": "#1a9850", "label": "ç•…é€š", "range": [0, 0.3]},
                    {"color": "#91d1c2", "label": "ç¼“æ…¢", "range": [0.3, 0.6]},
                    {"color": "#fee08b", "label": "æ‹¥å µ", "range": [0.6, 0.8]},
                    {"color": "#d73027", "label": "ä¸¥é‡æ‹¥å µ", "range": [0.8, 1.0]}
                ]
            },
            "efficiency": {
                "title": "è¿è¡Œæ•ˆç‡",
                "ranges": [
                    {"color": "#d73027", "label": "å¾ˆä½", "range": [0, 30]},
                    {"color": "#fc8d59", "label": "è¾ƒä½", "range": [30, 50]},
                    {"color": "#fee08b", "label": "ä¸­ç­‰", "range": [50, 70]},
                    {"color": "#d9ef8b", "label": "è¾ƒé«˜", "range": [70, 85]},
                    {"color": "#1a9850", "label": "å¾ˆé«˜", "range": [85, 100]}
                ]
            }
        }
        return legends.get(viz_type, {"title": "æœªçŸ¥", "ranges": []})
    
    def _get_speed_color(self, speed: float) -> str:
        """æ ¹æ®é€Ÿåº¦è·å–é¢œè‰²"""
        if speed < 20:
            return "#d73027"
        elif speed < 40:
            return "#fc8d59"
        elif speed < 60:
            return "#fee08b"
        elif speed < 80:
            return "#d9ef8b"
        else:
            return "#91d1c2"
    
    def _get_flow_color(self, flow: float) -> str:
        """æ ¹æ®æµé‡è·å–é¢œè‰²"""
        if flow < 100:
            return "#2c7fb8"
        elif flow < 300:
            return "#7fcdbb"
        elif flow < 500:
            return "#c7e9b4"
        elif flow < 800:
            return "#fec44f"
        else:
            return "#d95f0e"
    
    def _get_congestion_color(self, congestion_score: float) -> str:
        """æ ¹æ®æ‹¥å µåˆ†æ•°è·å–é¢œè‰²"""
        if congestion_score < 0.3:
            return "#1a9850"  # ç»¿è‰² - ç•…é€š
        elif congestion_score < 0.6:
            return "#91d1c2"  # æµ…ç»¿ - ç¼“æ…¢
        elif congestion_score < 0.8:
            return "#fee08b"  # é»„è‰² - æ‹¥å µ
        else:
            return "#d73027"  # çº¢è‰² - ä¸¥é‡æ‹¥å µ
    
    def _get_efficiency_color(self, efficiency: float) -> str:
        """æ ¹æ®æ•ˆç‡åˆ†æ•°è·å–é¢œè‰²"""
        if efficiency < 30:
            return "#d73027"
        elif efficiency < 50:
            return "#fc8d59"
        elif efficiency < 70:
            return "#fee08b"
        elif efficiency < 85:
            return "#d9ef8b"
        else:
            return "#1a9850"
    
    def _calculate_congestion_score(self, congestion_levels: List[str]) -> float:
        """è®¡ç®—æ‹¥å µåˆ†æ•°"""
        if not congestion_levels:
            return 0.0
        
        level_scores = {
            'free': 0.1,
            'moderate': 0.4,
            'heavy': 0.7,
            'jam': 0.9
        }
        
        total_score = sum(level_scores.get(level, 0.1) for level in congestion_levels)
        return total_score / len(congestion_levels)
    
    def calculate_road_network_metrics(
        self, 
        segments_data: List[Dict],
        traffic_data: List[Dict]
    ) -> Dict[str, Any]:
        """
        è®¡ç®—è·¯ç½‘æ•´ä½“æŒ‡æ ‡
        
        Args:
            segments_data: è·¯æ®µæ•°æ®
            traffic_data: äº¤é€šæ•°æ®
            
        Returns:
            è·¯ç½‘æŒ‡æ ‡å­—å…¸
        """
        try:
            if not segments_data or not traffic_data:
                return {"error": "æ•°æ®ä¸è¶³"}
            
            # åŸºç¡€ç»Ÿè®¡
            total_segments = len(segments_data)
            total_length = sum(s.get('segment_length', 0) for s in segments_data)
            
            # äº¤é€šæŒ‡æ ‡
            speeds = [t.get('avg_speed', 0) for t in traffic_data]
            flows = [t.get('flow_rate', 0) for t in traffic_data]
            densities = [t.get('traffic_density', 0) for t in traffic_data]
            
            # æ‹¥å µåˆ†æ
            congestion_counts = {}
            for t in traffic_data:
                level = t.get('congestion_level', 'free')
                congestion_counts[level] = congestion_counts.get(level, 0) + 1
            
            # é“è·¯ç±»å‹åˆ†å¸ƒ
            road_type_counts = {}
            for s in segments_data:
                road_type = s.get('road_type', 'unknown')
                road_type_counts[road_type] = road_type_counts.get(road_type, 0) + 1
            
            metrics = {
                "network_overview": {
                    "total_segments": total_segments,
                    "total_length_km": round(total_length, 2),
                    "avg_segment_length": round(total_length / total_segments, 3) if total_segments > 0 else 0,
                    "active_segments": len(traffic_data)
                },
                "traffic_performance": {
                    "avg_speed": round(np.mean(speeds), 2) if speeds else 0,
                    "min_speed": round(min(speeds), 2) if speeds else 0,
                    "max_speed": round(max(speeds), 2) if speeds else 0,
                    "speed_variance": round(np.var(speeds), 2) if len(speeds) > 1 else 0,
                    "avg_flow": round(np.mean(flows), 2) if flows else 0,
                    "max_flow": round(max(flows), 2) if flows else 0,
                    "avg_density": round(np.mean(densities), 2) if densities else 0
                },
                "congestion_analysis": {
                    "congestion_distribution": congestion_counts,
                    "congestion_rate": {
                        level: round(count / len(traffic_data) * 100, 1) 
                        for level, count in congestion_counts.items()
                    } if traffic_data else {},
                    "total_congested_segments": congestion_counts.get('heavy', 0) + congestion_counts.get('jam', 0)
                },
                "road_infrastructure": {
                    "road_type_distribution": road_type_counts,
                    "road_type_percentage": {
                        road_type: round(count / total_segments * 100, 1)
                        for road_type, count in road_type_counts.items()
                    } if total_segments > 0 else {}
                },
                "efficiency_indicators": {
                    "network_utilization": round(len(traffic_data) / total_segments * 100, 1) if total_segments > 0 else 0,
                    "free_flow_percentage": round(congestion_counts.get('free', 0) / len(traffic_data) * 100, 1) if traffic_data else 0,
                    "bottleneck_rate": round((congestion_counts.get('heavy', 0) + congestion_counts.get('jam', 0)) / len(traffic_data) * 100, 1) if traffic_data else 0
                }
            }
            
            return metrics
            
        except Exception as e:
            print(f"è®¡ç®—è·¯ç½‘æŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}")
            return {"error": f"è®¡ç®—å¤±è´¥: {str(e)}"}
    
    def analyze_weekly_passenger_flow(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†æå‘¨å®¢æµé‡æ•°æ®
        
        Args:
            df: äº¤é€šæ•°æ®DataFrame
            
        Returns:
            å‘¨å®¢æµé‡åˆ†æç»“æœ
        """
        if df.empty:
            return {
                'success': False,
                'message': 'æ²¡æœ‰å¯ç”¨æ•°æ®',
                'weekly_flow': {},
                'comparison': {},
                'patterns': {},
                'statistics': {}
            }
        
        try:
            # æ ‡å‡†åŒ–DataFrameåˆ—å
            df = self._standardize_dataframe(df)
            
            # æ·»åŠ æ—¶é—´ç›¸å…³åˆ—
            df['datetime'] = pd.to_datetime(df['UTC'], unit='s')
            df['weekday'] = df['datetime'].dt.weekday  # 0=Monday, 6=Sunday
            df['hour'] = df['datetime'].dt.hour
            df['date'] = df['datetime'].dt.date
            df['week'] = df['datetime'].dt.isocalendar().week
            
            # 1. æ¯æ—¥å®¢æµé‡ç»Ÿè®¡
            daily_flow = self._calculate_daily_passenger_flow(df)
            
            # 2. å·¥ä½œæ—¥vså‘¨æœ«å¯¹æ¯”
            weekday_comparison = self._calculate_weekday_weekend_comparison(df)
            
            # 3. ä¸€å‘¨å†…å„å¤©çš„æµé‡æ¨¡å¼
            weekly_patterns = self._calculate_weekly_patterns(df)
            
            # 4. å°æ—¶çº§æµé‡åˆ†å¸ƒï¼ˆæŒ‰å·¥ä½œæ—¥/å‘¨æœ«åˆ†ç»„ï¼‰
            hourly_patterns = self._calculate_hourly_patterns_by_day_type(df)
            
            # 5. å®¢æµé‡è¶‹åŠ¿åˆ†æ
            flow_trends = self._calculate_weekly_flow_trends(df)
            
            # 6. é«˜å³°æ—¶æ®µåˆ†æ
            peak_analysis = self._analyze_weekly_peak_periods(df)
            
            # 7. ç»¼åˆç»Ÿè®¡ä¿¡æ¯
            weekly_statistics = self._calculate_weekly_statistics(df)
            
            return {
                'success': True,
                'message': 'å‘¨å®¢æµé‡åˆ†æå®Œæˆ',
                'daily_flow': daily_flow,
                'weekday_comparison': weekday_comparison,
                'weekly_patterns': weekly_patterns,
                'hourly_patterns': hourly_patterns,
                'flow_trends': flow_trends,
                'peak_analysis': peak_analysis,
                'statistics': weekly_statistics,
                'analysis_period': {
                    'start_date': str(df['date'].min()),
                    'end_date': str(df['date'].max()),
                    'total_days': df['date'].nunique(),
                    'total_weeks': df['week'].nunique()
                }
            }
        
        except Exception as e:
            logging.error(f"å‘¨å®¢æµé‡åˆ†æå¤±è´¥: {str(e)}")
            return {
                'success': False,
                'message': f'åˆ†æå¤±è´¥: {str(e)}',
                'weekly_flow': {},
                'comparison': {},
                'patterns': {},
                'statistics': {}
            }
    
    def _calculate_daily_passenger_flow(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—æ¯æ—¥å®¢æµé‡"""
        # æŒ‰æ—¥æœŸåˆ†ç»„è®¡ç®—è½¦è¾†æ•°å’Œè½¨è¿¹ç‚¹æ•°
        daily_stats = df.groupby('date').agg({
            'COMMADDR': 'nunique',  # ç‹¬ç«‹è½¦è¾†æ•°
            'UTC': 'count'          # è½¨è¿¹ç‚¹æ•°
        }).reset_index()
        
        daily_stats.columns = ['date', 'vehicles', 'total_points']
        daily_stats['date'] = daily_stats['date'].astype(str)
        
        # æ·»åŠ æ˜ŸæœŸä¿¡æ¯
        daily_stats['weekday'] = pd.to_datetime(daily_stats['date']).dt.weekday
        daily_stats['weekday_name'] = pd.to_datetime(daily_stats['date']).dt.day_name()
        daily_stats['is_weekend'] = daily_stats['weekday'].isin([5, 6])
        
        return {
            'daily_data': daily_stats.to_dict('records'),
            'total_days': len(daily_stats),
            'avg_daily_vehicles': daily_stats['vehicles'].mean(),
            'max_daily_vehicles': daily_stats['vehicles'].max(),
            'min_daily_vehicles': daily_stats['vehicles'].min()
        }
    
    def _calculate_weekday_weekend_comparison(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—å·¥ä½œæ—¥vså‘¨æœ«å¯¹æ¯”"""
        df['is_weekend'] = df['weekday'].isin([5, 6])
        
        # æŒ‰å·¥ä½œæ—¥/å‘¨æœ«åˆ†ç»„ç»Ÿè®¡
        comparison = df.groupby(['date', 'is_weekend']).agg({
            'COMMADDR': 'nunique',
            'UTC': 'count'
        }).reset_index()
        
        weekday_data = comparison[comparison['is_weekend'] == False]['COMMADDR']
        weekend_data = comparison[comparison['is_weekend'] == True]['COMMADDR']
        
        weekday_avg = weekday_data.mean() if len(weekday_data) > 0 else 0
        weekend_avg = weekend_data.mean() if len(weekend_data) > 0 else 0
        
        # è®¡ç®—å·®å¼‚ç™¾åˆ†æ¯”
        if weekday_avg > 0:
            difference_pct = ((weekend_avg - weekday_avg) / weekday_avg) * 100
        else:
            difference_pct = 0
        
        return {
            'weekday_avg': round(weekday_avg, 1),
            'weekend_avg': round(weekend_avg, 1),
            'difference_pct': round(difference_pct, 1),
            'weekday_days': len(weekday_data),
            'weekend_days': len(weekend_data),
            'pattern': 'weekend_higher' if weekend_avg > weekday_avg else 'weekday_higher'
        }
    
    def _calculate_weekly_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—ä¸€å‘¨å†…å„å¤©çš„æµé‡æ¨¡å¼"""
        weekday_names = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
        
        # æŒ‰æ˜ŸæœŸåˆ†ç»„ç»Ÿè®¡
        weekly_flow = df.groupby(['date', 'weekday']).agg({
            'COMMADDR': 'nunique',
            'UTC': 'count'
        }).reset_index()
        
        # è®¡ç®—æ¯ä¸ªæ˜ŸæœŸå‡ çš„å¹³å‡æµé‡
        avg_by_weekday = weekly_flow.groupby('weekday')['COMMADDR'].mean().reset_index()
        avg_by_weekday['weekday_name'] = [weekday_names[i] for i in avg_by_weekday['weekday']]
        
        # æ‰¾å‡ºæœ€é«˜å’Œæœ€ä½æµé‡çš„å¤©
        max_day = avg_by_weekday.loc[avg_by_weekday['COMMADDR'].idxmax()]
        min_day = avg_by_weekday.loc[avg_by_weekday['COMMADDR'].idxmin()]
        
        return {
            'weekly_data': avg_by_weekday.to_dict('records'),
            'peak_day': {
                'day': max_day['weekday_name'],
                'vehicles': round(max_day['COMMADDR'], 1)
            },
            'lowest_day': {
                'day': min_day['weekday_name'],
                'vehicles': round(min_day['COMMADDR'], 1)
            },
            'weekly_variance': round(avg_by_weekday['COMMADDR'].var(), 2)
        }
    
    def _calculate_hourly_patterns_by_day_type(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—æŒ‰å·¥ä½œæ—¥/å‘¨æœ«åˆ†ç»„çš„å°æ—¶æµé‡æ¨¡å¼"""
        df['is_weekend'] = df['weekday'].isin([5, 6])
        df['day_type'] = df['is_weekend'].map({True: 'å‘¨æœ«', False: 'å·¥ä½œæ—¥'})
        
        # æŒ‰å°æ—¶å’Œæ—¥æœŸç±»å‹åˆ†ç»„
        hourly_patterns = df.groupby(['day_type', 'hour', 'date']).agg({
            'COMMADDR': 'nunique'
        }).reset_index()
        
        # è®¡ç®—æ¯å°æ—¶çš„å¹³å‡æµé‡
        hourly_avg = hourly_patterns.groupby(['day_type', 'hour'])['COMMADDR'].mean().reset_index()
        
        # åˆ†ç¦»å·¥ä½œæ—¥å’Œå‘¨æœ«æ•°æ®
        weekday_hourly = hourly_avg[hourly_avg['day_type'] == 'å·¥ä½œæ—¥']
        weekend_hourly = hourly_avg[hourly_avg['day_type'] == 'å‘¨æœ«']
        
        # æ‰¾å‡ºé«˜å³°æ—¶æ®µ
        weekday_peak_hour = weekday_hourly.loc[weekday_hourly['COMMADDR'].idxmax(), 'hour'] if len(weekday_hourly) > 0 else 0
        weekend_peak_hour = weekend_hourly.loc[weekend_hourly['COMMADDR'].idxmax(), 'hour'] if len(weekend_hourly) > 0 else 0
        
        return {
            'hourly_data': hourly_avg.to_dict('records'),
            'weekday_pattern': weekday_hourly.to_dict('records'),
            'weekend_pattern': weekend_hourly.to_dict('records'),
            'peak_hours': {
                'weekday': int(weekday_peak_hour),
                'weekend': int(weekend_peak_hour)
            }
        }
    
    def _calculate_weekly_flow_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—å‘¨å®¢æµé‡è¶‹åŠ¿"""
        # æŒ‰å‘¨ç»Ÿè®¡
        weekly_trends = df.groupby(['week', 'date']).agg({
            'COMMADDR': 'nunique'
        }).reset_index()
        
        # æŒ‰å‘¨èšåˆ
        weekly_summary = weekly_trends.groupby('week').agg({
            'COMMADDR': ['mean', 'sum', 'count']
        }).reset_index()
        
        weekly_summary.columns = ['week', 'avg_daily_vehicles', 'total_vehicles', 'days_count']
        
        # è®¡ç®—è¶‹åŠ¿
        if len(weekly_summary) > 1:
            trend_slope = np.polyfit(range(len(weekly_summary)), weekly_summary['avg_daily_vehicles'], 1)[0]
            trend_direction = 'increasing' if trend_slope > 0 else 'decreasing'
        else:
            trend_slope = 0
            trend_direction = 'stable'
        
        return {
            'weekly_summary': weekly_summary.to_dict('records'),
            'trend_analysis': {
                'slope': round(trend_slope, 2),
                'direction': trend_direction,
                'total_weeks': len(weekly_summary)
            }
        }
    
    def _analyze_weekly_peak_periods(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå‘¨é«˜å³°æ—¶æ®µ"""
        df['is_weekend'] = df['weekday'].isin([5, 6])
        
        # å®šä¹‰æ—¶æ®µ
        def get_time_period(hour):
            if 6 <= hour < 9:
                return 'æ—©é«˜å³°'
            elif 17 <= hour < 20:
                return 'æ™šé«˜å³°'
            elif 9 <= hour < 17:
                return 'æ—¥é—´'
            else:
                return 'å¤œé—´'
        
        df['time_period'] = df['hour'].apply(get_time_period)
        
        # æŒ‰æ—¶æ®µå’Œæ—¥æœŸç±»å‹ç»Ÿè®¡
        period_stats = df.groupby(['is_weekend', 'time_period', 'date']).agg({
            'COMMADDR': 'nunique'
        }).reset_index()
        
        period_avg = period_stats.groupby(['is_weekend', 'time_period'])['COMMADDR'].mean().reset_index()
        period_avg['day_type'] = period_avg['is_weekend'].map({True: 'å‘¨æœ«', False: 'å·¥ä½œæ—¥'})
        
        return {
            'period_analysis': period_avg.to_dict('records'),
            'peak_periods': self._identify_peak_periods(period_avg)
        }
    
    def _identify_peak_periods(self, period_avg: pd.DataFrame) -> Dict[str, Any]:
        """è¯†åˆ«é«˜å³°æ—¶æ®µ"""
        weekday_data = period_avg[period_avg['is_weekend'] == False]
        weekend_data = period_avg[period_avg['is_weekend'] == True]
        
        weekday_peak = weekday_data.loc[weekday_data['COMMADDR'].idxmax()] if len(weekday_data) > 0 else None
        weekend_peak = weekend_data.loc[weekend_data['COMMADDR'].idxmax()] if len(weekend_data) > 0 else None
        
        return {
            'weekday_peak': {
                'period': weekday_peak['time_period'] if weekday_peak is not None else 'N/A',
                'vehicles': round(weekday_peak['COMMADDR'], 1) if weekday_peak is not None else 0
            },
            'weekend_peak': {
                'period': weekend_peak['time_period'] if weekend_peak is not None else 'N/A', 
                'vehicles': round(weekend_peak['COMMADDR'], 1) if weekend_peak is not None else 0
            }
        }
    
    def _calculate_weekly_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—å‘¨å®¢æµé‡ç»¼åˆç»Ÿè®¡"""
        # åŸºç¡€ç»Ÿè®¡
        total_vehicles = df['COMMADDR'].nunique()
        total_points = len(df)
        date_span = df['date'].nunique()
        
        # æŒ‰æ—¥ç»Ÿè®¡
        daily_vehicles = df.groupby('date')['COMMADDR'].nunique()
        
        return {
            'total_unique_vehicles': int(total_vehicles),
            'total_data_points': int(total_points),
            'analysis_days': int(date_span),
            'avg_daily_vehicles': round(daily_vehicles.mean(), 1),
            'max_daily_vehicles': int(daily_vehicles.max()),
            'min_daily_vehicles': int(daily_vehicles.min()),
            'vehicle_flow_variance': round(daily_vehicles.var(), 2),
            'data_completeness': round((date_span / 7) * 100, 1)  # å‡è®¾ä¸€å‘¨7å¤©çš„å®Œæ•´åº¦
        } 